% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{array}
\usepackage{url} 
\usepackage[OT1]{fontenc} 
\usepackage{fontspec}
\usepackage{float}
% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\usepackage[bottom]{footmisc}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\def\groupID{Group-Q}
\def\subjectNum{COMP6721}
\def\subName{Applied AI } 



\begin{document}
\definecolor{barblue}{RGB}{206,34,34}
\definecolor{groupblue}{RGB}{120,34,34}
\definecolor{linkred}{RGB}{165,0,33}
% \renewcommand\sfdefault{phv}
% \renewcommand\mddefault{mc}
% \renewcommand\bfdefault{bc}
%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{Group-Q} % *** Enter the CVPR Paper ID here
\def\confName{COMP6721}
\def\confYear{2022}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Lung Disease Clasification - \subName Final Report}
\author{Rohan Chopra\\
\small 40233019\\
\and
Harman Singh Jolly\\
\small 40204947\\
\and
Harman Preet Kaur\\
\small 40198317\\
\and
Abhishek Handa\\
\small 40231719\\
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
  The ABSTRACT is to be in fully justified italicized text, at the top of the left-hand column, 
  below the author and affiliation information.
  Use the word ``Abstract'' as the title, in 12-point Times, boldface type, centered relative 
  to the column, initially capitalized.
  The abstract is to be in 10-point, single-spaced type.
  Leave two blank lines after the Abstract, then begin the main text.
  Look at previous CVPR abstracts to get a feel for style and length.
  Look at previous CVPR abstracts to get a feel for style and length.
  Look at previous CVPR abstracts to get a feel for style and length.
  Look at previous CVPR abstracts to get a feel for style and length.
  Look at previous CVPR abstracts to get a feel for style and length.
  Look at previous CVPR abstracts to get a feel for style and length.
  Look at previous CVPR abstracts.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}

Early diagnosis of respiratory diseases like pneumonia and COVID-19 leads to decreased mortality 
rate \cite{daniel2016time} and is a powerful way to manage a pandemic\cite{xu2020facile}. 
These diseases can be diagnosed using a variety of tests like pulse oximetry, chest x-ray, 
CT scan\cite{mattsmith2022},  PCR\cite{akhtar1996pcr} however chest X-rays are by far the most 
accessible\cite{frija2021improve} to low and middle income countries. 
Furthermore, the scan is available in minutes making it one of the fastest ways of diagnosis
\cite{healthwise2021}. However, the bottleneck with this method 
is the need for an expert radiologists to evaluate the scan\cite{mehrotra2009radiologists}. 
Many researchers have tried to solve this problem by creating a deep learning based
lung disease classification system \cite{wang2021deep} but 
haven't been able to come up with models that can replace radiologists. Small 
\cite{guefrechi2021deep} and highly imbalanced data \cite{wang2021deep}, along with varying 
specifications of X-ray scanners leading to low inter-hospital accuracy \cite{melissarohman2018}
are the biggest problems that 
researchers have faced. Another issue with using deep neural networks in medical settings is 
its black-box nature\cite{paulblazek2022}, doctors and patients will not trust a model that 
cannot explain its results\cite{aleksandra2019}. 

This project is an attempt to compare three CNN backbone architectures namely, ResNet-34, 
MobileNet V3 Large and EfficientNet B1 along with three lung disease datasets 
to identify the type of architecture that works best for lung disease classification. Two 
of the datasets used presented a multiclass classification problem with 3 classes while 
the third dataset presented a multiclass, multilabel classification problem. A total 
of 12 models were trained in this study, four for each of the three datasets. The first 
three models for each dataset was trained from scratch and the fourth model was trained 
using transfer learning. Transfer learning was performed by deep-tuning ImageNet
weights and the performance was evaluated to check improvement over the models trained from scratch. 
The small dataset problem and the issue of different radiographic contrast \cite{andrew2022rad} 
is mitigated using data augmentation. Imbalanced data problem is handled by undersampling the 
majority class. The hyperparameters were fixed across models and the F1 scores and cross entropy loss 
have been used to compare models and select the best overall model. All the models were optimized using 
the Adam optimizer \cite{kingma2014adam} with default parameters and the cosine annealing \cite{loshchilov2016sgdr} 
learning rate scheduler was used to decrease the learning rate as training progressed.
Further, an ablation study was performed to find the best learning rate for the selected model.
Finally, GradCAM \cite{jacobgilpytorchcam} and T-SNE were used to visualize the trained models and understand 
model predictions better. An F1 score of 0.8 and 0.98 was achieved for the two multiclass
datasets, whereas the maximum F1 for the multilabel dataset with 7 classes was 0.46.

\textbf{Related Works:} 
Li \etal \cite{li2014medical} were among the first
to use CNNs in a medical setting. They used a single convolutional layer to classify 
interstitial lung diseases using CT scans, achieving better performance than existing 
approaches. Since then there has been a dramtic increase in application of CNNs 
in healthcare, deep neural networks have been used to perform various tasks like 
segmenting regions of interest in MRI \cite{kayalibay2017cnn,dolz2018hyperdense}, classifying X-Ray  
\cite{rajpurkar2017chexnet}, MRI \cite{farooq2017deep}, and CT \cite{alakwaa2017lung} scans.
Further, GANs have been used to generate high quality scans \cite{loey2020within} 
when there is a lack of available data due to either privacy reasons or availability of subjects. 
GANs have also been used to generate high quality CT scans from MRI scans 
\cite{liu2021ct}. Apart from radiographic scans,
deep CNNs have also been used to detect malarial parasite in blood smear images 
\cite{umer2020novel} with an accuracy of 99.96\%. Another interesting 
application is the use of 1-D convolutions to detect heart anomalies using ECG data
\cite{kiranyaz2015real}. Researchers have also used architectures like the Inception V3 to perform 
dermatologist level skin-cancer detection using skin lesion images \cite{esteva2017dermatologist} using transfer 
learning.  

In the recent years, many researchers have tried to predict lung diseases using deep CNNs, 
Wang \etal \cite{wang2017chestx} used state of the art backbone architectures to train 
a lung disease classifier for multilabel data by training only the prediction and transition 
layers from scratch and leaving pre-trained ImageNet weights freezed while training. 
They achieved a high AUC of over 0.6 for most of the classes in the dataset with this technique.
Rajpurkar \etal \cite{rajpurkar2017chexnet} created a 121 layer deep CNN - CheXNet to detect
pneumonia using chest X-rays with radiologist level accuracy. 
Labhane \etal \cite{labhane2020detection} used transfer learning with state of the art backbone architectures like VGG16, 
VGG19 and InceptionV3 to predict pneumonia in pediatric patients and achieved an F1 score of 0.97.
Islam \etal combined CNN and LSTM 
to create a COVID-19 detector \cite{islam2020combined}. The CNN extracted complex features from scans and the LSTM was 
used as a classifier. This method resulted in an improvement over a vanilla CNN network and 
an F1 score of 98.9\% was achieved. Abbas \etal \cite{abbas2021classification} created the 
DeTraC network to detect COVID in chest X-rays that improved performance of existing backbone 
models significantly with the highest accuracy of 98.23\% using the VGG19 architecture.  
Guefrechi \etal \cite{guefrechi2021deep} on the other hand used data augmentation techniques like random rotation, 
flipping and noise with transfer learning on backbone architectures like ResNet50, 
InceptionV3 and VGG16 to achieve a high accuracy of 98.3\%.

In the following sections methodology of the approach and the results will be discussed.



%-------------------------------------------------------------------------
\section{Methodology}
\label{sec:prop_method}



\begin{table}
  \centering
  \begin{tabular}{p{2.9cm}|p{2.3cm}|wc{0.8cm}|p{0.8cm}}
    \toprule
    Dataset & No. of Images & Classes & Size\\
    \midrule
    COVID\cite{RAHMAN2021104319,9144185,kagglecovid} & 3.6k:3.6k:1.3k & 3 & 299\textsuperscript{2}\\
    \midrule
    Pneumonia\cite{kermany2018labeled,kagglepneu} & 3k:1.5k:1.5k & 3 & 224\textsuperscript{2}\\
    \midrule
    Chest X-Ray8\cite{wang2017chestx,kaggle8} & 7.2k:7k:7k:4.1k :3.9k:3.5k:2.9k & 7 & 1024\textsuperscript{2}\\
    \bottomrule
  \end{tabular}
  \caption{Shortlisted Datasets.}
  %\vspace{-1.5em}
  \label{tab:selDataset}
\end{table}

\textbf{Datasets:} (\cref{tab:selDataset}) with varying disease types were chosen to ensure 
model robustness. Other criteria included the \textit{number of images per class} and 
\textit{image quality} as noisy scans can lead to mis-diagnosis\cite{sivakumar2012computed}. 

\begin{figure}
  \subfloat[COVID Dataset]{\includegraphics[width = 1\linewidth]{samp_img.eps}}\\
  \subfloat[Pneumonia Dataset]{\includegraphics[width = 1\linewidth]{abhishek_dataset_comparison.eps}}\\
  \subfloat[Chest X-Ray 8 Dataset]{\includegraphics[width = 1\linewidth]{rohan_dataset_comparison.eps}}
  \caption{Sample Chest X-rays from the datasets used.}
  \label{fig:sample_imgs}
  \end{figure}

The \textbf{COVID} dataset was created by a team of researchers from Qatar University, Doha, 
Qatar, and the University of Dhaka, Bangladesh along with collaborators from Pakistan 
and Malaysia in collaboration with medical doctors from the Italian Society of Medical 
and Interventional Radiology database using 43 different 
publications \cite{RAHMAN2021104319,9144185,kagglecovid}. It is a multiclass data with 
three classes, COVID, viral pneumonia and normal. X-rays with widespread, hazy, 
and irregular ground glass opacities are of the COVID-19 class \cite{jacobi2020portable}. 
Whereas, the ones with haziness only in the lower regions \cite{zhan2021clinical} are 
viral pneumonia cases as shown in \cref{fig:sample_imgs}. Chest X-rays of normal lungs provide 
a clear view of the lungs. The normal class was undersampled to use only 3.6k scans and reduce the 
data imbalance.

The \textbf{Pneumonia}, dataset contains scans from pediatric patients of one to five year olds 
collected as part of patients' routine clinical care at the Guangzhou Women and Children's 
Medical Center, Guangzhou, China. \cite{kermany2018labeled,kagglepneu} 
This dataset is multiclass with three classes, viral pneumonia, bacterial pneumonia and normal.
Scans with one white condensed area affecting only one 
side of the lungs are tagged as bacterial pneumonia\cite{areviral} as bacteria tends to 
aggressively attack one part of the lungs causing inflammation to replace the cells 
that were otherwise filled with air. On the other hand, X-rays which 
show bilateral patchy areas of consolidation are classified as viral pneumonia
\cite{guo2012radiological} as viruses attack both sides of the lungs producing a 
homogeneous inflammatory reaction causing mucus and cellular debris. Normal scans here as well 
produce a clear view of the lungs. 

NIH \cite{chestxray2017data} released over 100k anonymized chest X-ray images along with 
their radiological reports from over 30k patients. Wang \etal \cite{wang2017chestx} 
used this data to create the \textbf{Chest X-ray 8} dataset by generating disease labels 
through NLP fromt he radiological reports. \cite{kaggle8} The dataset contains 15 classes but only 
7 \cref{fig:sample_imgs} were chosen for this study. This dataset is significantly different from the 
other two as it is a multilabel dataset. Classes were iteratively removed, ensuring that
they are not highly imbalanced to finally reach the 7 classes. With over 29,000 images of size 
1024 x 1024, this dataset was the biggest and thus had to be resized down to 384 x 384 to 
reduce training and processing times. Furthermore, normal class images were undersampled by first
choosing one scan per patient and then selecting 7000 scans out of this subset randomly. 
The data consists of multiple scans from the same subject which could lead to data leakage between 
the train, val and test sets if a random train-test-val split was performed. 
This was prevented with the use of GroupShuffleSplit from the scikit library. 

\begin{figure}[t]
  \centering
  \includegraphics[width=0.7\linewidth]{pre_proc_img.eps}  
   \caption{Effect of pre-processing on Chest X-ray images.}
   %\vspace{-1em}
   \label{fig:pre_proc_img}
\end{figure}

Before training, all the images were pre-processed using histogram equalization and Gaussian blur 
with a 5x5 filter as Giełczyk \etal \cite{gielczyk2022pre} showed that this improved the 
F1 score by about 4\% for the chest X-ray classification task. Visually, the contrast of the scan improved 
and allowed irregularities to stand out as shown in \cref{fig:pre_proc_img}. Next, the 
scans were divided into train, validation and test with the 70:15:15 split. 
During training, the scans were augmented using RandomAdjustSharpness and 
RandomAutocontrast in Pytorch \cite{transforms} to increase the number of images the 
model gets to learn from and ensure that the model is robust to scans from different machines.
RandomHorizontalFlip was also used to make the models invarient to the direction of the scan as 
some scans were anterior-posterior while others were posterior-anterior \cite{botev2022regularising}.

\textbf{Backbone Architectures:} (\cref{tab:selArch}) of various configuration and blocks were chosen 
to ensure that different ideas are tested in this study. 
Other selection criteria were the \textit{number of trainable parameters}, important to keep track of 
the total training time, \textit{FLOPS} as models that could easily be 
deployed on to embedded devices were required and the \textit{top 5 classification accuracy} on the ImageNet 
1K benchmark dataset.

\begin{table}
  \centering
  \begin{tabular}{p{1.7cm}|p{1cm}|p{1cm}|p{1cm}|p{1cm}}
  \toprule
  Arch. & Params (Mil.) & Layers & FLOPS (Bil.) & Imagenet Acc.\\
  \midrule
  MobileNet & 5.5 & 18 & 8.7 & 92.6\\
  \midrule
  EfficientNet & 7.8 & 25 & 25.8 & 94.9\\
  \midrule
  Resnet & 21.8 & 34 & 153.9 & 91.4\\
  \bottomrule
  \end{tabular}
  \caption{Shortlisted Backbone Architectures.}
  %\vspace{-1.5em}
  \label{tab:selArch}
\end{table}

\textbf{ResNet 34} residual learning network with 34 layers that are made possible by skip 
connections. The 34 layer variant was chosen to decrease training time while not compromising on 
the accuracy much. This architecture had the highest trainable parameters and FLOPS while the lowest 
Imagenet accuracy.
\cite{he2016deep}

\textbf{MobileNet V3 Large} uses depthwise separable convolution from MobileNet V2 along with 
squeeze-excitation blocks in residual layers 
from MnasNet. This makes it really quick to train while still performing 
at par with other architectures. This architecture had the lowest trainable parameters and FLOPS 
among the three selected. Howard \etal \cite{howard2019searching} also used 
network architecture search to find the most effective model. The large configuration 
was chosen to not compromise on the prediction accuracy.

\textbf{EfficientNet B1} uses compound scaling to scale the model by depth, width and 
resolution. The B1 version was chosen to have faster training without compromising on the 
accuracy. \cite{tan2019efficientnet} This architecture performs the best among the selected 
on the Imagenet benchmark dataset while having a third of the trainable parameters of Resnet34.

\textbf{Optimization Algorithm:}
The Adam optimizer \cite{kingma2014adam} is an adaptive learning rate algorithm which 
was chosen as the algorithm of choice as it converges faster by integrating benefits of  
RMSProp and momentum. It is also robust to hyperparameters but, 
requires tweaking of the learning rate depending on the task at hand. For this study, 
a learning rate of 0.01 and the author recommend settings for $\beta_{1} =
0.9$, $\beta_{2} = 0.999$ and $\epsilon = 10^{-8}$ were used for the first and second order moment estimate 
as defined in \cref{eq:adam1} and \cref{eq:adam2} where $\beta_{1}$ and $\beta_{2}$ control the decay rates.
\begin{equation}
m_{t} = \beta_{1} \cdot m_{t-1} + (1 - \beta_{1}) \cdot g_{t}
\label{eq:adam1}
\end{equation}
\begin{equation}
  v_{t} = \beta_{2} \cdot v_{t-1} + (1 - \beta_{2}) \cdot g_{t}^{2}
  \label{eq:adam2}
  \end{equation}
Further, Cosine annealing \cite{loshchilov2016sgdr} learning rate scheduler was used 
to reduce the learning rate as the training progressed down to a low of 0.001. 

\section{Results} 
\label{sec:result}
\textbf{Experiment Setup:}

First undersampling was performed as described in \cref{sec:prop_method} on the datasets. Then,
the scans were pre-processed using histogram equalization and Gaussian blur before resizing them 
and storing them in separate directories to make it easier for PyTorch dataloaders. 
Two datasets in this study presented the multiclass classification problem while the third, 
chest X-ray 8 dataset presented the multiclass, multilabel classification problem. Thus, the 
training methodology was separated for these two problems. For the multilabel problem, 
a softmax layer had to be added before the loss function to get 0 or 1 prediction for 
all the classes of the data. For this, the BCEWITHLOGITSLOSS function of PyTorch was used 
as it combines the Sigmoid layer and the BCELoss function in one single class. This makes 
theses operations more numerically stable than their separate counterparts \cite{bcelogits}.
The backbone architectures were obtained directly from the torchvision library and the final 
classification layer was modified for the selected datasets. For the models which had to be 
trained from scratch, the weights were randomly initialized and the entire model was trained 
for a total of 100 epochs each. For the transfer learning models, the weights were initialized 
with the IMAGENET1K\_V2 weights but the entire model was fine-tuned. The rationale behind 
performing deep-tuning was that the Imagenet data is very different from chest X-ray scans 
thus the model would need to learn features from X-ray scans. 

The batch size was fixed to 32 for all the models. 
While training the best model by validation loss was saved to prevent the usage of overfit 
models for test set analysis. The actual and predicted results from each epoch was also stored 
to calculate the F1 score at each step of training. While calculating the F1 score, macro averaging 
was used to get an average score across classes. All images were normalized before getting trained 
with the mean and standard deviation of the training set of each of the selected datasets.

Initial runs of the multilabel data training produced 0 F1 score due to the highly imbalanced 
nature of multilabel classes. To mitigate this, class wise weights were calculated using the training 
data and used with the loss function. This improved the F1 score considerably. 

\begin{table*}[tbh]
  \centering
  \boldmath
  \scalebox{0.9}{%
  \begin{tabular}{l|ccc|ccc|ccc|ccc} \hline
    Model & \multicolumn{3}{c|}{\textbf{ResNet}} & \multicolumn{3}{c|}{\textbf{MobileNet}} & \multicolumn{3}{c|}{\textbf{EfficientNet}} & \multicolumn{3}{c}{\textbf{EN - Transfer Learning}} \\
  \cline{1-13}
  Dataset & \emph{F1} & \emph{Time} & \emph{Epoch} & \emph{F1} & \emph{Time} & \emph{Epoch} & \emph{F1} & \emph{Time} & \emph{Epoch} & \emph{F1} & \emph{Time} & \emph{Epoch}  \\
   \hline
  \emph{Pneumonia}           & 0.784 & 82 &  \textbf{22} &   \textbf{0.804} & \textbf{75} &  42 &   0.768 & 110 &  44 &   0.782 & 114 &  70  \\
  \hline
  \emph{COVID}        & 0.959 & 50 & \textbf{21} & 0.967 & \textbf{37} & 44 & \textbf{0.979} & 56 & 46 & 0.978 & 56 & 43   \\ % \hdashline

  \hline
  \emph{X-Ray 8} &   0.411 & 11,502 & \textbf{19} & 0.406 & \textbf{7,275} & 42 & 0.445 & 13,820 & 31 & \textbf{0.457} & 13,813 & 29   \\ \hline
  \end{tabular}
  }
  \caption{F1 (higher is better), time per epoch in seconds (lower is better), and number of epochs to reach the best validation loss (lower is better) for the 12 models that were trained.}
  \label{table:test_metrics}
  \end{table*}
  
  \begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{f1_loss.eps}  
     \caption{Train \& Val, F1 \& Loss plots for the 9 models.}
     %\vspace{-1em}
     \label{fig:acc_loss_sep}
  \end{figure}
  
  \begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{tl_f1_loss.eps}  
     \caption{Train \& Val, F1 \& Loss plots for EfficientNet trained from scratch and with ImageNet weights.}
     %\vspace{-1em}
     \label{fig:tl_acc_loss}
  \end{figure}

Finally, the best models from each run by validation loss were used to get the test set metrics that 
are displayed in \cref{table:test_metrics}. Training and validation F1 score and loss are also provided in 
\cref{fig:acc_loss_sep} and \cref{fig:tl_acc_loss}.


\textbf{Main Results:}

From \cref{fig:acc_loss_sep} it is clear that going from a smaller 
architecture to a bigger architecture, makes the model start to overfit earlier. 
The MobileNet model was the most unstable among the three and also took more epochs to reach 
the minima. The EfficientNet algorithm performs best for the COVID and Chest X-ray 8 
dataset and all three architectures performed similar for the pneumonia dataset. 
This shows that the compound scaline of EfficientNet gives good results for chest X-ray data.
The X-ray 8 dataset performed the worst among the three datasets which could be due to the 
high number of classes, class imbalance and the multilabel nature of the problem. 
Surprisingly, the pneumonia dataset performed worse than the COVID + pneumonia dataset 
which indicates that COVID cases are easier to distinguish from pneumonia cases. 

In \cref{fig:tl_acc_loss} it can be seen that the transfer learning model had a much better start than 
the randomly initialized model. It also converged much quicker than the model trained from scratch.
For the Pneumonia dataset, the model trained from scratch was highly unstable at the start 
and could not catch up to the transfer learning model even after 100 epochs in terms of the F1 score.

Finally, looking at \cref{table:test_metrics} it can be seen that the MobileNet architecture was 
the fastest to train per epoch. It consistantly took less time per epoch but, if number of 
epochs required to converge is considered, it does not train the fastest all the time.
It is also evident that ResNet converged the fastest at half the number of epochs compared with other models. 
EfficientNet models perform the best in terms of the overall F1 score on the test set with the exception of 
the Pneumonia dataset where surprisingly 
MobileNet performed the best. The transfer learning models converged quicker 
than the other models with the exception of the Pneumonia dataset. Another surprising observation 
is that the EfficientNet model takes the longest to train per epoch even though the number of trainable 
parameters is nowhere close to ResNet. Also, MobileNet isn't as fast to train as expected when 
compared to ResNet even though it has 4 times the learnable paramenters. This could be due to two 
reasons, depthwise convolutions are not optimized in the version of PyTorch and CUDA used and 
training is getting CPU bound due to the data augmentation before each training run which 
would take the same amount of time for all the models. 

\begin{figure}
  \subfloat[Resnet]{\includegraphics[width = 0.5\linewidth]{tsne_pneumonia_resnet.eps}}
  \subfloat[MobileNet]{\includegraphics[width = 0.5\linewidth]{tsne_pneumonia_mobilenet.eps}}\\
  \subfloat[EfficientNet]{\includegraphics[width = 0.5\linewidth]{tsne_pneumonia_efficientnet.eps}}
  \subfloat[EN - Transfer Learning]{\includegraphics[width = 0.5\linewidth]{tsne_pneumonia_efficientnet_tl.eps}}
  \caption{T-SNE and Confusion matrics for the test set of the Pneumonia dataset.}
  \label{fig:tsne_pneumonia}
\end{figure}


\cref{fig:tsne_pneumonia} shows that the models are able to differentiate well between the normal and pneumonia 
classes but struggle with the viral pneumonia vs bacterial pneumonia classification. MobileNet performs better 
but the EfficientNet transfer learning model creates better separation of classes. Thus, even though 
MobileNet performs better in this case, 
the EfficientNet transfer learning model would generalize well on new unseen data. This is correlated in 
the confusion matrix where the transfer learning and MobileNet models perform the best.

\begin{figure}
  \subfloat[Resnet]{\includegraphics[width = 0.5\linewidth]{tsne_covid_pneumonia_resnet.eps}}
  \subfloat[MobileNet]{\includegraphics[width = 0.5\linewidth]{tsne_covid_pneumonia_mobilenetnet.eps}}\\
  \subfloat[EfficientNet]{\includegraphics[width = 0.5\linewidth]{tsne_covid_pneumonia_efficientnet.eps}}
  \subfloat[EN - Transfer Learning]{\includegraphics[width = 0.5\linewidth]{tsne_covid_pneumonia_efficientnet_tl.eps}}
  \caption{T-SNE and Confusion matrics for the test set of the COVID dataset.}
  \label{fig:tsne_covid}
\end{figure}

\cref{fig:tsne_covid} shows that all models do a good job of separating classes to create distinct clusters 
but, the transfer learning model creates better clusters with separate smaller clusters. These smaller 
clusters could indicate other factors of the disease, for example the severity and amount of lung damage caused 
by the disease. This performance of the transfer learning model can be confirmed by looking at the confusion 
matrix as well.



\begin{figure}
  \subfloat[Bacterial Pneumonia]{\includegraphics[width = 1\linewidth]{gradcam_pneumonia_1_.eps}}\\
  \subfloat[Viral Pneumonia]{\includegraphics[width = 1\linewidth]{gradcam_pneumonia_2_.eps}}\\
  \subfloat[Normal]{\includegraphics[width = 1\linewidth]{gradcam_pneumonia_0_.eps}}
  \caption{GradCAM visualization for the Pneumonia dataset.}
  \label{fig:gradcam_pneumonia}
\end{figure}

\cref{fig:gradcam_pneumonia} shows the gradCAM visualization of the last layer of the convolutional network.
Here it can be seen that the ResNet is learning completely different features as compared to the other 
models. This could be the reason for its low performance. In case of bacterial pneumonia, the network identifies 
affected area on the right side of the scan. On the other hand, in case of viral pneumonia, models look at both 
sides of the lungs. This correlates with the actual progrssion of these diseases as given in \cref{sec:prop_method}.



\begin{figure}
  \subfloat[COVID]{\includegraphics[width = 1\linewidth]{gradcam_covid_pneumonia_1.eps}}\\
  \subfloat[Pneumonia]{\includegraphics[width = 1\linewidth]{gradcam_covid_pneumonia_3.eps}}\\
  \subfloat[Normal]{\includegraphics[width = 1\linewidth]{gradcam_covid_pneumonia_2.eps}}
  \caption{GradCAM visualization for the COVID dataset.}
  \label{fig:gradcam_covid}
\end{figure}

\cref{fig:gradcam_covid} shows that MobileNet activates the entire image incase of COVID, this be the 
reason for its low performance. In case of pneumonia, the EfficientNet models identifies 
affected areas on the bootom of the lungs. On the other hand, in case of COVID, the models look at a bigger 
region of the lungs. This correlates with how these diseases impact the lungs as given in \cref{sec:prop_method}.




\textbf{Ablative Study:}
\begin{figure}[t]
  \centering
  \includegraphics[width=1\linewidth]{ablation_plot.eps}  
   \caption{Train \& Val, F1 \& Loss plots for ablative study models.}
   %\vspace{-1em}
   \label{fig:ablation_plot}
\end{figure}
For the ablative study, the COVID dataset was chosen along with the EfficientNet B1 architecture 
trained from scratch. The learning rates chosen for the study are 0.001, 0.005, 0.01, 0.05, and 0.1.
From the training and validation F1 score and loss plots given in 
\cref{fig:ablation_plot} it is seen that a very high learning rate of 0.1 is highly unstable and 
prevents the model from reaching close to global minima. Similarly, learning rate of 0.05 also prevented 
the model from converging on the validation set even after 100 epochs. The other three learning 
rates all converged on the validation set but, the learning rate of 0.001 was the most stable 
and reached the highest F1 score earliest. On the other hand, learning rate of 0.01 performed marginally 
better on the loss plot.
\begin{figure}[t]
  \centering
  \includegraphics[width=1\linewidth]{tl_ablation_f1.eps}  
   \caption{Ablative Study F1 scores (Higher is better).}
   %\vspace{-1em}
   \label{fig:ablation}
\end{figure}
From \cref{fig:ablation} it can be seen that the best performing learning rate is 0.001 
on the F1 score of the test set with 0.005, 0.01 close second and 0.05, 0.1 performing the worst.
This matches the results of the validation set on \cref{fig:ablation_plot}. Thus, a learning rate of 
0.001 performs the best on the COVID dataset with the transfer learning EfficientNet model.


%%%%%%%%% REFERENCES
\clearpage
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}
\end{document}

\begin{figure}
  \subfloat[Atelectasis]{\includegraphics[width = 0.25\linewidth]{tsne_xray8_efficientnet_tl_Atelectasis.eps}}
  \subfloat[Consolidation]{\includegraphics[width = 0.25\linewidth]{tsne_xray8_efficientnet_tl_Consolidation.eps}}
  \subfloat[Effusion]{\includegraphics[width = 0.25\linewidth]{tsne_xray8_efficientnet_tl_Effusion.eps}}
  \subfloat[Pneumothorax]{\includegraphics[width = 0.25\linewidth]{tsne_xray8_efficientnet_tl_No_findingPneumothorax.eps}}\\
  \subfloat[Mass]{\includegraphics[width = 0.3\linewidth]{tsne_xray8_efficientnet_tl_Mass.eps}}
  \subfloat[Nodule]{\includegraphics[width = 0.3\linewidth]{tsne_xray8_efficientnet_tl_Nodule.eps}}
  \subfloat[Normal]{\includegraphics[width = 0.3\linewidth]{tsne_xray8_efficientnet_tl_No_finding.eps}}
  \caption{Confusion matrics of the Chest X-ray 8 dataset.}
  \label{fig:cm_xray8}
\end{figure}


\begin{figure}
  \subfloat[Resnet]{\includegraphics[width = 0.5\linewidth]{tsne_xray8_resnet.eps}}
  \subfloat[MobileNet]{\includegraphics[width = 0.5\linewidth]{tsne_xray8_mobilenet.eps}}\\
  \subfloat[EfficientNet]{\includegraphics[width = 0.5\linewidth]{tsne_xray8_efficientnet.eps}}
  \subfloat[EN - Transfer Learning]{\includegraphics[width = 0.5\linewidth]{tsne_xray8_efficientnet_tl.eps}}
  \caption{T-SNE and Confusion matrics of the Chest X-ray 8 dataset.}
  \label{fig:tsne_xray8}
\end{figure}

The t-SNE plots for the Chest X-ray 8 dataset is quite different as it is a multi label. Thus, for each class 
a different plot is generated. From \cref{fig:tsne_xray8} it can be seen that for the class Effusion, 
