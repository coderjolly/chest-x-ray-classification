{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: \n",
    "## 1. Get mean and std of dataset - done\n",
    "## 2. Write a script to plot loss + accuracy graph\n",
    "## 3. Get FLOPs - done\n",
    "## 4. Get num layers - done\n",
    "## ----------------------------------------------------\n",
    "## 1. Implement differentiable F1 loss function\n",
    "## 2. Add class weights\n",
    "## 3. Implement transfer learning part - done\n",
    "## 4. Implement T-SNE\n",
    "## 5. Implement gradcam\n",
    "## 6. Ablation study\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from train import train_model\n",
    "from model import initialize_model\n",
    "from utils import set_requires_grad, save_model\n",
    "from data import load_data\n",
    "from plotting import plot_data_loader\n",
    "from eval import eval_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0w24yf-Tj47H"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from numpy.random import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.autograd.profiler as tprofiler\n",
    "import torch.utils.data as td\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "O7W8BTtF3BN1"
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "\n",
    "# pytorch RNGs\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# numpy RNG\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "21_bts2Wj47M",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = \"../../data\"\n",
    "images_dir = \"../../data/chest_xray\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nr7fQfkuj47u"
   },
   "outputs": [],
   "source": [
    "# Get best num_workers\n",
    "# for i in range(97):\n",
    "#     start = time.time()\n",
    "#     data_loader = load_data(images_dir,\n",
    "#                                                                    batch_size = 96, \n",
    "#                                                                    input_size = 299, \n",
    "#                                                                    norm_arr = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "#                                                                    num_workers = i)\n",
    "#     iter(data_loader['train']).next()[0].shape\n",
    "#     print(f\"{i}: {time.time()-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96wB0P9Gj47u"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k-vpcOXE1pmg",
    "outputId": "25752964-a425-490c-d154-0cc8baab3b61"
   },
   "outputs": [],
   "source": [
    "# Models options: resnet50, resnet34, inceptionv3, vgg16, mobile_net_v3_large, efficient_net_b1, efficient_net_b0.\n",
    "model_name = \"mobile_net_v3_large\"\n",
    "\n",
    "# Number of classes.\n",
    "num_classes = 3\n",
    "\n",
    "# Batch Size.\n",
    "batch_size = 32\n",
    "\n",
    "# Epochs to train for.\n",
    "num_epochs = 100\n",
    "\n",
    "# Number of workers for data loader.\n",
    "num_workers = 0\n",
    "\n",
    "# Imagenet norm array passed as default value.\n",
    "# norm_arr=([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "# Chest x-ray8 training dataset metrics \n",
    "# norm_arr=([0.4951, 0.4951, 0.4951], [0.2896, 0.2896, 0.2896])\n",
    "# Pneumonia dataset metrics\n",
    "norm_arr=([0.4810, 0.4810, 0.4810], [0.2373, 0.2373, 0.2373])\n",
    "\n",
    "# Feature extract flag: False - Tune the whole model,\n",
    "#                       True - Update only the reshaped layer parameters.\n",
    "feature_extract = False\n",
    "\n",
    "# Use pretrained flag: None - Use random weights\n",
    "#                      String - Use pretrained weights given by String\n",
    "use_pretrained = None\n",
    "\n",
    "# Initialize the model for this run.\n",
    "model_pyt, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=use_pretrained)\n",
    "\n",
    "# lr start and end points for training.\n",
    "lr_start = 0.01\n",
    "lr_end = 0.001\n",
    "\n",
    "# How many epochs to restart.\n",
    "iter_restart = 10\n",
    "\n",
    "# Multiplication factor after restart.\n",
    "mul_restart = 1\n",
    "\n",
    "# Print the model we just instantiated\n",
    "#print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "e9FOwaso3LAc"
   },
   "outputs": [],
   "source": [
    "data_loaders = load_data(images_dir,\n",
    "                         batch_size = batch_size, \n",
    "                         input_size = (input_size, input_size), \n",
    "                         norm_arr = norm_arr,\n",
    "                         num_workers = num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "vacZgHSCj47u",
    "outputId": "35a65cef-1d6d-4657-ff01-be15854ca24b"
   },
   "source": [
    "plot_data_loader(data_loader['train'], (2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRIOYWbV1cnS"
   },
   "source": [
    "plot_data_loader(data_loader['test'], (2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znBg5tkd1dXF"
   },
   "source": [
    "plot_data_loader(data_loader['val'], (2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "0rj7Qeg41wLm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Params to learn:\n",
      "\t features.0.0.weight\n",
      "\t features.0.1.weight\n",
      "\t features.0.1.bias\n",
      "\t features.1.block.0.0.weight\n",
      "\t features.1.block.0.1.weight\n",
      "\t features.1.block.0.1.bias\n",
      "\t features.1.block.1.0.weight\n",
      "\t features.1.block.1.1.weight\n",
      "\t features.1.block.1.1.bias\n",
      "\t features.2.block.0.0.weight\n",
      "\t features.2.block.0.1.weight\n",
      "\t features.2.block.0.1.bias\n",
      "\t features.2.block.1.0.weight\n",
      "\t features.2.block.1.1.weight\n",
      "\t features.2.block.1.1.bias\n",
      "\t features.2.block.2.0.weight\n",
      "\t features.2.block.2.1.weight\n",
      "\t features.2.block.2.1.bias\n",
      "\t features.3.block.0.0.weight\n",
      "\t features.3.block.0.1.weight\n",
      "\t features.3.block.0.1.bias\n",
      "\t features.3.block.1.0.weight\n",
      "\t features.3.block.1.1.weight\n",
      "\t features.3.block.1.1.bias\n",
      "\t features.3.block.2.0.weight\n",
      "\t features.3.block.2.1.weight\n",
      "\t features.3.block.2.1.bias\n",
      "\t features.4.block.0.0.weight\n",
      "\t features.4.block.0.1.weight\n",
      "\t features.4.block.0.1.bias\n",
      "\t features.4.block.1.0.weight\n",
      "\t features.4.block.1.1.weight\n",
      "\t features.4.block.1.1.bias\n",
      "\t features.4.block.2.fc1.weight\n",
      "\t features.4.block.2.fc1.bias\n",
      "\t features.4.block.2.fc2.weight\n",
      "\t features.4.block.2.fc2.bias\n",
      "\t features.4.block.3.0.weight\n",
      "\t features.4.block.3.1.weight\n",
      "\t features.4.block.3.1.bias\n",
      "\t features.5.block.0.0.weight\n",
      "\t features.5.block.0.1.weight\n",
      "\t features.5.block.0.1.bias\n",
      "\t features.5.block.1.0.weight\n",
      "\t features.5.block.1.1.weight\n",
      "\t features.5.block.1.1.bias\n",
      "\t features.5.block.2.fc1.weight\n",
      "\t features.5.block.2.fc1.bias\n",
      "\t features.5.block.2.fc2.weight\n",
      "\t features.5.block.2.fc2.bias\n",
      "\t features.5.block.3.0.weight\n",
      "\t features.5.block.3.1.weight\n",
      "\t features.5.block.3.1.bias\n",
      "\t features.6.block.0.0.weight\n",
      "\t features.6.block.0.1.weight\n",
      "\t features.6.block.0.1.bias\n",
      "\t features.6.block.1.0.weight\n",
      "\t features.6.block.1.1.weight\n",
      "\t features.6.block.1.1.bias\n",
      "\t features.6.block.2.fc1.weight\n",
      "\t features.6.block.2.fc1.bias\n",
      "\t features.6.block.2.fc2.weight\n",
      "\t features.6.block.2.fc2.bias\n",
      "\t features.6.block.3.0.weight\n",
      "\t features.6.block.3.1.weight\n",
      "\t features.6.block.3.1.bias\n",
      "\t features.7.block.0.0.weight\n",
      "\t features.7.block.0.1.weight\n",
      "\t features.7.block.0.1.bias\n",
      "\t features.7.block.1.0.weight\n",
      "\t features.7.block.1.1.weight\n",
      "\t features.7.block.1.1.bias\n",
      "\t features.7.block.2.0.weight\n",
      "\t features.7.block.2.1.weight\n",
      "\t features.7.block.2.1.bias\n",
      "\t features.8.block.0.0.weight\n",
      "\t features.8.block.0.1.weight\n",
      "\t features.8.block.0.1.bias\n",
      "\t features.8.block.1.0.weight\n",
      "\t features.8.block.1.1.weight\n",
      "\t features.8.block.1.1.bias\n",
      "\t features.8.block.2.0.weight\n",
      "\t features.8.block.2.1.weight\n",
      "\t features.8.block.2.1.bias\n",
      "\t features.9.block.0.0.weight\n",
      "\t features.9.block.0.1.weight\n",
      "\t features.9.block.0.1.bias\n",
      "\t features.9.block.1.0.weight\n",
      "\t features.9.block.1.1.weight\n",
      "\t features.9.block.1.1.bias\n",
      "\t features.9.block.2.0.weight\n",
      "\t features.9.block.2.1.weight\n",
      "\t features.9.block.2.1.bias\n",
      "\t features.10.block.0.0.weight\n",
      "\t features.10.block.0.1.weight\n",
      "\t features.10.block.0.1.bias\n",
      "\t features.10.block.1.0.weight\n",
      "\t features.10.block.1.1.weight\n",
      "\t features.10.block.1.1.bias\n",
      "\t features.10.block.2.0.weight\n",
      "\t features.10.block.2.1.weight\n",
      "\t features.10.block.2.1.bias\n",
      "\t features.11.block.0.0.weight\n",
      "\t features.11.block.0.1.weight\n",
      "\t features.11.block.0.1.bias\n",
      "\t features.11.block.1.0.weight\n",
      "\t features.11.block.1.1.weight\n",
      "\t features.11.block.1.1.bias\n",
      "\t features.11.block.2.fc1.weight\n",
      "\t features.11.block.2.fc1.bias\n",
      "\t features.11.block.2.fc2.weight\n",
      "\t features.11.block.2.fc2.bias\n",
      "\t features.11.block.3.0.weight\n",
      "\t features.11.block.3.1.weight\n",
      "\t features.11.block.3.1.bias\n",
      "\t features.12.block.0.0.weight\n",
      "\t features.12.block.0.1.weight\n",
      "\t features.12.block.0.1.bias\n",
      "\t features.12.block.1.0.weight\n",
      "\t features.12.block.1.1.weight\n",
      "\t features.12.block.1.1.bias\n",
      "\t features.12.block.2.fc1.weight\n",
      "\t features.12.block.2.fc1.bias\n",
      "\t features.12.block.2.fc2.weight\n",
      "\t features.12.block.2.fc2.bias\n",
      "\t features.12.block.3.0.weight\n",
      "\t features.12.block.3.1.weight\n",
      "\t features.12.block.3.1.bias\n",
      "\t features.13.block.0.0.weight\n",
      "\t features.13.block.0.1.weight\n",
      "\t features.13.block.0.1.bias\n",
      "\t features.13.block.1.0.weight\n",
      "\t features.13.block.1.1.weight\n",
      "\t features.13.block.1.1.bias\n",
      "\t features.13.block.2.fc1.weight\n",
      "\t features.13.block.2.fc1.bias\n",
      "\t features.13.block.2.fc2.weight\n",
      "\t features.13.block.2.fc2.bias\n",
      "\t features.13.block.3.0.weight\n",
      "\t features.13.block.3.1.weight\n",
      "\t features.13.block.3.1.bias\n",
      "\t features.14.block.0.0.weight\n",
      "\t features.14.block.0.1.weight\n",
      "\t features.14.block.0.1.bias\n",
      "\t features.14.block.1.0.weight\n",
      "\t features.14.block.1.1.weight\n",
      "\t features.14.block.1.1.bias\n",
      "\t features.14.block.2.fc1.weight\n",
      "\t features.14.block.2.fc1.bias\n",
      "\t features.14.block.2.fc2.weight\n",
      "\t features.14.block.2.fc2.bias\n",
      "\t features.14.block.3.0.weight\n",
      "\t features.14.block.3.1.weight\n",
      "\t features.14.block.3.1.bias\n",
      "\t features.15.block.0.0.weight\n",
      "\t features.15.block.0.1.weight\n",
      "\t features.15.block.0.1.bias\n",
      "\t features.15.block.1.0.weight\n",
      "\t features.15.block.1.1.weight\n",
      "\t features.15.block.1.1.bias\n",
      "\t features.15.block.2.fc1.weight\n",
      "\t features.15.block.2.fc1.bias\n",
      "\t features.15.block.2.fc2.weight\n",
      "\t features.15.block.2.fc2.bias\n",
      "\t features.15.block.3.0.weight\n",
      "\t features.15.block.3.1.weight\n",
      "\t features.15.block.3.1.bias\n",
      "\t features.16.0.weight\n",
      "\t features.16.1.weight\n",
      "\t features.16.1.bias\n",
      "\t classifier.0.weight\n",
      "\t classifier.0.bias\n",
      "\t classifier.3.weight\n",
      "\t classifier.3.bias\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Send model to GPU\n",
    "model_pyt = model_pyt.to(device)\n",
    "\n",
    "# Find parameters to be updated in this run.\n",
    "# parameters with requires_grad = True.\n",
    "params_to_update = model_pyt.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_pyt.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_pyt.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "A4B1-pfYm0Ea"
   },
   "outputs": [],
   "source": [
    "# 17 min 1 epoch - 128 batch size - inception\n",
    "# Efficientnet b0 - batch 96 - epoch 50 - num_workers 2 - flip, auto cont, sharp - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8wBR8vcG2jcd",
    "outputId": "7d01aa07-d235-4cb6-dfaf-53ce0c5a577d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "----------\n",
      "Epoch [1/100], Step [100/129], Loss: 1.0218, Accuracy: 17.00%\n",
      "train Loss: 1.2388 Acc: 0.5333\n",
      "val Loss: 1.4020 Acc: 0.2965\n",
      "\n",
      "Epoch 2/100\n",
      "----------\n",
      "Epoch [2/100], Step [100/129], Loss: 0.6817, Accuracy: 23.00%\n",
      "train Loss: 0.7713 Acc: 0.6496\n",
      "val Loss: 0.9066 Acc: 0.5633\n",
      "\n",
      "Epoch 3/100\n",
      "----------\n",
      "Epoch [3/100], Step [100/129], Loss: 0.7783, Accuracy: 20.00%\n",
      "train Loss: 0.6632 Acc: 0.6946\n",
      "val Loss: 0.8463 Acc: 0.6739\n",
      "\n",
      "Epoch 4/100\n",
      "----------\n",
      "Epoch [4/100], Step [100/129], Loss: 0.8705, Accuracy: 18.00%\n",
      "train Loss: 0.6379 Acc: 0.7162\n",
      "val Loss: 0.7065 Acc: 0.7298\n",
      "\n",
      "Epoch 5/100\n",
      "----------\n",
      "Epoch [5/100], Step [100/129], Loss: 0.5123, Accuracy: 25.00%\n",
      "train Loss: 0.6072 Acc: 0.7284\n",
      "val Loss: 0.8175 Acc: 0.6511\n",
      "\n",
      "Epoch 6/100\n",
      "----------\n",
      "Epoch [6/100], Step [100/129], Loss: 0.9863, Accuracy: 23.00%\n",
      "train Loss: 0.5749 Acc: 0.7507\n",
      "val Loss: 0.9046 Acc: 0.6477\n",
      "\n",
      "Epoch 7/100\n",
      "----------\n",
      "Epoch [7/100], Step [100/129], Loss: 0.6640, Accuracy: 22.00%\n",
      "train Loss: 0.6035 Acc: 0.7500\n",
      "val Loss: 1.5405 Acc: 0.5439\n",
      "\n",
      "Epoch 8/100\n",
      "----------\n",
      "Epoch [8/100], Step [100/129], Loss: 0.7164, Accuracy: 22.00%\n",
      "train Loss: 0.5792 Acc: 0.7549\n",
      "val Loss: 2.2895 Acc: 0.4949\n",
      "\n",
      "Epoch 9/100\n",
      "----------\n",
      "Epoch [9/100], Step [100/129], Loss: 0.5534, Accuracy: 25.00%\n",
      "train Loss: 0.5882 Acc: 0.7493\n",
      "val Loss: 0.7364 Acc: 0.6454\n",
      "\n",
      "Epoch 10/100\n",
      "----------\n",
      "Epoch [10/100], Step [100/129], Loss: 0.6121, Accuracy: 21.00%\n",
      "train Loss: 0.5557 Acc: 0.7673\n",
      "val Loss: 0.6725 Acc: 0.7218\n",
      "\n",
      "Epoch 11/100\n",
      "----------\n",
      "Epoch [11/100], Step [100/129], Loss: 0.3955, Accuracy: 29.00%\n",
      "train Loss: 0.5607 Acc: 0.7634\n",
      "val Loss: 1.1767 Acc: 0.5884\n",
      "\n",
      "Epoch 12/100\n",
      "----------\n",
      "Epoch [12/100], Step [100/129], Loss: 0.6210, Accuracy: 26.00%\n",
      "train Loss: 0.5414 Acc: 0.7794\n",
      "val Loss: 0.8856 Acc: 0.7491\n",
      "\n",
      "Epoch 13/100\n",
      "----------\n",
      "Epoch [13/100], Step [100/129], Loss: 0.9584, Accuracy: 18.00%\n",
      "train Loss: 0.5360 Acc: 0.7741\n",
      "val Loss: 0.8270 Acc: 0.6739\n",
      "\n",
      "Epoch 14/100\n",
      "----------\n",
      "Epoch [14/100], Step [100/129], Loss: 0.6492, Accuracy: 23.00%\n",
      "train Loss: 0.5194 Acc: 0.7913\n",
      "val Loss: 0.5673 Acc: 0.7526\n",
      "\n",
      "Epoch 15/100\n",
      "----------\n",
      "Epoch [15/100], Step [100/129], Loss: 0.6997, Accuracy: 22.00%\n",
      "train Loss: 0.5349 Acc: 0.7777\n",
      "val Loss: 1.0578 Acc: 0.6021\n",
      "\n",
      "Epoch 16/100\n",
      "----------\n",
      "Epoch [16/100], Step [100/129], Loss: 0.4061, Accuracy: 25.00%\n",
      "train Loss: 0.5350 Acc: 0.7646\n",
      "val Loss: 2.5184 Acc: 0.5165\n",
      "\n",
      "Epoch 17/100\n",
      "----------\n",
      "Epoch [17/100], Step [100/129], Loss: 0.5289, Accuracy: 25.00%\n",
      "train Loss: 0.5224 Acc: 0.7809\n",
      "val Loss: 0.9262 Acc: 0.6899\n",
      "\n",
      "Epoch 18/100\n",
      "----------\n",
      "Epoch [18/100], Step [100/129], Loss: 0.4947, Accuracy: 27.00%\n",
      "train Loss: 0.4945 Acc: 0.7996\n",
      "val Loss: 0.8651 Acc: 0.6522\n",
      "\n",
      "Epoch 19/100\n",
      "----------\n",
      "Epoch [19/100], Step [100/129], Loss: 0.3093, Accuracy: 29.00%\n",
      "train Loss: 0.4883 Acc: 0.7964\n",
      "val Loss: 0.7160 Acc: 0.6693\n",
      "\n",
      "Epoch 20/100\n",
      "----------\n",
      "Epoch [20/100], Step [100/129], Loss: 0.5009, Accuracy: 25.00%\n",
      "train Loss: 0.5018 Acc: 0.7865\n",
      "val Loss: 0.5913 Acc: 0.7412\n",
      "\n",
      "Epoch 21/100\n",
      "----------\n",
      "Epoch [21/100], Step [100/129], Loss: 0.6256, Accuracy: 24.00%\n",
      "train Loss: 0.4915 Acc: 0.7989\n",
      "val Loss: 1.1089 Acc: 0.4994\n",
      "\n",
      "Epoch 22/100\n",
      "----------\n",
      "Epoch [22/100], Step [100/129], Loss: 0.6264, Accuracy: 24.00%\n",
      "train Loss: 0.4887 Acc: 0.8016\n",
      "val Loss: 0.5254 Acc: 0.7583\n",
      "\n",
      "Epoch 23/100\n",
      "----------\n",
      "Epoch [23/100], Step [100/129], Loss: 0.5390, Accuracy: 25.00%\n",
      "train Loss: 0.4649 Acc: 0.8069\n",
      "val Loss: 0.5997 Acc: 0.7594\n",
      "\n",
      "Epoch 24/100\n",
      "----------\n",
      "Epoch [24/100], Step [100/129], Loss: 0.2911, Accuracy: 28.00%\n",
      "train Loss: 0.4739 Acc: 0.8016\n",
      "val Loss: 0.6884 Acc: 0.6853\n",
      "\n",
      "Epoch 25/100\n",
      "----------\n",
      "Epoch [25/100], Step [100/129], Loss: 0.4022, Accuracy: 27.00%\n",
      "train Loss: 0.4472 Acc: 0.8052\n",
      "val Loss: 0.6069 Acc: 0.7127\n",
      "\n",
      "Epoch 26/100\n",
      "----------\n",
      "Epoch [26/100], Step [100/129], Loss: 0.6729, Accuracy: 18.00%\n",
      "train Loss: 0.4666 Acc: 0.8096\n",
      "val Loss: 0.7657 Acc: 0.7127\n",
      "\n",
      "Epoch 27/100\n",
      "----------\n",
      "Epoch [27/100], Step [100/129], Loss: 0.4936, Accuracy: 24.00%\n",
      "train Loss: 0.4674 Acc: 0.8013\n",
      "val Loss: 0.6445 Acc: 0.7560\n",
      "\n",
      "Epoch 28/100\n",
      "----------\n",
      "Epoch [28/100], Step [100/129], Loss: 0.3992, Accuracy: 26.00%\n",
      "train Loss: 0.4450 Acc: 0.8132\n",
      "val Loss: 0.5710 Acc: 0.7298\n",
      "\n",
      "Epoch 29/100\n",
      "----------\n",
      "Epoch [29/100], Step [100/129], Loss: 0.4343, Accuracy: 27.00%\n",
      "train Loss: 0.4602 Acc: 0.8071\n",
      "val Loss: 0.7082 Acc: 0.6306\n",
      "\n",
      "Epoch 30/100\n",
      "----------\n",
      "Epoch [30/100], Step [100/129], Loss: 0.6413, Accuracy: 23.00%\n",
      "train Loss: 0.4428 Acc: 0.8106\n",
      "val Loss: 0.8394 Acc: 0.6750\n",
      "\n",
      "Epoch 31/100\n",
      "----------\n",
      "Epoch [31/100], Step [100/129], Loss: 0.4654, Accuracy: 26.00%\n",
      "train Loss: 0.4292 Acc: 0.8242\n",
      "val Loss: 0.8026 Acc: 0.6625\n",
      "\n",
      "Epoch 32/100\n",
      "----------\n",
      "Epoch [32/100], Step [100/129], Loss: 0.7951, Accuracy: 21.00%\n",
      "train Loss: 0.4707 Acc: 0.7974\n",
      "val Loss: 1.0802 Acc: 0.6454\n",
      "\n",
      "Epoch 33/100\n",
      "----------\n",
      "Epoch [33/100], Step [100/129], Loss: 0.3838, Accuracy: 25.00%\n",
      "train Loss: 0.4601 Acc: 0.8106\n",
      "val Loss: 1.3365 Acc: 0.5222\n",
      "\n",
      "Epoch 34/100\n",
      "----------\n",
      "Epoch [34/100], Step [100/129], Loss: 0.6269, Accuracy: 24.00%\n",
      "train Loss: 0.4207 Acc: 0.8251\n",
      "val Loss: 0.6278 Acc: 0.7583\n",
      "\n",
      "Epoch 35/100\n",
      "----------\n",
      "Epoch [35/100], Step [100/129], Loss: 0.4330, Accuracy: 25.00%\n",
      "train Loss: 0.4098 Acc: 0.8303\n",
      "val Loss: 0.5284 Acc: 0.7617\n",
      "\n",
      "Epoch 36/100\n",
      "----------\n",
      "Epoch [36/100], Step [100/129], Loss: 0.3077, Accuracy: 28.00%\n",
      "train Loss: 0.4153 Acc: 0.8251\n",
      "val Loss: 0.4986 Acc: 0.7982\n",
      "\n",
      "Epoch 37/100\n",
      "----------\n",
      "Epoch [37/100], Step [100/129], Loss: 0.4537, Accuracy: 25.00%\n",
      "train Loss: 0.4096 Acc: 0.8273\n",
      "val Loss: 0.6821 Acc: 0.6830\n",
      "\n",
      "Epoch 38/100\n",
      "----------\n",
      "Epoch [38/100], Step [100/129], Loss: 0.3807, Accuracy: 26.00%\n",
      "train Loss: 0.4078 Acc: 0.8290\n",
      "val Loss: 0.5301 Acc: 0.7868\n",
      "\n",
      "Epoch 39/100\n",
      "----------\n",
      "Epoch [39/100], Step [100/129], Loss: 0.4569, Accuracy: 26.00%\n",
      "train Loss: 0.4175 Acc: 0.8276\n",
      "val Loss: 0.5748 Acc: 0.7708\n",
      "\n",
      "Epoch 40/100\n",
      "----------\n",
      "Epoch [40/100], Step [100/129], Loss: 0.3607, Accuracy: 28.00%\n",
      "train Loss: 0.4017 Acc: 0.8351\n",
      "val Loss: 0.4987 Acc: 0.8062\n",
      "\n",
      "Epoch 41/100\n",
      "----------\n",
      "Epoch [41/100], Step [100/129], Loss: 0.5281, Accuracy: 24.00%\n",
      "train Loss: 0.3953 Acc: 0.8329\n",
      "val Loss: 0.6692 Acc: 0.7081\n",
      "\n",
      "Epoch 42/100\n",
      "----------\n",
      "Epoch [42/100], Step [100/129], Loss: 0.4799, Accuracy: 25.00%\n",
      "train Loss: 0.3920 Acc: 0.8380\n",
      "val Loss: 0.5166 Acc: 0.8084\n",
      "\n",
      "Epoch 43/100\n",
      "----------\n",
      "Epoch [43/100], Step [100/129], Loss: 0.4262, Accuracy: 26.00%\n",
      "train Loss: 0.4010 Acc: 0.8300\n",
      "val Loss: 0.9245 Acc: 0.6990\n",
      "\n",
      "Epoch 44/100\n",
      "----------\n",
      "Epoch [44/100], Step [100/129], Loss: 0.4009, Accuracy: 26.00%\n",
      "train Loss: 0.3845 Acc: 0.8395\n",
      "val Loss: 0.5792 Acc: 0.7913\n",
      "\n",
      "Epoch 45/100\n",
      "----------\n",
      "Epoch [45/100], Step [100/129], Loss: 0.4945, Accuracy: 23.00%\n",
      "train Loss: 0.3884 Acc: 0.8383\n",
      "val Loss: 0.6482 Acc: 0.7708\n",
      "\n",
      "Epoch 46/100\n",
      "----------\n",
      "Epoch [46/100], Step [100/129], Loss: 0.6709, Accuracy: 23.00%\n",
      "train Loss: 0.3804 Acc: 0.8354\n",
      "val Loss: 0.5413 Acc: 0.7948\n",
      "\n",
      "Epoch 47/100\n",
      "----------\n",
      "Epoch [47/100], Step [100/129], Loss: 0.3071, Accuracy: 28.00%\n",
      "train Loss: 0.3587 Acc: 0.8456\n",
      "val Loss: 0.5140 Acc: 0.7959\n",
      "\n",
      "Epoch 48/100\n",
      "----------\n",
      "Epoch [48/100], Step [100/129], Loss: 0.6051, Accuracy: 22.00%\n",
      "train Loss: 0.3578 Acc: 0.8492\n",
      "val Loss: 0.6111 Acc: 0.7662\n",
      "\n",
      "Epoch 49/100\n",
      "----------\n",
      "Epoch [49/100], Step [100/129], Loss: 0.2760, Accuracy: 27.00%\n",
      "train Loss: 0.3574 Acc: 0.8461\n",
      "val Loss: 0.5280 Acc: 0.7902\n",
      "\n",
      "Epoch 50/100\n",
      "----------\n",
      "Epoch [50/100], Step [100/129], Loss: 0.6486, Accuracy: 23.00%\n",
      "train Loss: 0.3495 Acc: 0.8643\n",
      "val Loss: 0.5008 Acc: 0.7959\n",
      "\n",
      "Epoch 51/100\n",
      "----------\n",
      "Epoch [51/100], Step [100/129], Loss: 0.3131, Accuracy: 28.00%\n",
      "train Loss: 0.3525 Acc: 0.8507\n",
      "val Loss: 0.6865 Acc: 0.7548\n",
      "\n",
      "Epoch 52/100\n",
      "----------\n",
      "Epoch [52/100], Step [100/129], Loss: 0.1990, Accuracy: 28.00%\n",
      "train Loss: 0.3381 Acc: 0.8563\n",
      "val Loss: 0.5534 Acc: 0.7834\n",
      "\n",
      "Epoch 53/100\n",
      "----------\n",
      "Epoch [53/100], Step [100/129], Loss: 0.2569, Accuracy: 29.00%\n",
      "train Loss: 0.3379 Acc: 0.8614\n",
      "val Loss: 0.6516 Acc: 0.7503\n",
      "\n",
      "Epoch 54/100\n",
      "----------\n",
      "Epoch [54/100], Step [100/129], Loss: 0.2600, Accuracy: 28.00%\n",
      "train Loss: 0.3429 Acc: 0.8582\n",
      "val Loss: 0.4953 Acc: 0.8016\n",
      "\n",
      "Epoch 55/100\n",
      "----------\n",
      "Epoch [55/100], Step [100/129], Loss: 0.2476, Accuracy: 29.00%\n",
      "train Loss: 0.3103 Acc: 0.8692\n",
      "val Loss: 0.5949 Acc: 0.7708\n",
      "\n",
      "Epoch 56/100\n",
      "----------\n",
      "Epoch [56/100], Step [100/129], Loss: 0.3283, Accuracy: 29.00%\n",
      "train Loss: 0.3002 Acc: 0.8833\n",
      "val Loss: 0.6489 Acc: 0.7662\n",
      "\n",
      "Epoch 57/100\n",
      "----------\n",
      "Epoch [57/100], Step [100/129], Loss: 0.2283, Accuracy: 28.00%\n",
      "train Loss: 0.2835 Acc: 0.8821\n",
      "val Loss: 0.7408 Acc: 0.7058\n",
      "\n",
      "Epoch 58/100\n",
      "----------\n",
      "Epoch [58/100], Step [100/129], Loss: 0.3317, Accuracy: 26.00%\n",
      "train Loss: 0.2920 Acc: 0.8738\n",
      "val Loss: 0.5183 Acc: 0.7902\n",
      "\n",
      "Epoch 59/100\n",
      "----------\n",
      "Epoch [59/100], Step [100/129], Loss: 0.4419, Accuracy: 27.00%\n",
      "train Loss: 0.2894 Acc: 0.8842\n",
      "val Loss: 1.2474 Acc: 0.6328\n",
      "\n",
      "Epoch 60/100\n",
      "----------\n",
      "Epoch [60/100], Step [100/129], Loss: 0.4697, Accuracy: 26.00%\n",
      "train Loss: 0.2629 Acc: 0.8957\n",
      "val Loss: 0.5844 Acc: 0.7868\n",
      "\n",
      "Epoch 61/100\n",
      "----------\n",
      "Epoch [61/100], Step [100/129], Loss: 0.2721, Accuracy: 28.00%\n",
      "train Loss: 0.2657 Acc: 0.8928\n",
      "val Loss: 0.6320 Acc: 0.7913\n",
      "\n",
      "Epoch 62/100\n",
      "----------\n",
      "Epoch [62/100], Step [100/129], Loss: 0.1724, Accuracy: 30.00%\n",
      "train Loss: 0.2550 Acc: 0.8949\n",
      "val Loss: 0.6036 Acc: 0.7674\n",
      "\n",
      "Epoch 63/100\n",
      "----------\n",
      "Epoch [63/100], Step [100/129], Loss: 0.2776, Accuracy: 26.00%\n",
      "train Loss: 0.2496 Acc: 0.8974\n",
      "val Loss: 0.6569 Acc: 0.7480\n",
      "\n",
      "Epoch 64/100\n",
      "----------\n",
      "Epoch [64/100], Step [100/129], Loss: 0.2755, Accuracy: 30.00%\n",
      "train Loss: 0.2314 Acc: 0.9059\n",
      "val Loss: 0.6583 Acc: 0.7765\n",
      "\n",
      "Epoch 65/100\n",
      "----------\n",
      "Epoch [65/100], Step [100/129], Loss: 0.2773, Accuracy: 28.00%\n",
      "train Loss: 0.2317 Acc: 0.9027\n",
      "val Loss: 0.6604 Acc: 0.7742\n",
      "\n",
      "Epoch 66/100\n",
      "----------\n",
      "Epoch [66/100], Step [100/129], Loss: 0.2655, Accuracy: 28.00%\n",
      "train Loss: 0.2123 Acc: 0.9137\n",
      "val Loss: 0.6231 Acc: 0.7879\n",
      "\n",
      "Epoch 67/100\n",
      "----------\n",
      "Epoch [67/100], Step [100/129], Loss: 0.1414, Accuracy: 29.00%\n",
      "train Loss: 0.2004 Acc: 0.9183\n",
      "val Loss: 0.7741 Acc: 0.7503\n",
      "\n",
      "Epoch 68/100\n",
      "----------\n",
      "Epoch [68/100], Step [100/129], Loss: 0.4080, Accuracy: 27.00%\n",
      "train Loss: 0.1980 Acc: 0.9214\n",
      "val Loss: 0.8098 Acc: 0.7537\n",
      "\n",
      "Epoch 69/100\n",
      "----------\n",
      "Epoch [69/100], Step [100/129], Loss: 0.0271, Accuracy: 32.00%\n",
      "train Loss: 0.1605 Acc: 0.9387\n",
      "val Loss: 1.0583 Acc: 0.7047\n",
      "\n",
      "Epoch 70/100\n",
      "----------\n",
      "Epoch [70/100], Step [100/129], Loss: 0.1395, Accuracy: 30.00%\n",
      "train Loss: 0.1677 Acc: 0.9358\n",
      "val Loss: 0.8017 Acc: 0.7594\n",
      "\n",
      "Epoch 71/100\n",
      "----------\n",
      "Epoch [71/100], Step [100/129], Loss: 0.1495, Accuracy: 29.00%\n",
      "train Loss: 0.1632 Acc: 0.9353\n",
      "val Loss: 0.8669 Acc: 0.7480\n",
      "\n",
      "Epoch 72/100\n",
      "----------\n",
      "Epoch [72/100], Step [100/129], Loss: 0.1112, Accuracy: 31.00%\n",
      "train Loss: 0.1395 Acc: 0.9436\n",
      "val Loss: 0.7298 Acc: 0.7731\n",
      "\n",
      "Epoch 73/100\n",
      "----------\n",
      "Epoch [73/100], Step [100/129], Loss: 0.1080, Accuracy: 31.00%\n",
      "train Loss: 0.1328 Acc: 0.9501\n",
      "val Loss: 0.8920 Acc: 0.7834\n",
      "\n",
      "Epoch 74/100\n",
      "----------\n",
      "Epoch [74/100], Step [100/129], Loss: 0.1270, Accuracy: 30.00%\n",
      "train Loss: 0.1369 Acc: 0.9514\n",
      "val Loss: 0.7609 Acc: 0.7936\n",
      "\n",
      "Epoch 75/100\n",
      "----------\n",
      "Epoch [75/100], Step [100/129], Loss: 0.0685, Accuracy: 31.00%\n",
      "train Loss: 0.1292 Acc: 0.9489\n",
      "val Loss: 0.8442 Acc: 0.7845\n",
      "\n",
      "Epoch 76/100\n",
      "----------\n",
      "Epoch [76/100], Step [100/129], Loss: 0.0274, Accuracy: 32.00%\n",
      "train Loss: 0.1088 Acc: 0.9604\n",
      "val Loss: 0.9867 Acc: 0.7970\n",
      "\n",
      "Epoch 77/100\n",
      "----------\n",
      "Epoch [77/100], Step [100/129], Loss: 0.0828, Accuracy: 31.00%\n",
      "train Loss: 0.1130 Acc: 0.9570\n",
      "val Loss: 1.0433 Acc: 0.7560\n",
      "\n",
      "Epoch 78/100\n",
      "----------\n",
      "Epoch [78/100], Step [100/129], Loss: 0.0911, Accuracy: 30.00%\n",
      "train Loss: 0.0946 Acc: 0.9650\n",
      "val Loss: 0.8502 Acc: 0.7856\n",
      "\n",
      "Epoch 79/100\n",
      "----------\n",
      "Epoch [79/100], Step [100/129], Loss: 0.2352, Accuracy: 28.00%\n",
      "train Loss: 0.0769 Acc: 0.9703\n",
      "val Loss: 1.0209 Acc: 0.7811\n",
      "\n",
      "Epoch 80/100\n",
      "----------\n",
      "Epoch [80/100], Step [100/129], Loss: 0.3301, Accuracy: 28.00%\n",
      "train Loss: 0.0931 Acc: 0.9647\n",
      "val Loss: 0.9414 Acc: 0.7834\n",
      "\n",
      "Epoch 81/100\n",
      "----------\n",
      "Epoch [81/100], Step [100/129], Loss: 0.0266, Accuracy: 32.00%\n",
      "train Loss: 0.0743 Acc: 0.9715\n",
      "val Loss: 1.0190 Acc: 0.7902\n",
      "\n",
      "Epoch 82/100\n",
      "----------\n",
      "Epoch [82/100], Step [100/129], Loss: 0.0236, Accuracy: 32.00%\n",
      "train Loss: 0.0666 Acc: 0.9759\n",
      "val Loss: 1.0688 Acc: 0.7674\n",
      "\n",
      "Epoch 83/100\n",
      "----------\n",
      "Epoch [83/100], Step [100/129], Loss: 0.0508, Accuracy: 31.00%\n",
      "train Loss: 0.0687 Acc: 0.9735\n",
      "val Loss: 1.1073 Acc: 0.7822\n",
      "\n",
      "Epoch 84/100\n",
      "----------\n",
      "Epoch [84/100], Step [100/129], Loss: 0.0316, Accuracy: 32.00%\n",
      "train Loss: 0.0587 Acc: 0.9779\n",
      "val Loss: 1.1037 Acc: 0.7731\n",
      "\n",
      "Epoch 85/100\n",
      "----------\n",
      "Epoch [85/100], Step [100/129], Loss: 0.1268, Accuracy: 30.00%\n",
      "train Loss: 0.0539 Acc: 0.9791\n",
      "val Loss: 1.1401 Acc: 0.7891\n",
      "\n",
      "Epoch 86/100\n",
      "----------\n",
      "Epoch [86/100], Step [100/129], Loss: 0.0308, Accuracy: 32.00%\n",
      "train Loss: 0.0579 Acc: 0.9793\n",
      "val Loss: 1.0738 Acc: 0.7708\n",
      "\n",
      "Epoch 87/100\n",
      "----------\n",
      "Epoch [87/100], Step [100/129], Loss: 0.0427, Accuracy: 32.00%\n",
      "train Loss: 0.0410 Acc: 0.9830\n",
      "val Loss: 1.2872 Acc: 0.7799\n",
      "\n",
      "Epoch 88/100\n",
      "----------\n",
      "Epoch [88/100], Step [100/129], Loss: 0.3798, Accuracy: 27.00%\n",
      "train Loss: 0.0443 Acc: 0.9854\n",
      "val Loss: 1.3210 Acc: 0.7640\n",
      "\n",
      "Epoch 89/100\n",
      "----------\n",
      "Epoch [89/100], Step [100/129], Loss: 0.0173, Accuracy: 32.00%\n",
      "train Loss: 0.0392 Acc: 0.9857\n",
      "val Loss: 1.4640 Acc: 0.7491\n",
      "\n",
      "Epoch 90/100\n",
      "----------\n",
      "Epoch [90/100], Step [100/129], Loss: 0.0434, Accuracy: 31.00%\n",
      "train Loss: 0.0471 Acc: 0.9818\n",
      "val Loss: 1.1842 Acc: 0.7651\n",
      "\n",
      "Epoch 91/100\n",
      "----------\n",
      "Epoch [91/100], Step [100/129], Loss: 0.0112, Accuracy: 32.00%\n",
      "train Loss: 0.0327 Acc: 0.9878\n",
      "val Loss: 1.3024 Acc: 0.7799\n",
      "\n",
      "Epoch 92/100\n",
      "----------\n",
      "Epoch [92/100], Step [100/129], Loss: 0.0765, Accuracy: 30.00%\n",
      "train Loss: 0.0326 Acc: 0.9891\n",
      "val Loss: 1.2932 Acc: 0.7708\n",
      "\n",
      "Epoch 93/100\n",
      "----------\n",
      "Epoch [93/100], Step [100/129], Loss: 0.0195, Accuracy: 32.00%\n",
      "train Loss: 0.0342 Acc: 0.9859\n",
      "val Loss: 1.3546 Acc: 0.7845\n",
      "\n",
      "Epoch 94/100\n",
      "----------\n",
      "Epoch [94/100], Step [100/129], Loss: 0.0061, Accuracy: 32.00%\n",
      "train Loss: 0.0367 Acc: 0.9874\n",
      "val Loss: 1.3501 Acc: 0.7902\n",
      "\n",
      "Epoch 95/100\n",
      "----------\n",
      "Epoch [95/100], Step [100/129], Loss: 0.0040, Accuracy: 32.00%\n",
      "train Loss: 0.0250 Acc: 0.9908\n",
      "val Loss: 1.3662 Acc: 0.7879\n",
      "\n",
      "Epoch 96/100\n",
      "----------\n",
      "Epoch [96/100], Step [100/129], Loss: 0.0396, Accuracy: 31.00%\n",
      "train Loss: 0.0281 Acc: 0.9910\n",
      "val Loss: 1.3597 Acc: 0.7982\n",
      "\n",
      "Epoch 97/100\n",
      "----------\n",
      "Epoch [97/100], Step [100/129], Loss: 0.0003, Accuracy: 32.00%\n",
      "train Loss: 0.0291 Acc: 0.9888\n",
      "val Loss: 1.4704 Acc: 0.7708\n",
      "\n",
      "Epoch 98/100\n",
      "----------\n",
      "Epoch [98/100], Step [100/129], Loss: 0.0054, Accuracy: 32.00%\n",
      "train Loss: 0.0246 Acc: 0.9910\n",
      "val Loss: 1.4398 Acc: 0.7856\n",
      "\n",
      "Epoch 99/100\n",
      "----------\n",
      "Epoch [99/100], Step [100/129], Loss: 0.0726, Accuracy: 30.00%\n",
      "train Loss: 0.0289 Acc: 0.9886\n",
      "val Loss: 1.5747 Acc: 0.7731\n",
      "\n",
      "Epoch 100/100\n",
      "----------\n",
      "Epoch [100/100], Step [100/129], Loss: 0.0943, Accuracy: 31.00%\n",
      "train Loss: 0.0254 Acc: 0.9910\n",
      "val Loss: 1.6082 Acc: 0.7731\n",
      "\n",
      "Training complete in 125m 49s\n",
      "Best val Acc: 0.808438\n"
     ]
    }
   ],
   "source": [
    "# Observe that all parameters are being optimized\n",
    "optimizer = optim.Adam(params_to_update, lr=lr_start)\n",
    "\n",
    "# Learning rate scheduler.\n",
    "# Learning rate scheduler.\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, \n",
    "                                                           eta_min=lr_end, last_epoch=-1)\n",
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate\n",
    "model_pyt, prof, val_history, train_history = train_model(device, model_pyt, data_loaders, \n",
    "                                                            optimizer, scheduler,\n",
    "                                                            criterion, \n",
    "                                                            num_epochs=num_epochs,\n",
    "                                                            num_classes=num_classes,\n",
    "                                                            is_inception=(model_name==\"inceptionv3\"),\n",
    "                                                            profiler=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'pneumonia'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "zM-dorQBJAZb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_model(model_pyt, '../../models/', \n",
    "           f'{model_name}_{dataset_name}_{num_epochs}_{batch_size}_{lr_start}_{lr_end}_model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics = eval_model(device=device, model=model_pyt, test_loader=data_loaders['test'], is_inception=(model_name==\"inceptionv3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.8152793614595211\n",
      "f1: 0.8042504507638534\n",
      "cm: [[226   4   6]\n",
      " [  7 358  52]\n",
      " [  6  87 131]]\n",
      "outputs: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2 1 1 1 2 2 2 1 1 2 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 1 2 1 2 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 2 2 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 2 1 1 1 2\n",
      " 1 1 2 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 2 2 1 1 1 1 1 1 1 1 2 1\n",
      " 1 1 1 1 2 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 0 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 2 1 1\n",
      " 1 2 2 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 2 1\n",
      " 1 1 2 1 1 1 1 1 1 1 1 2 1 1 1 2 1 1 1 2 1 1 1 1 0 1 1 1 1 2 2 2 0 1 1 1 1\n",
      " 1 1 2 2 1 1 2 1 1 1 1 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 2 1 1\n",
      " 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 2 2 2 2 2 1 1 2 2 2 1 2 2 2\n",
      " 2 1 2 2 1 2 1 1 2 2 2 1 2 2 2 2 2 2 2 1 1 2 1 1 1 1 1 1 2 2 2 2 2 2 2 1 2\n",
      " 2 2 1 2 2 1 1 1 2 2 1 1 2 0 2 0 2 2 2 2 1 2 2 1 1 2 2 2 1 1 2 2 1 2 2 2 1\n",
      " 2 1 2 1 1 1 2 2 2 1 1 1 2 2 2 2 1 2 1 2 1 1 2 2 2 2 0 1 0 2 1 2 2 2 0 2 2\n",
      " 1 1 1 1 2 2 2 2 1 1 1 2 2 2 1 1 2 1 1 1 2 1 2 2 2 2 1 1 2 1 1 1 2 2 2 2 1\n",
      " 2 1 2 2 2 1 1 1 1 1 1 2 2 2 2 1 1 2 2 2 2 2 2 2 2 2 2 2 1 1 2 1 2 1 2 1 1\n",
      " 1 2 1 2 1 2 1 1 2 2 2 2 1 2 0 2 1 1 2 2 2 2 1 2 1 2]\n",
      "targets: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "for i, v in eval_metrics.items():\n",
    "    print(f\"{i}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../../models/val_history_{model_name}_{dataset_name}_{num_epochs}_{batch_size}_{lr_start}_{lr_end}_.pickle', 'wb') as handle:\n",
    "    pickle.dump(val_history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(f'../../models/train_history_{model_name}_{dataset_name}_{num_epochs}_{batch_size}_{lr_start}_{lr_end}_.pickle', 'wb') as handle:\n",
    "    pickle.dump(train_history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(f'../../models/eval_metrics_{model_name}_{dataset_name}_{num_epochs}_{batch_size}_{lr_start}_{lr_end}_.pickle', 'wb') as handle:\n",
    "    pickle.dump(eval_metrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#with open('filename.pickle', 'rb') as handle:\n",
    "#    b = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WvFygat_aiDN"
   },
   "outputs": [],
   "source": [
    "#print(prof.key_averages(group_by_stack_n=5).table(sort_by='self_cpu_time_total', row_limit=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "EQ6hb3iO2mXv"
   },
   "outputs": [],
   "source": [
    "# Plot the training curves of validation accuracy vs. number\n",
    "#  of training epochs for the transfer learning method and\n",
    "#  the model trained from scratch\n",
    "# vhist = []\n",
    "# vhist = [h.cpu().numpy() for h in val_acc_history]\n",
    "# thist = []\n",
    "# thist = [h.cpu().numpy() for h in train_acc_history]\n",
    "\n",
    "# plt.title(\"Accuracy vs. Number of Training Epochs\")\n",
    "# plt.xlabel(\"Training Epochs\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# #plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\n",
    "# plt.plot(range(1,num_epochs+1),vhist,label=\"Validation\")\n",
    "# plt.plot(range(1,num_epochs+1),thist,label=\"Training\")\n",
    "# plt.ylim((0,1.))\n",
    "# plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "aXpHASjTUE_Q"
   },
   "outputs": [],
   "source": [
    "# Plot the training curves of validation accuracy vs. number\n",
    "#  of training epochs for the transfer learning method and\n",
    "#  the model trained from scratch\n",
    "# vhist = []\n",
    "# vhist = [h for h in val_loss_history]\n",
    "# thist = []\n",
    "# thist = [h for h in train_loss_history]\n",
    "\n",
    "# plt.title(\"Loss vs. Number of Training Epochs\")\n",
    "# plt.xlabel(\"Training Epochs\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# #plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\n",
    "# plt.plot(range(1,num_epochs+1),vhist,label=\"Validation\")\n",
    "# plt.plot(range(1,num_epochs+1),thist,label=\"Training\")\n",
    "# plt.ylim((0,1.))\n",
    "# plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
