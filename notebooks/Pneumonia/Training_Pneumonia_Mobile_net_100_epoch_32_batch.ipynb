{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: \n",
    "## 1. Get mean and std of dataset - done\n",
    "## 2. Write a script to plot loss + accuracy graph\n",
    "## 3. Get FLOPs - done\n",
    "## 4. Get num layers - done\n",
    "## ----------------------------------------------------\n",
    "## 1. Implement differentiable F1 loss function\n",
    "## 2. Add class weights\n",
    "## 3. Implement transfer learning part - done\n",
    "## 4. Implement T-SNE\n",
    "## 5. Implement gradcam\n",
    "## 6. Ablation study\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from train import train_model\n",
    "from model import initialize_model\n",
    "from utils import set_requires_grad, save_model\n",
    "from data import load_data\n",
    "from plotting import plot_data_loader\n",
    "from eval import eval_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0w24yf-Tj47H"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from numpy.random import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.autograd.profiler as tprofiler\n",
    "import torch.utils.data as td\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "O7W8BTtF3BN1"
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "\n",
    "# pytorch RNGs\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# numpy RNG\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "21_bts2Wj47M",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = \"../../data\"\n",
    "images_dir = \"../../data/chest_xray\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nr7fQfkuj47u"
   },
   "outputs": [],
   "source": [
    "# Get best num_workers\n",
    "# for i in range(97):\n",
    "#     start = time.time()\n",
    "#     data_loader = load_data(images_dir,\n",
    "#                                                                    batch_size = 96, \n",
    "#                                                                    input_size = 299, \n",
    "#                                                                    norm_arr = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "#                                                                    num_workers = i)\n",
    "#     iter(data_loader['train']).next()[0].shape\n",
    "#     print(f\"{i}: {time.time()-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96wB0P9Gj47u"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k-vpcOXE1pmg",
    "outputId": "25752964-a425-490c-d154-0cc8baab3b61"
   },
   "outputs": [],
   "source": [
    "# Models options: resnet50, resnet34, inceptionv3, vgg16, mobile_net_v3_large, efficient_net_b1, efficient_net_b0.\n",
    "model_name = \"mobile_net_v3_large\"\n",
    "\n",
    "# Number of classes.\n",
    "num_classes = 3\n",
    "\n",
    "# Batch Size.\n",
    "batch_size = 32\n",
    "\n",
    "# Epochs to train for.\n",
    "num_epochs = 100\n",
    "\n",
    "# Number of workers for data loader.\n",
    "num_workers = 0\n",
    "\n",
    "# Imagenet norm array passed as default value.\n",
    "# norm_arr=([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "# Chest x-ray8 training dataset metrics \n",
    "# norm_arr=([0.4951, 0.4951, 0.4951], [0.2896, 0.2896, 0.2896])\n",
    "# Pneumonia dataset metrics\n",
    "norm_arr=([0.4810, 0.4810, 0.4810], [0.2373, 0.2373, 0.2373])\n",
    "\n",
    "# Feature extract flag: False - Tune the whole model,\n",
    "#                       True - Update only the reshaped layer parameters.\n",
    "feature_extract = False\n",
    "\n",
    "# Use pretrained flag: None - Use random weights\n",
    "#                      String - Use pretrained weights given by String\n",
    "use_pretrained = None\n",
    "\n",
    "# Initialize the model for this run.\n",
    "model_pyt, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=use_pretrained)\n",
    "\n",
    "# lr start and end points for training.\n",
    "lr_start = 0.01\n",
    "lr_end = 0.001\n",
    "\n",
    "# How many epochs to restart.\n",
    "iter_restart = 10\n",
    "\n",
    "# Multiplication factor after restart.\n",
    "mul_restart = 1\n",
    "\n",
    "# Print the model we just instantiated\n",
    "#print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "e9FOwaso3LAc"
   },
   "outputs": [],
   "source": [
    "data_loaders = load_data(images_dir,\n",
    "                         batch_size = batch_size, \n",
    "                         input_size = (input_size, input_size), \n",
    "                         norm_arr = norm_arr,\n",
    "                         num_workers = num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "vacZgHSCj47u",
    "outputId": "35a65cef-1d6d-4657-ff01-be15854ca24b"
   },
   "source": [
    "plot_data_loader(data_loader['train'], (2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRIOYWbV1cnS"
   },
   "source": [
    "plot_data_loader(data_loader['test'], (2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znBg5tkd1dXF"
   },
   "source": [
    "plot_data_loader(data_loader['val'], (2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0rj7Qeg41wLm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Params to learn:\n",
      "\t features.0.0.weight\n",
      "\t features.0.1.weight\n",
      "\t features.0.1.bias\n",
      "\t features.1.block.0.0.weight\n",
      "\t features.1.block.0.1.weight\n",
      "\t features.1.block.0.1.bias\n",
      "\t features.1.block.1.0.weight\n",
      "\t features.1.block.1.1.weight\n",
      "\t features.1.block.1.1.bias\n",
      "\t features.2.block.0.0.weight\n",
      "\t features.2.block.0.1.weight\n",
      "\t features.2.block.0.1.bias\n",
      "\t features.2.block.1.0.weight\n",
      "\t features.2.block.1.1.weight\n",
      "\t features.2.block.1.1.bias\n",
      "\t features.2.block.2.0.weight\n",
      "\t features.2.block.2.1.weight\n",
      "\t features.2.block.2.1.bias\n",
      "\t features.3.block.0.0.weight\n",
      "\t features.3.block.0.1.weight\n",
      "\t features.3.block.0.1.bias\n",
      "\t features.3.block.1.0.weight\n",
      "\t features.3.block.1.1.weight\n",
      "\t features.3.block.1.1.bias\n",
      "\t features.3.block.2.0.weight\n",
      "\t features.3.block.2.1.weight\n",
      "\t features.3.block.2.1.bias\n",
      "\t features.4.block.0.0.weight\n",
      "\t features.4.block.0.1.weight\n",
      "\t features.4.block.0.1.bias\n",
      "\t features.4.block.1.0.weight\n",
      "\t features.4.block.1.1.weight\n",
      "\t features.4.block.1.1.bias\n",
      "\t features.4.block.2.fc1.weight\n",
      "\t features.4.block.2.fc1.bias\n",
      "\t features.4.block.2.fc2.weight\n",
      "\t features.4.block.2.fc2.bias\n",
      "\t features.4.block.3.0.weight\n",
      "\t features.4.block.3.1.weight\n",
      "\t features.4.block.3.1.bias\n",
      "\t features.5.block.0.0.weight\n",
      "\t features.5.block.0.1.weight\n",
      "\t features.5.block.0.1.bias\n",
      "\t features.5.block.1.0.weight\n",
      "\t features.5.block.1.1.weight\n",
      "\t features.5.block.1.1.bias\n",
      "\t features.5.block.2.fc1.weight\n",
      "\t features.5.block.2.fc1.bias\n",
      "\t features.5.block.2.fc2.weight\n",
      "\t features.5.block.2.fc2.bias\n",
      "\t features.5.block.3.0.weight\n",
      "\t features.5.block.3.1.weight\n",
      "\t features.5.block.3.1.bias\n",
      "\t features.6.block.0.0.weight\n",
      "\t features.6.block.0.1.weight\n",
      "\t features.6.block.0.1.bias\n",
      "\t features.6.block.1.0.weight\n",
      "\t features.6.block.1.1.weight\n",
      "\t features.6.block.1.1.bias\n",
      "\t features.6.block.2.fc1.weight\n",
      "\t features.6.block.2.fc1.bias\n",
      "\t features.6.block.2.fc2.weight\n",
      "\t features.6.block.2.fc2.bias\n",
      "\t features.6.block.3.0.weight\n",
      "\t features.6.block.3.1.weight\n",
      "\t features.6.block.3.1.bias\n",
      "\t features.7.block.0.0.weight\n",
      "\t features.7.block.0.1.weight\n",
      "\t features.7.block.0.1.bias\n",
      "\t features.7.block.1.0.weight\n",
      "\t features.7.block.1.1.weight\n",
      "\t features.7.block.1.1.bias\n",
      "\t features.7.block.2.0.weight\n",
      "\t features.7.block.2.1.weight\n",
      "\t features.7.block.2.1.bias\n",
      "\t features.8.block.0.0.weight\n",
      "\t features.8.block.0.1.weight\n",
      "\t features.8.block.0.1.bias\n",
      "\t features.8.block.1.0.weight\n",
      "\t features.8.block.1.1.weight\n",
      "\t features.8.block.1.1.bias\n",
      "\t features.8.block.2.0.weight\n",
      "\t features.8.block.2.1.weight\n",
      "\t features.8.block.2.1.bias\n",
      "\t features.9.block.0.0.weight\n",
      "\t features.9.block.0.1.weight\n",
      "\t features.9.block.0.1.bias\n",
      "\t features.9.block.1.0.weight\n",
      "\t features.9.block.1.1.weight\n",
      "\t features.9.block.1.1.bias\n",
      "\t features.9.block.2.0.weight\n",
      "\t features.9.block.2.1.weight\n",
      "\t features.9.block.2.1.bias\n",
      "\t features.10.block.0.0.weight\n",
      "\t features.10.block.0.1.weight\n",
      "\t features.10.block.0.1.bias\n",
      "\t features.10.block.1.0.weight\n",
      "\t features.10.block.1.1.weight\n",
      "\t features.10.block.1.1.bias\n",
      "\t features.10.block.2.0.weight\n",
      "\t features.10.block.2.1.weight\n",
      "\t features.10.block.2.1.bias\n",
      "\t features.11.block.0.0.weight\n",
      "\t features.11.block.0.1.weight\n",
      "\t features.11.block.0.1.bias\n",
      "\t features.11.block.1.0.weight\n",
      "\t features.11.block.1.1.weight\n",
      "\t features.11.block.1.1.bias\n",
      "\t features.11.block.2.fc1.weight\n",
      "\t features.11.block.2.fc1.bias\n",
      "\t features.11.block.2.fc2.weight\n",
      "\t features.11.block.2.fc2.bias\n",
      "\t features.11.block.3.0.weight\n",
      "\t features.11.block.3.1.weight\n",
      "\t features.11.block.3.1.bias\n",
      "\t features.12.block.0.0.weight\n",
      "\t features.12.block.0.1.weight\n",
      "\t features.12.block.0.1.bias\n",
      "\t features.12.block.1.0.weight\n",
      "\t features.12.block.1.1.weight\n",
      "\t features.12.block.1.1.bias\n",
      "\t features.12.block.2.fc1.weight\n",
      "\t features.12.block.2.fc1.bias\n",
      "\t features.12.block.2.fc2.weight\n",
      "\t features.12.block.2.fc2.bias\n",
      "\t features.12.block.3.0.weight\n",
      "\t features.12.block.3.1.weight\n",
      "\t features.12.block.3.1.bias\n",
      "\t features.13.block.0.0.weight\n",
      "\t features.13.block.0.1.weight\n",
      "\t features.13.block.0.1.bias\n",
      "\t features.13.block.1.0.weight\n",
      "\t features.13.block.1.1.weight\n",
      "\t features.13.block.1.1.bias\n",
      "\t features.13.block.2.fc1.weight\n",
      "\t features.13.block.2.fc1.bias\n",
      "\t features.13.block.2.fc2.weight\n",
      "\t features.13.block.2.fc2.bias\n",
      "\t features.13.block.3.0.weight\n",
      "\t features.13.block.3.1.weight\n",
      "\t features.13.block.3.1.bias\n",
      "\t features.14.block.0.0.weight\n",
      "\t features.14.block.0.1.weight\n",
      "\t features.14.block.0.1.bias\n",
      "\t features.14.block.1.0.weight\n",
      "\t features.14.block.1.1.weight\n",
      "\t features.14.block.1.1.bias\n",
      "\t features.14.block.2.fc1.weight\n",
      "\t features.14.block.2.fc1.bias\n",
      "\t features.14.block.2.fc2.weight\n",
      "\t features.14.block.2.fc2.bias\n",
      "\t features.14.block.3.0.weight\n",
      "\t features.14.block.3.1.weight\n",
      "\t features.14.block.3.1.bias\n",
      "\t features.15.block.0.0.weight\n",
      "\t features.15.block.0.1.weight\n",
      "\t features.15.block.0.1.bias\n",
      "\t features.15.block.1.0.weight\n",
      "\t features.15.block.1.1.weight\n",
      "\t features.15.block.1.1.bias\n",
      "\t features.15.block.2.fc1.weight\n",
      "\t features.15.block.2.fc1.bias\n",
      "\t features.15.block.2.fc2.weight\n",
      "\t features.15.block.2.fc2.bias\n",
      "\t features.15.block.3.0.weight\n",
      "\t features.15.block.3.1.weight\n",
      "\t features.15.block.3.1.bias\n",
      "\t features.16.0.weight\n",
      "\t features.16.1.weight\n",
      "\t features.16.1.bias\n",
      "\t classifier.0.weight\n",
      "\t classifier.0.bias\n",
      "\t classifier.3.weight\n",
      "\t classifier.3.bias\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Send model to GPU\n",
    "model_pyt = model_pyt.to(device)\n",
    "\n",
    "# Find parameters to be updated in this run.\n",
    "# parameters with requires_grad = True.\n",
    "params_to_update = model_pyt.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_pyt.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_pyt.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "A4B1-pfYm0Ea"
   },
   "outputs": [],
   "source": [
    "# 17 min 1 epoch - 128 batch size - inception\n",
    "# Efficientnet b0 - batch 96 - epoch 50 - num_workers 2 - flip, auto cont, sharp - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8wBR8vcG2jcd",
    "outputId": "7d01aa07-d235-4cb6-dfaf-53ce0c5a577d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "----------\n",
      "Epoch [1/100], Step [100/129], Loss: 0.6338, Accuracy: 21.00%\n",
      "train Loss: 1.0992 Acc: 0.5589\n",
      "val Loss: 1.6196 Acc: 0.4755\n",
      "\n",
      "Epoch 2/100\n",
      "----------\n",
      "Epoch [2/100], Step [100/129], Loss: 0.5955, Accuracy: 22.00%\n",
      "train Loss: 0.7221 Acc: 0.6700\n",
      "val Loss: 2.3183 Acc: 0.4755\n",
      "\n",
      "Epoch 3/100\n",
      "----------\n",
      "Epoch [3/100], Step [100/129], Loss: 0.7716, Accuracy: 19.00%\n",
      "train Loss: 0.7396 Acc: 0.6561\n",
      "val Loss: 1.9446 Acc: 0.4778\n",
      "\n",
      "Epoch 4/100\n",
      "----------\n",
      "Epoch [4/100], Step [100/129], Loss: 0.5077, Accuracy: 24.00%\n",
      "train Loss: 0.6588 Acc: 0.6975\n",
      "val Loss: 1.2033 Acc: 0.5656\n",
      "\n",
      "Epoch 5/100\n",
      "----------\n",
      "Epoch [5/100], Step [100/129], Loss: 0.7023, Accuracy: 23.00%\n",
      "train Loss: 0.6279 Acc: 0.7223\n",
      "val Loss: 0.6835 Acc: 0.6853\n",
      "\n",
      "Epoch 6/100\n",
      "----------\n",
      "Epoch [6/100], Step [100/129], Loss: 0.5681, Accuracy: 25.00%\n",
      "train Loss: 0.5770 Acc: 0.7505\n",
      "val Loss: 0.7710 Acc: 0.5564\n",
      "\n",
      "Epoch 7/100\n",
      "----------\n",
      "Epoch [7/100], Step [100/129], Loss: 0.5942, Accuracy: 23.00%\n",
      "train Loss: 0.5441 Acc: 0.7588\n",
      "val Loss: 0.7052 Acc: 0.7013\n",
      "\n",
      "Epoch 8/100\n",
      "----------\n",
      "Epoch [8/100], Step [100/129], Loss: 0.5014, Accuracy: 23.00%\n",
      "train Loss: 0.5228 Acc: 0.7748\n",
      "val Loss: 0.5719 Acc: 0.7640\n",
      "\n",
      "Epoch 9/100\n",
      "----------\n",
      "Epoch [9/100], Step [100/129], Loss: 0.4270, Accuracy: 27.00%\n",
      "train Loss: 0.4886 Acc: 0.7952\n",
      "val Loss: 0.5359 Acc: 0.7799\n",
      "\n",
      "Epoch 10/100\n",
      "----------\n",
      "Epoch [10/100], Step [100/129], Loss: 0.3657, Accuracy: 27.00%\n",
      "train Loss: 0.4751 Acc: 0.7994\n",
      "val Loss: 0.5339 Acc: 0.7834\n",
      "\n",
      "Epoch 11/100\n",
      "----------\n",
      "Epoch [11/100], Step [100/129], Loss: 0.7791, Accuracy: 23.00%\n",
      "train Loss: 0.6898 Acc: 0.7045\n",
      "val Loss: 3.2075 Acc: 0.4504\n",
      "\n",
      "Epoch 12/100\n",
      "----------\n",
      "Epoch [12/100], Step [100/129], Loss: 0.3844, Accuracy: 28.00%\n",
      "train Loss: 0.6502 Acc: 0.7016\n",
      "val Loss: 4.3616 Acc: 0.3010\n",
      "\n",
      "Epoch 13/100\n",
      "----------\n",
      "Epoch [13/100], Step [100/129], Loss: 0.4229, Accuracy: 25.00%\n",
      "train Loss: 0.6290 Acc: 0.7252\n",
      "val Loss: 0.9140 Acc: 0.6009\n",
      "\n",
      "Epoch 14/100\n",
      "----------\n",
      "Epoch [14/100], Step [100/129], Loss: 0.2812, Accuracy: 29.00%\n",
      "train Loss: 0.5773 Acc: 0.7561\n",
      "val Loss: 1.1411 Acc: 0.5336\n",
      "\n",
      "Epoch 15/100\n",
      "----------\n",
      "Epoch [15/100], Step [100/129], Loss: 0.5119, Accuracy: 24.00%\n",
      "train Loss: 0.5320 Acc: 0.7753\n",
      "val Loss: 0.7361 Acc: 0.6682\n",
      "\n",
      "Epoch 16/100\n",
      "----------\n",
      "Epoch [16/100], Step [100/129], Loss: 0.4860, Accuracy: 27.00%\n",
      "train Loss: 0.5299 Acc: 0.7794\n",
      "val Loss: 0.5786 Acc: 0.7628\n",
      "\n",
      "Epoch 17/100\n",
      "----------\n",
      "Epoch [17/100], Step [100/129], Loss: 0.4986, Accuracy: 26.00%\n",
      "train Loss: 0.5059 Acc: 0.7899\n",
      "val Loss: 0.5467 Acc: 0.7719\n",
      "\n",
      "Epoch 18/100\n",
      "----------\n",
      "Epoch [18/100], Step [100/129], Loss: 0.4934, Accuracy: 27.00%\n",
      "train Loss: 0.4701 Acc: 0.8089\n",
      "val Loss: 0.8156 Acc: 0.6956\n",
      "\n",
      "Epoch 19/100\n",
      "----------\n",
      "Epoch [19/100], Step [100/129], Loss: 0.4470, Accuracy: 25.00%\n",
      "train Loss: 0.4619 Acc: 0.8057\n",
      "val Loss: 0.4874 Acc: 0.7982\n",
      "\n",
      "Epoch 20/100\n",
      "----------\n",
      "Epoch [20/100], Step [100/129], Loss: 0.4798, Accuracy: 24.00%\n",
      "train Loss: 0.4429 Acc: 0.8147\n",
      "val Loss: 0.5701 Acc: 0.7594\n",
      "\n",
      "Epoch 21/100\n",
      "----------\n",
      "Epoch [21/100], Step [100/129], Loss: 0.7876, Accuracy: 21.00%\n",
      "train Loss: 0.6019 Acc: 0.7327\n",
      "val Loss: 9.6209 Acc: 0.3101\n",
      "\n",
      "Epoch 22/100\n",
      "----------\n",
      "Epoch [22/100], Step [100/129], Loss: 0.8602, Accuracy: 21.00%\n",
      "train Loss: 0.6419 Acc: 0.7301\n",
      "val Loss: 10.2987 Acc: 0.2987\n",
      "\n",
      "Epoch 23/100\n",
      "----------\n",
      "Epoch [23/100], Step [100/129], Loss: 0.3112, Accuracy: 30.00%\n",
      "train Loss: 0.5812 Acc: 0.7571\n",
      "val Loss: 1.8218 Acc: 0.4230\n",
      "\n",
      "Epoch 24/100\n",
      "----------\n",
      "Epoch [24/100], Step [100/129], Loss: 0.3699, Accuracy: 28.00%\n",
      "train Loss: 0.5201 Acc: 0.7804\n",
      "val Loss: 1.0048 Acc: 0.5827\n",
      "\n",
      "Epoch 25/100\n",
      "----------\n",
      "Epoch [25/100], Step [100/129], Loss: 0.3595, Accuracy: 27.00%\n",
      "train Loss: 0.5116 Acc: 0.7899\n",
      "val Loss: 0.7470 Acc: 0.6978\n",
      "\n",
      "Epoch 26/100\n",
      "----------\n",
      "Epoch [26/100], Step [100/129], Loss: 0.4958, Accuracy: 25.00%\n",
      "train Loss: 0.4888 Acc: 0.7955\n",
      "val Loss: 1.1692 Acc: 0.5405\n",
      "\n",
      "Epoch 27/100\n",
      "----------\n",
      "Epoch [27/100], Step [100/129], Loss: 0.3729, Accuracy: 26.00%\n",
      "train Loss: 0.4664 Acc: 0.8074\n",
      "val Loss: 0.5452 Acc: 0.7662\n",
      "\n",
      "Epoch 28/100\n",
      "----------\n",
      "Epoch [28/100], Step [100/129], Loss: 0.3735, Accuracy: 29.00%\n",
      "train Loss: 0.4406 Acc: 0.8152\n",
      "val Loss: 0.4747 Acc: 0.8050\n",
      "\n",
      "Epoch 29/100\n",
      "----------\n",
      "Epoch [29/100], Step [100/129], Loss: 0.3031, Accuracy: 30.00%\n",
      "train Loss: 0.4212 Acc: 0.8237\n",
      "val Loss: 0.4898 Acc: 0.7868\n",
      "\n",
      "Epoch 30/100\n",
      "----------\n",
      "Epoch [30/100], Step [100/129], Loss: 0.4567, Accuracy: 27.00%\n",
      "train Loss: 0.4060 Acc: 0.8264\n",
      "val Loss: 0.4789 Acc: 0.8062\n",
      "\n",
      "Epoch 31/100\n",
      "----------\n",
      "Epoch [31/100], Step [100/129], Loss: 0.5549, Accuracy: 26.00%\n",
      "train Loss: 0.5497 Acc: 0.7631\n",
      "val Loss: 3.9943 Acc: 0.3147\n",
      "\n",
      "Epoch 32/100\n",
      "----------\n",
      "Epoch [32/100], Step [100/129], Loss: 0.5243, Accuracy: 25.00%\n",
      "train Loss: 0.7011 Acc: 0.7111\n",
      "val Loss: 11.0789 Acc: 0.2406\n",
      "\n",
      "Epoch 33/100\n",
      "----------\n",
      "Epoch [33/100], Step [100/129], Loss: 0.8466, Accuracy: 19.00%\n",
      "train Loss: 0.5365 Acc: 0.7760\n",
      "val Loss: 1.2734 Acc: 0.5599\n",
      "\n",
      "Epoch 34/100\n",
      "----------\n",
      "Epoch [34/100], Step [100/129], Loss: 0.4914, Accuracy: 26.00%\n",
      "train Loss: 0.4969 Acc: 0.7904\n",
      "val Loss: 0.6733 Acc: 0.7320\n",
      "\n",
      "Epoch 35/100\n",
      "----------\n",
      "Epoch [35/100], Step [100/129], Loss: 0.2954, Accuracy: 27.00%\n",
      "train Loss: 0.4694 Acc: 0.8030\n",
      "val Loss: 0.5350 Acc: 0.7799\n",
      "\n",
      "Epoch 36/100\n",
      "----------\n",
      "Epoch [36/100], Step [100/129], Loss: 0.5189, Accuracy: 26.00%\n",
      "train Loss: 0.4587 Acc: 0.8074\n",
      "val Loss: 0.5889 Acc: 0.7434\n",
      "\n",
      "Epoch 37/100\n",
      "----------\n",
      "Epoch [37/100], Step [100/129], Loss: 0.4824, Accuracy: 23.00%\n",
      "train Loss: 0.4395 Acc: 0.8132\n",
      "val Loss: 0.4592 Acc: 0.8062\n",
      "\n",
      "Epoch 38/100\n",
      "----------\n",
      "Epoch [38/100], Step [100/129], Loss: 0.3431, Accuracy: 29.00%\n",
      "train Loss: 0.4119 Acc: 0.8271\n",
      "val Loss: 0.4461 Acc: 0.8107\n",
      "\n",
      "Epoch 39/100\n",
      "----------\n",
      "Epoch [39/100], Step [100/129], Loss: 0.2016, Accuracy: 29.00%\n",
      "train Loss: 0.3899 Acc: 0.8349\n",
      "val Loss: 0.5212 Acc: 0.7697\n",
      "\n",
      "Epoch 40/100\n",
      "----------\n",
      "Epoch [40/100], Step [100/129], Loss: 0.3823, Accuracy: 27.00%\n",
      "train Loss: 0.3862 Acc: 0.8375\n",
      "val Loss: 0.4595 Acc: 0.7982\n",
      "\n",
      "Epoch 41/100\n",
      "----------\n",
      "Epoch [41/100], Step [100/129], Loss: 0.6531, Accuracy: 27.00%\n",
      "train Loss: 0.4849 Acc: 0.7935\n",
      "val Loss: 1.4725 Acc: 0.6157\n",
      "\n",
      "Epoch 42/100\n",
      "----------\n",
      "Epoch [42/100], Step [100/129], Loss: 0.2440, Accuracy: 28.00%\n",
      "train Loss: 0.5098 Acc: 0.7855\n",
      "val Loss: 0.8719 Acc: 0.6511\n",
      "\n",
      "Epoch 43/100\n",
      "----------\n",
      "Epoch [43/100], Step [100/129], Loss: 0.6466, Accuracy: 22.00%\n",
      "train Loss: 0.5090 Acc: 0.7828\n",
      "val Loss: 0.8000 Acc: 0.7127\n",
      "\n",
      "Epoch 44/100\n",
      "----------\n",
      "Epoch [44/100], Step [100/129], Loss: 0.2732, Accuracy: 29.00%\n",
      "train Loss: 0.4583 Acc: 0.8045\n",
      "val Loss: 0.5174 Acc: 0.7731\n",
      "\n",
      "Epoch 45/100\n",
      "----------\n",
      "Epoch [45/100], Step [100/129], Loss: 0.5156, Accuracy: 26.00%\n",
      "train Loss: 0.4455 Acc: 0.8144\n",
      "val Loss: 0.8278 Acc: 0.6648\n",
      "\n",
      "Epoch 46/100\n",
      "----------\n",
      "Epoch [46/100], Step [100/129], Loss: 0.2481, Accuracy: 30.00%\n",
      "train Loss: 0.4360 Acc: 0.8213\n",
      "val Loss: 0.6447 Acc: 0.7263\n",
      "\n",
      "Epoch 47/100\n",
      "----------\n",
      "Epoch [47/100], Step [100/129], Loss: 0.5269, Accuracy: 24.00%\n",
      "train Loss: 0.3948 Acc: 0.8334\n",
      "val Loss: 0.4903 Acc: 0.7993\n",
      "\n",
      "Epoch 48/100\n",
      "----------\n",
      "Epoch [48/100], Step [100/129], Loss: 0.2833, Accuracy: 29.00%\n",
      "train Loss: 0.3859 Acc: 0.8358\n",
      "val Loss: 0.7923 Acc: 0.6499\n",
      "\n",
      "Epoch 49/100\n",
      "----------\n",
      "Epoch [49/100], Step [100/129], Loss: 0.4027, Accuracy: 25.00%\n",
      "train Loss: 0.3755 Acc: 0.8378\n",
      "val Loss: 0.4381 Acc: 0.8096\n",
      "\n",
      "Epoch 50/100\n",
      "----------\n",
      "Epoch [50/100], Step [100/129], Loss: 0.1901, Accuracy: 29.00%\n",
      "train Loss: 0.3558 Acc: 0.8509\n",
      "val Loss: 0.4630 Acc: 0.8016\n",
      "\n",
      "Epoch 51/100\n",
      "----------\n",
      "Epoch [51/100], Step [100/129], Loss: 0.5260, Accuracy: 27.00%\n",
      "train Loss: 0.9300 Acc: 0.7519\n",
      "val Loss: 91.0861 Acc: 0.3273\n",
      "\n",
      "Epoch 52/100\n",
      "----------\n",
      "Epoch [52/100], Step [100/129], Loss: 1.1787, Accuracy: 21.00%\n",
      "train Loss: 1.3254 Acc: 0.6486\n",
      "val Loss: 17.5453 Acc: 0.3307\n",
      "\n",
      "Epoch 53/100\n",
      "----------\n",
      "Epoch [53/100], Step [100/129], Loss: 0.5767, Accuracy: 25.00%\n",
      "train Loss: 0.5994 Acc: 0.7425\n",
      "val Loss: 1.3166 Acc: 0.5325\n",
      "\n",
      "Epoch 54/100\n",
      "----------\n",
      "Epoch [54/100], Step [100/129], Loss: 0.2794, Accuracy: 29.00%\n",
      "train Loss: 0.5576 Acc: 0.7678\n",
      "val Loss: 0.5588 Acc: 0.7777\n",
      "\n",
      "Epoch 55/100\n",
      "----------\n",
      "Epoch [55/100], Step [100/129], Loss: 0.4619, Accuracy: 26.00%\n",
      "train Loss: 0.5240 Acc: 0.7865\n",
      "val Loss: 0.5333 Acc: 0.7742\n",
      "\n",
      "Epoch 56/100\n",
      "----------\n",
      "Epoch [56/100], Step [100/129], Loss: 0.4890, Accuracy: 25.00%\n",
      "train Loss: 0.4884 Acc: 0.7887\n",
      "val Loss: 0.5312 Acc: 0.7674\n",
      "\n",
      "Epoch 57/100\n",
      "----------\n",
      "Epoch [57/100], Step [100/129], Loss: 0.5308, Accuracy: 24.00%\n",
      "train Loss: 0.4616 Acc: 0.8035\n",
      "val Loss: 0.5383 Acc: 0.7765\n",
      "\n",
      "Epoch 58/100\n",
      "----------\n",
      "Epoch [58/100], Step [100/129], Loss: 0.4345, Accuracy: 26.00%\n",
      "train Loss: 0.4453 Acc: 0.8130\n",
      "val Loss: 0.5103 Acc: 0.7856\n",
      "\n",
      "Epoch 59/100\n",
      "----------\n",
      "Epoch [59/100], Step [100/129], Loss: 0.4506, Accuracy: 25.00%\n",
      "train Loss: 0.4381 Acc: 0.8159\n",
      "val Loss: 0.6202 Acc: 0.7548\n",
      "\n",
      "Epoch 60/100\n",
      "----------\n",
      "Epoch [60/100], Step [100/129], Loss: 0.5182, Accuracy: 24.00%\n",
      "train Loss: 0.4281 Acc: 0.8254\n",
      "val Loss: 0.5205 Acc: 0.7856\n",
      "\n",
      "Epoch 61/100\n",
      "----------\n",
      "Epoch [61/100], Step [100/129], Loss: 0.6901, Accuracy: 23.00%\n",
      "train Loss: 0.4929 Acc: 0.7857\n",
      "val Loss: 1.5213 Acc: 0.4527\n",
      "\n",
      "Epoch 62/100\n",
      "----------\n",
      "Epoch [62/100], Step [100/129], Loss: 0.5064, Accuracy: 25.00%\n",
      "train Loss: 0.5102 Acc: 0.7828\n",
      "val Loss: 0.6068 Acc: 0.7526\n",
      "\n",
      "Epoch 63/100\n",
      "----------\n",
      "Epoch [63/100], Step [100/129], Loss: 0.6736, Accuracy: 19.00%\n",
      "train Loss: 0.4878 Acc: 0.7911\n",
      "val Loss: 0.6441 Acc: 0.7218\n",
      "\n",
      "Epoch 64/100\n",
      "----------\n",
      "Epoch [64/100], Step [100/129], Loss: 0.4819, Accuracy: 26.00%\n",
      "train Loss: 0.4743 Acc: 0.7982\n",
      "val Loss: 0.5734 Acc: 0.7309\n",
      "\n",
      "Epoch 65/100\n",
      "----------\n",
      "Epoch [65/100], Step [100/129], Loss: 0.5871, Accuracy: 22.00%\n",
      "train Loss: 0.4643 Acc: 0.8018\n",
      "val Loss: 0.6400 Acc: 0.7252\n",
      "\n",
      "Epoch 66/100\n",
      "----------\n",
      "Epoch [66/100], Step [100/129], Loss: 0.2888, Accuracy: 30.00%\n",
      "train Loss: 0.4344 Acc: 0.8247\n",
      "val Loss: 0.5072 Acc: 0.7788\n",
      "\n",
      "Epoch 67/100\n",
      "----------\n",
      "Epoch [67/100], Step [100/129], Loss: 0.5383, Accuracy: 24.00%\n",
      "train Loss: 0.4247 Acc: 0.8264\n",
      "val Loss: 0.5137 Acc: 0.8016\n",
      "\n",
      "Epoch 68/100\n",
      "----------\n",
      "Epoch [68/100], Step [100/129], Loss: 0.1630, Accuracy: 32.00%\n",
      "train Loss: 0.3975 Acc: 0.8354\n",
      "val Loss: 0.5249 Acc: 0.7685\n",
      "\n",
      "Epoch 69/100\n",
      "----------\n",
      "Epoch [69/100], Step [100/129], Loss: 0.3387, Accuracy: 26.00%\n",
      "train Loss: 0.3887 Acc: 0.8373\n",
      "val Loss: 0.4768 Acc: 0.8062\n",
      "\n",
      "Epoch 70/100\n",
      "----------\n",
      "Epoch [70/100], Step [100/129], Loss: 0.3922, Accuracy: 27.00%\n",
      "train Loss: 0.3735 Acc: 0.8431\n",
      "val Loss: 0.4715 Acc: 0.8073\n",
      "\n",
      "Epoch 71/100\n",
      "----------\n",
      "Epoch [71/100], Step [100/129], Loss: 0.4921, Accuracy: 26.00%\n",
      "train Loss: 0.5101 Acc: 0.7918\n",
      "val Loss: 1.9293 Acc: 0.4094\n",
      "\n",
      "Epoch 72/100\n",
      "----------\n",
      "Epoch [72/100], Step [100/129], Loss: 0.5104, Accuracy: 23.00%\n",
      "train Loss: 0.5668 Acc: 0.7768\n",
      "val Loss: 0.6499 Acc: 0.6956\n",
      "\n",
      "Epoch 73/100\n",
      "----------\n",
      "Epoch [73/100], Step [100/129], Loss: 0.4216, Accuracy: 28.00%\n",
      "train Loss: 0.4691 Acc: 0.8028\n",
      "val Loss: 0.8741 Acc: 0.6739\n",
      "\n",
      "Epoch 74/100\n",
      "----------\n",
      "Epoch [74/100], Step [100/129], Loss: 0.3730, Accuracy: 27.00%\n",
      "train Loss: 0.4506 Acc: 0.8098\n",
      "val Loss: 0.9213 Acc: 0.6192\n",
      "\n",
      "Epoch 75/100\n",
      "----------\n",
      "Epoch [75/100], Step [100/129], Loss: 0.3974, Accuracy: 27.00%\n",
      "train Loss: 0.4298 Acc: 0.8215\n",
      "val Loss: 0.4900 Acc: 0.7879\n",
      "\n",
      "Epoch 76/100\n",
      "----------\n",
      "Epoch [76/100], Step [100/129], Loss: 0.3699, Accuracy: 27.00%\n",
      "train Loss: 0.4063 Acc: 0.8281\n",
      "val Loss: 0.5624 Acc: 0.7651\n",
      "\n",
      "Epoch 77/100\n",
      "----------\n",
      "Epoch [77/100], Step [100/129], Loss: 0.4786, Accuracy: 24.00%\n",
      "train Loss: 0.3929 Acc: 0.8339\n",
      "val Loss: 0.5583 Acc: 0.7594\n",
      "\n",
      "Epoch 78/100\n",
      "----------\n",
      "Epoch [78/100], Step [100/129], Loss: 0.4623, Accuracy: 26.00%\n",
      "train Loss: 0.3542 Acc: 0.8526\n",
      "val Loss: 0.5564 Acc: 0.7788\n",
      "\n",
      "Epoch 79/100\n",
      "----------\n",
      "Epoch [79/100], Step [100/129], Loss: 0.2057, Accuracy: 30.00%\n",
      "train Loss: 0.3355 Acc: 0.8631\n",
      "val Loss: 0.5135 Acc: 0.7811\n",
      "\n",
      "Epoch 80/100\n",
      "----------\n",
      "Epoch [80/100], Step [100/129], Loss: 0.1571, Accuracy: 31.00%\n",
      "train Loss: 0.3271 Acc: 0.8602\n",
      "val Loss: 0.5013 Acc: 0.7970\n",
      "\n",
      "Epoch 81/100\n",
      "----------\n",
      "Epoch [81/100], Step [100/129], Loss: 0.5822, Accuracy: 23.00%\n",
      "train Loss: 0.4475 Acc: 0.8071\n",
      "val Loss: 0.8062 Acc: 0.6522\n",
      "\n",
      "Epoch 82/100\n",
      "----------\n",
      "Epoch [82/100], Step [100/129], Loss: 0.7073, Accuracy: 25.00%\n",
      "train Loss: 0.4888 Acc: 0.8067\n",
      "val Loss: 0.9308 Acc: 0.6522\n",
      "\n",
      "Epoch 83/100\n",
      "----------\n",
      "Epoch [83/100], Step [100/129], Loss: 0.4751, Accuracy: 24.00%\n",
      "train Loss: 0.4622 Acc: 0.8028\n",
      "val Loss: 0.7631 Acc: 0.7480\n",
      "\n",
      "Epoch 84/100\n",
      "----------\n",
      "Epoch [84/100], Step [100/129], Loss: 0.5406, Accuracy: 26.00%\n",
      "train Loss: 0.4315 Acc: 0.8244\n",
      "val Loss: 0.7202 Acc: 0.7149\n",
      "\n",
      "Epoch 85/100\n",
      "----------\n",
      "Epoch [85/100], Step [100/129], Loss: 0.5137, Accuracy: 22.00%\n",
      "train Loss: 0.4122 Acc: 0.8264\n",
      "val Loss: 0.5553 Acc: 0.7742\n",
      "\n",
      "Epoch 86/100\n",
      "----------\n",
      "Epoch [86/100], Step [100/129], Loss: 0.4401, Accuracy: 25.00%\n",
      "train Loss: 0.3842 Acc: 0.8400\n",
      "val Loss: 0.5649 Acc: 0.7856\n",
      "\n",
      "Epoch 87/100\n",
      "----------\n",
      "Epoch [87/100], Step [100/129], Loss: 0.5120, Accuracy: 23.00%\n",
      "train Loss: 0.3491 Acc: 0.8507\n",
      "val Loss: 0.5764 Acc: 0.7879\n",
      "\n",
      "Epoch 88/100\n",
      "----------\n",
      "Epoch [88/100], Step [100/129], Loss: 0.2632, Accuracy: 30.00%\n",
      "train Loss: 0.3171 Acc: 0.8684\n",
      "val Loss: 0.5610 Acc: 0.7777\n",
      "\n",
      "Epoch 89/100\n",
      "----------\n",
      "Epoch [89/100], Step [100/129], Loss: 0.2034, Accuracy: 29.00%\n",
      "train Loss: 0.3040 Acc: 0.8731\n",
      "val Loss: 0.4932 Acc: 0.7959\n",
      "\n",
      "Epoch 90/100\n",
      "----------\n",
      "Epoch [90/100], Step [100/129], Loss: 0.1746, Accuracy: 31.00%\n",
      "train Loss: 0.2837 Acc: 0.8779\n",
      "val Loss: 0.5392 Acc: 0.7982\n",
      "\n",
      "Epoch 91/100\n",
      "----------\n",
      "Epoch [91/100], Step [100/129], Loss: 0.3736, Accuracy: 29.00%\n",
      "train Loss: 0.4793 Acc: 0.8023\n",
      "val Loss: 0.9768 Acc: 0.6385\n",
      "\n",
      "Epoch 92/100\n",
      "----------\n",
      "Epoch [92/100], Step [100/129], Loss: 0.3452, Accuracy: 27.00%\n",
      "train Loss: 0.4969 Acc: 0.8011\n",
      "val Loss: 0.8556 Acc: 0.6750\n",
      "\n",
      "Epoch 93/100\n",
      "----------\n",
      "Epoch [93/100], Step [100/129], Loss: 0.4197, Accuracy: 27.00%\n",
      "train Loss: 0.4192 Acc: 0.8247\n",
      "val Loss: 0.8620 Acc: 0.6739\n",
      "\n",
      "Epoch 94/100\n",
      "----------\n",
      "Epoch [94/100], Step [100/129], Loss: 0.4606, Accuracy: 26.00%\n",
      "train Loss: 0.3928 Acc: 0.8397\n",
      "val Loss: 0.5639 Acc: 0.7651\n",
      "\n",
      "Epoch 95/100\n",
      "----------\n",
      "Epoch [95/100], Step [100/129], Loss: 0.2864, Accuracy: 29.00%\n",
      "train Loss: 0.3601 Acc: 0.8555\n",
      "val Loss: 0.5448 Acc: 0.7834\n",
      "\n",
      "Epoch 96/100\n",
      "----------\n",
      "Epoch [96/100], Step [100/129], Loss: 0.3125, Accuracy: 29.00%\n",
      "train Loss: 0.3407 Acc: 0.8560\n",
      "val Loss: 0.7955 Acc: 0.7229\n",
      "\n",
      "Epoch 97/100\n",
      "----------\n",
      "Epoch [97/100], Step [100/129], Loss: 0.4567, Accuracy: 26.00%\n",
      "train Loss: 0.3150 Acc: 0.8699\n",
      "val Loss: 0.6365 Acc: 0.7548\n",
      "\n",
      "Epoch 98/100\n",
      "----------\n",
      "Epoch [98/100], Step [100/129], Loss: 0.3208, Accuracy: 27.00%\n",
      "train Loss: 0.2806 Acc: 0.8842\n",
      "val Loss: 0.5476 Acc: 0.7913\n",
      "\n",
      "Epoch 99/100\n",
      "----------\n",
      "Epoch [99/100], Step [100/129], Loss: 0.2895, Accuracy: 26.00%\n",
      "train Loss: 0.2675 Acc: 0.8884\n",
      "val Loss: 0.5646 Acc: 0.7777\n",
      "\n",
      "Epoch 100/100\n",
      "----------\n",
      "Epoch [100/100], Step [100/129], Loss: 0.1550, Accuracy: 31.00%\n",
      "train Loss: 0.2403 Acc: 0.8988\n",
      "val Loss: 0.5603 Acc: 0.7891\n",
      "\n",
      "Training complete in 126m 50s\n",
      "Best val Acc: 0.810718\n"
     ]
    }
   ],
   "source": [
    "# Observe that all parameters are being optimized\n",
    "optimizer = optim.Adam(params_to_update, lr=lr_start)\n",
    "\n",
    "# Learning rate scheduler.\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, \n",
    "                                                           eta_min=lr_end, last_epoch=-1)\n",
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate\n",
    "model_pyt, prof, val_history, train_history = train_model(device, model_pyt, data_loaders, \n",
    "                                                            optimizer, scheduler,\n",
    "                                                            criterion, \n",
    "                                                            num_epochs=num_epochs,\n",
    "                                                            num_classes=num_classes,\n",
    "                                                            is_inception=(model_name==\"inceptionv3\"),\n",
    "                                                            profiler=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'pneumonia'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "zM-dorQBJAZb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_model(model_pyt, '../../models/', \n",
    "           f'{model_name}_{dataset_name}_{num_epochs}_{batch_size}_{lr_start}_{lr_end}_model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics = eval_model(device=device, model=model_pyt, test_loader=data_loaders['test'], is_inception=(model_name==\"inceptionv3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.7993158494868872\n",
      "f1: 0.7794996204700008\n",
      "cm: [[219   8   9]\n",
      " [  2 373  42]\n",
      " [  4 111 109]]\n",
      "outputs: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0\n",
      " 2 0 0 0 0 1 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 2\n",
      " 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 2 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 2 2 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 2 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 2 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1\n",
      " 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 2 1 2 2 1 1 1 1 1 1 1 1 1 1 2 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1\n",
      " 1 1 2 1 1 1 1 1 1 1 1 2 1 1 2 2 2 1 1 2 1 1 1 1 0 2 1 2 2 2 2 2 1 1 1 1 1\n",
      " 1 1 2 2 1 1 2 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 1 2 2 2 2 1 1 1 2\n",
      " 1 1 2 2 2 1 1 1 1 2 2 1 2 1 2 1 2 2 2 1 1 2 2 1 1 1 1 2 1 2 1 1 1 2 1 2 2\n",
      " 2 2 1 2 2 1 1 1 2 2 1 1 2 2 2 0 2 1 2 2 2 2 2 2 1 0 2 2 1 2 2 2 1 1 1 2 1\n",
      " 2 1 2 2 1 1 2 2 2 1 1 1 2 2 1 1 1 2 1 2 1 1 2 2 2 2 0 1 1 2 1 2 2 2 2 2 2\n",
      " 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 2 1 2 2 2 2 1 1 1 1 1 1 1 2 2 2 1\n",
      " 2 1 1 2 2 1 1 1 1 1 1 2 2 2 2 1 1 2 2 1 2 1 1 2 2 1 2 1 1 1 2 2 2 1 1 1 1\n",
      " 1 2 1 2 1 2 1 1 2 2 2 2 1 2 0 2 1 2 1 2 2 1 1 2 1 2]\n",
      "targets: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "for i, v in eval_metrics.items():\n",
    "    print(f\"{i}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../../models/val_history_{model_name}_{dataset_name}_{num_epochs}_{batch_size}_{lr_start}_{lr_end}_.pickle', 'wb') as handle:\n",
    "    pickle.dump(val_history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(f'../../models/train_history_{model_name}_{dataset_name}_{num_epochs}_{batch_size}_{lr_start}_{lr_end}_.pickle', 'wb') as handle:\n",
    "    pickle.dump(train_history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(f'../../models/eval_metrics_{model_name}_{dataset_name}_{num_epochs}_{batch_size}_{lr_start}_{lr_end}_.pickle', 'wb') as handle:\n",
    "    pickle.dump(eval_metrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#with open('filename.pickle', 'rb') as handle:\n",
    "#    b = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WvFygat_aiDN"
   },
   "outputs": [],
   "source": [
    "#print(prof.key_averages(group_by_stack_n=5).table(sort_by='self_cpu_time_total', row_limit=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "EQ6hb3iO2mXv"
   },
   "outputs": [],
   "source": [
    "# Plot the training curves of validation accuracy vs. number\n",
    "#  of training epochs for the transfer learning method and\n",
    "#  the model trained from scratch\n",
    "# vhist = []\n",
    "# vhist = [h.cpu().numpy() for h in val_acc_history]\n",
    "# thist = []\n",
    "# thist = [h.cpu().numpy() for h in train_acc_history]\n",
    "\n",
    "# plt.title(\"Accuracy vs. Number of Training Epochs\")\n",
    "# plt.xlabel(\"Training Epochs\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# #plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\n",
    "# plt.plot(range(1,num_epochs+1),vhist,label=\"Validation\")\n",
    "# plt.plot(range(1,num_epochs+1),thist,label=\"Training\")\n",
    "# plt.ylim((0,1.))\n",
    "# plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "aXpHASjTUE_Q"
   },
   "outputs": [],
   "source": [
    "# Plot the training curves of validation accuracy vs. number\n",
    "#  of training epochs for the transfer learning method and\n",
    "#  the model trained from scratch\n",
    "# vhist = []\n",
    "# vhist = [h for h in val_loss_history]\n",
    "# thist = []\n",
    "# thist = [h for h in train_loss_history]\n",
    "\n",
    "# plt.title(\"Loss vs. Number of Training Epochs\")\n",
    "# plt.xlabel(\"Training Epochs\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# #plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\n",
    "# plt.plot(range(1,num_epochs+1),vhist,label=\"Validation\")\n",
    "# plt.plot(range(1,num_epochs+1),thist,label=\"Training\")\n",
    "# plt.ylim((0,1.))\n",
    "# plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
