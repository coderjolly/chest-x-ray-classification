{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: \n",
    "## 1. Get mean and std of dataset - done\n",
    "## 2. Write a script to plot loss + accuracy graph\n",
    "## 3. Get FLOPs - done\n",
    "## 4. Get num layers - done\n",
    "## ----------------------------------------------------\n",
    "## 1. Implement differentiable F1 loss function\n",
    "## 2. Add class weights\n",
    "## 3. Implement transfer learning part - done\n",
    "## 4. Implement T-SNE\n",
    "## 5. Implement gradcam\n",
    "## 6. Ablation study\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from train import train_model\n",
    "from model import initialize_model\n",
    "from utils import set_requires_grad, save_model\n",
    "from data import load_data\n",
    "from plotting import plot_data_loader\n",
    "from eval import eval_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0w24yf-Tj47H"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from numpy.random import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.autograd.profiler as tprofiler\n",
    "import torch.utils.data as td\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "O7W8BTtF3BN1"
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "\n",
    "# pytorch RNGs\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# numpy RNG\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "21_bts2Wj47M",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = \"../../data\"\n",
    "images_dir = \"../../data/chest_xray\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nr7fQfkuj47u"
   },
   "outputs": [],
   "source": [
    "# Get best num_workers\n",
    "# for i in range(97):\n",
    "#     start = time.time()\n",
    "#     data_loader = load_data(images_dir,\n",
    "#                                                                    batch_size = 96, \n",
    "#                                                                    input_size = 299, \n",
    "#                                                                    norm_arr = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "#                                                                    num_workers = i)\n",
    "#     iter(data_loader['train']).next()[0].shape\n",
    "#     print(f\"{i}: {time.time()-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96wB0P9Gj47u"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k-vpcOXE1pmg",
    "outputId": "25752964-a425-490c-d154-0cc8baab3b61"
   },
   "outputs": [],
   "source": [
    "# Models options: resnet50, resnet34, inceptionv3, vgg16, mobile_net_v3_large, efficient_net_b1, efficient_net_b0.\n",
    "model_name = \"mobile_net_v3_large\"\n",
    "\n",
    "# Number of classes.\n",
    "num_classes = 3\n",
    "\n",
    "# Batch Size.\n",
    "batch_size = 64\n",
    "\n",
    "# Epochs to train for.\n",
    "num_epochs = 100\n",
    "\n",
    "# Number of workers for data loader.\n",
    "num_workers = 0\n",
    "\n",
    "# Imagenet norm array passed as default value.\n",
    "# norm_arr=([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "# Chest x-ray8 training dataset metrics \n",
    "# norm_arr=([0.4951, 0.4951, 0.4951], [0.2896, 0.2896, 0.2896])\n",
    "# Pneumonia dataset metrics\n",
    "norm_arr=([0.4810, 0.4810, 0.4810], [0.2373, 0.2373, 0.2373])\n",
    "\n",
    "# Feature extract flag: False - Tune the whole model,\n",
    "#                       True - Update only the reshaped layer parameters.\n",
    "feature_extract = False\n",
    "\n",
    "# Use pretrained flag: None - Use random weights\n",
    "#                      String - Use pretrained weights given by String\n",
    "use_pretrained = None\n",
    "\n",
    "# Initialize the model for this run.\n",
    "model_pyt, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=use_pretrained)\n",
    "\n",
    "# lr start and end points for training.\n",
    "lr_start = 0.01\n",
    "lr_end = 0.001\n",
    "\n",
    "# How many epochs to restart.\n",
    "iter_restart = 10\n",
    "\n",
    "# Multiplication factor after restart.\n",
    "mul_restart = 1\n",
    "\n",
    "# Print the model we just instantiated\n",
    "#print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "e9FOwaso3LAc"
   },
   "outputs": [],
   "source": [
    "data_loaders = load_data(images_dir,\n",
    "                         batch_size = batch_size, \n",
    "                         input_size = (input_size, input_size), \n",
    "                         norm_arr = norm_arr,\n",
    "                         num_workers = num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "vacZgHSCj47u",
    "outputId": "35a65cef-1d6d-4657-ff01-be15854ca24b"
   },
   "source": [
    "plot_data_loader(data_loader['train'], (2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRIOYWbV1cnS"
   },
   "source": [
    "plot_data_loader(data_loader['test'], (2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znBg5tkd1dXF"
   },
   "source": [
    "plot_data_loader(data_loader['val'], (2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "0rj7Qeg41wLm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Params to learn:\n",
      "\t features.0.0.weight\n",
      "\t features.0.1.weight\n",
      "\t features.0.1.bias\n",
      "\t features.1.block.0.0.weight\n",
      "\t features.1.block.0.1.weight\n",
      "\t features.1.block.0.1.bias\n",
      "\t features.1.block.1.0.weight\n",
      "\t features.1.block.1.1.weight\n",
      "\t features.1.block.1.1.bias\n",
      "\t features.2.block.0.0.weight\n",
      "\t features.2.block.0.1.weight\n",
      "\t features.2.block.0.1.bias\n",
      "\t features.2.block.1.0.weight\n",
      "\t features.2.block.1.1.weight\n",
      "\t features.2.block.1.1.bias\n",
      "\t features.2.block.2.0.weight\n",
      "\t features.2.block.2.1.weight\n",
      "\t features.2.block.2.1.bias\n",
      "\t features.3.block.0.0.weight\n",
      "\t features.3.block.0.1.weight\n",
      "\t features.3.block.0.1.bias\n",
      "\t features.3.block.1.0.weight\n",
      "\t features.3.block.1.1.weight\n",
      "\t features.3.block.1.1.bias\n",
      "\t features.3.block.2.0.weight\n",
      "\t features.3.block.2.1.weight\n",
      "\t features.3.block.2.1.bias\n",
      "\t features.4.block.0.0.weight\n",
      "\t features.4.block.0.1.weight\n",
      "\t features.4.block.0.1.bias\n",
      "\t features.4.block.1.0.weight\n",
      "\t features.4.block.1.1.weight\n",
      "\t features.4.block.1.1.bias\n",
      "\t features.4.block.2.fc1.weight\n",
      "\t features.4.block.2.fc1.bias\n",
      "\t features.4.block.2.fc2.weight\n",
      "\t features.4.block.2.fc2.bias\n",
      "\t features.4.block.3.0.weight\n",
      "\t features.4.block.3.1.weight\n",
      "\t features.4.block.3.1.bias\n",
      "\t features.5.block.0.0.weight\n",
      "\t features.5.block.0.1.weight\n",
      "\t features.5.block.0.1.bias\n",
      "\t features.5.block.1.0.weight\n",
      "\t features.5.block.1.1.weight\n",
      "\t features.5.block.1.1.bias\n",
      "\t features.5.block.2.fc1.weight\n",
      "\t features.5.block.2.fc1.bias\n",
      "\t features.5.block.2.fc2.weight\n",
      "\t features.5.block.2.fc2.bias\n",
      "\t features.5.block.3.0.weight\n",
      "\t features.5.block.3.1.weight\n",
      "\t features.5.block.3.1.bias\n",
      "\t features.6.block.0.0.weight\n",
      "\t features.6.block.0.1.weight\n",
      "\t features.6.block.0.1.bias\n",
      "\t features.6.block.1.0.weight\n",
      "\t features.6.block.1.1.weight\n",
      "\t features.6.block.1.1.bias\n",
      "\t features.6.block.2.fc1.weight\n",
      "\t features.6.block.2.fc1.bias\n",
      "\t features.6.block.2.fc2.weight\n",
      "\t features.6.block.2.fc2.bias\n",
      "\t features.6.block.3.0.weight\n",
      "\t features.6.block.3.1.weight\n",
      "\t features.6.block.3.1.bias\n",
      "\t features.7.block.0.0.weight\n",
      "\t features.7.block.0.1.weight\n",
      "\t features.7.block.0.1.bias\n",
      "\t features.7.block.1.0.weight\n",
      "\t features.7.block.1.1.weight\n",
      "\t features.7.block.1.1.bias\n",
      "\t features.7.block.2.0.weight\n",
      "\t features.7.block.2.1.weight\n",
      "\t features.7.block.2.1.bias\n",
      "\t features.8.block.0.0.weight\n",
      "\t features.8.block.0.1.weight\n",
      "\t features.8.block.0.1.bias\n",
      "\t features.8.block.1.0.weight\n",
      "\t features.8.block.1.1.weight\n",
      "\t features.8.block.1.1.bias\n",
      "\t features.8.block.2.0.weight\n",
      "\t features.8.block.2.1.weight\n",
      "\t features.8.block.2.1.bias\n",
      "\t features.9.block.0.0.weight\n",
      "\t features.9.block.0.1.weight\n",
      "\t features.9.block.0.1.bias\n",
      "\t features.9.block.1.0.weight\n",
      "\t features.9.block.1.1.weight\n",
      "\t features.9.block.1.1.bias\n",
      "\t features.9.block.2.0.weight\n",
      "\t features.9.block.2.1.weight\n",
      "\t features.9.block.2.1.bias\n",
      "\t features.10.block.0.0.weight\n",
      "\t features.10.block.0.1.weight\n",
      "\t features.10.block.0.1.bias\n",
      "\t features.10.block.1.0.weight\n",
      "\t features.10.block.1.1.weight\n",
      "\t features.10.block.1.1.bias\n",
      "\t features.10.block.2.0.weight\n",
      "\t features.10.block.2.1.weight\n",
      "\t features.10.block.2.1.bias\n",
      "\t features.11.block.0.0.weight\n",
      "\t features.11.block.0.1.weight\n",
      "\t features.11.block.0.1.bias\n",
      "\t features.11.block.1.0.weight\n",
      "\t features.11.block.1.1.weight\n",
      "\t features.11.block.1.1.bias\n",
      "\t features.11.block.2.fc1.weight\n",
      "\t features.11.block.2.fc1.bias\n",
      "\t features.11.block.2.fc2.weight\n",
      "\t features.11.block.2.fc2.bias\n",
      "\t features.11.block.3.0.weight\n",
      "\t features.11.block.3.1.weight\n",
      "\t features.11.block.3.1.bias\n",
      "\t features.12.block.0.0.weight\n",
      "\t features.12.block.0.1.weight\n",
      "\t features.12.block.0.1.bias\n",
      "\t features.12.block.1.0.weight\n",
      "\t features.12.block.1.1.weight\n",
      "\t features.12.block.1.1.bias\n",
      "\t features.12.block.2.fc1.weight\n",
      "\t features.12.block.2.fc1.bias\n",
      "\t features.12.block.2.fc2.weight\n",
      "\t features.12.block.2.fc2.bias\n",
      "\t features.12.block.3.0.weight\n",
      "\t features.12.block.3.1.weight\n",
      "\t features.12.block.3.1.bias\n",
      "\t features.13.block.0.0.weight\n",
      "\t features.13.block.0.1.weight\n",
      "\t features.13.block.0.1.bias\n",
      "\t features.13.block.1.0.weight\n",
      "\t features.13.block.1.1.weight\n",
      "\t features.13.block.1.1.bias\n",
      "\t features.13.block.2.fc1.weight\n",
      "\t features.13.block.2.fc1.bias\n",
      "\t features.13.block.2.fc2.weight\n",
      "\t features.13.block.2.fc2.bias\n",
      "\t features.13.block.3.0.weight\n",
      "\t features.13.block.3.1.weight\n",
      "\t features.13.block.3.1.bias\n",
      "\t features.14.block.0.0.weight\n",
      "\t features.14.block.0.1.weight\n",
      "\t features.14.block.0.1.bias\n",
      "\t features.14.block.1.0.weight\n",
      "\t features.14.block.1.1.weight\n",
      "\t features.14.block.1.1.bias\n",
      "\t features.14.block.2.fc1.weight\n",
      "\t features.14.block.2.fc1.bias\n",
      "\t features.14.block.2.fc2.weight\n",
      "\t features.14.block.2.fc2.bias\n",
      "\t features.14.block.3.0.weight\n",
      "\t features.14.block.3.1.weight\n",
      "\t features.14.block.3.1.bias\n",
      "\t features.15.block.0.0.weight\n",
      "\t features.15.block.0.1.weight\n",
      "\t features.15.block.0.1.bias\n",
      "\t features.15.block.1.0.weight\n",
      "\t features.15.block.1.1.weight\n",
      "\t features.15.block.1.1.bias\n",
      "\t features.15.block.2.fc1.weight\n",
      "\t features.15.block.2.fc1.bias\n",
      "\t features.15.block.2.fc2.weight\n",
      "\t features.15.block.2.fc2.bias\n",
      "\t features.15.block.3.0.weight\n",
      "\t features.15.block.3.1.weight\n",
      "\t features.15.block.3.1.bias\n",
      "\t features.16.0.weight\n",
      "\t features.16.1.weight\n",
      "\t features.16.1.bias\n",
      "\t classifier.0.weight\n",
      "\t classifier.0.bias\n",
      "\t classifier.3.weight\n",
      "\t classifier.3.bias\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Send model to GPU\n",
    "model_pyt = model_pyt.to(device)\n",
    "\n",
    "# Find parameters to be updated in this run.\n",
    "# parameters with requires_grad = True.\n",
    "params_to_update = model_pyt.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_pyt.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_pyt.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "A4B1-pfYm0Ea"
   },
   "outputs": [],
   "source": [
    "# 17 min 1 epoch - 128 batch size - inception\n",
    "# Efficientnet b0 - batch 96 - epoch 50 - num_workers 2 - flip, auto cont, sharp - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8wBR8vcG2jcd",
    "outputId": "7d01aa07-d235-4cb6-dfaf-53ce0c5a577d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "----------\n",
      "train Loss: 1.2163 Acc: 0.5382\n",
      "val Loss: 1.8975 Acc: 0.4755\n",
      "\n",
      "Epoch 2/100\n",
      "----------\n",
      "train Loss: 0.7245 Acc: 0.6756\n",
      "val Loss: 1.6637 Acc: 0.2554\n",
      "\n",
      "Epoch 3/100\n",
      "----------\n",
      "train Loss: 0.6527 Acc: 0.7198\n",
      "val Loss: 2.4656 Acc: 0.3022\n",
      "\n",
      "Epoch 4/100\n",
      "----------\n",
      "train Loss: 0.5907 Acc: 0.7505\n",
      "val Loss: 2.7244 Acc: 0.4139\n",
      "\n",
      "Epoch 5/100\n",
      "----------\n",
      "train Loss: 0.5743 Acc: 0.7629\n",
      "val Loss: 1.9393 Acc: 0.3033\n",
      "\n",
      "Epoch 6/100\n",
      "----------\n",
      "train Loss: 0.5215 Acc: 0.7860\n",
      "val Loss: 1.1606 Acc: 0.5325\n",
      "\n",
      "Epoch 7/100\n",
      "----------\n",
      "train Loss: 0.5185 Acc: 0.7833\n",
      "val Loss: 0.6049 Acc: 0.7412\n",
      "\n",
      "Epoch 8/100\n",
      "----------\n",
      "train Loss: 0.4847 Acc: 0.7989\n",
      "val Loss: 0.5804 Acc: 0.7526\n",
      "\n",
      "Epoch 9/100\n",
      "----------\n",
      "train Loss: 0.4712 Acc: 0.8045\n",
      "val Loss: 0.8112 Acc: 0.6887\n",
      "\n",
      "Epoch 10/100\n",
      "----------\n",
      "train Loss: 0.4665 Acc: 0.8118\n",
      "val Loss: 0.6173 Acc: 0.7184\n",
      "\n",
      "Epoch 11/100\n",
      "----------\n",
      "train Loss: 0.6219 Acc: 0.7415\n",
      "val Loss: 2.9042 Acc: 0.5884\n",
      "\n",
      "Epoch 12/100\n",
      "----------\n",
      "train Loss: 0.6462 Acc: 0.7252\n",
      "val Loss: 5.6638 Acc: 0.6648\n",
      "\n",
      "Epoch 13/100\n",
      "----------\n",
      "train Loss: 0.5701 Acc: 0.7634\n",
      "val Loss: 1.3652 Acc: 0.6568\n",
      "\n",
      "Epoch 14/100\n",
      "----------\n",
      "train Loss: 0.5350 Acc: 0.7746\n",
      "val Loss: 1.0039 Acc: 0.6328\n",
      "\n",
      "Epoch 15/100\n",
      "----------\n",
      "train Loss: 0.5113 Acc: 0.7865\n",
      "val Loss: 7.4248 Acc: 0.3615\n",
      "\n",
      "Epoch 16/100\n",
      "----------\n",
      "train Loss: 0.4913 Acc: 0.8006\n",
      "val Loss: 0.9633 Acc: 0.6591\n",
      "\n",
      "Epoch 17/100\n",
      "----------\n",
      "train Loss: 0.4602 Acc: 0.8084\n",
      "val Loss: 0.5789 Acc: 0.7446\n",
      "\n",
      "Epoch 18/100\n",
      "----------\n",
      "train Loss: 0.4500 Acc: 0.8106\n",
      "val Loss: 0.5326 Acc: 0.7651\n",
      "\n",
      "Epoch 19/100\n",
      "----------\n",
      "train Loss: 0.4353 Acc: 0.8174\n",
      "val Loss: 0.4841 Acc: 0.7845\n",
      "\n",
      "Epoch 20/100\n",
      "----------\n",
      "train Loss: 0.4164 Acc: 0.8254\n",
      "val Loss: 0.4865 Acc: 0.7936\n",
      "\n",
      "Epoch 21/100\n",
      "----------\n",
      "train Loss: 0.5531 Acc: 0.7656\n",
      "val Loss: 65.3965 Acc: 0.2691\n",
      "\n",
      "Epoch 22/100\n",
      "----------\n",
      "train Loss: 0.5024 Acc: 0.7964\n",
      "val Loss: 1.4857 Acc: 0.6784\n",
      "\n",
      "Epoch 23/100\n",
      "----------\n",
      "train Loss: 0.4867 Acc: 0.7982\n",
      "val Loss: 0.8502 Acc: 0.6830\n",
      "\n",
      "Epoch 24/100\n",
      "----------\n",
      "train Loss: 0.4909 Acc: 0.7896\n",
      "val Loss: 1.1075 Acc: 0.6454\n",
      "\n",
      "Epoch 25/100\n",
      "----------\n",
      "train Loss: 0.4591 Acc: 0.8169\n",
      "val Loss: 0.8456 Acc: 0.7206\n",
      "\n",
      "Epoch 26/100\n",
      "----------\n",
      "train Loss: 0.4386 Acc: 0.8215\n",
      "val Loss: 0.5173 Acc: 0.7799\n",
      "\n",
      "Epoch 27/100\n",
      "----------\n",
      "train Loss: 0.4356 Acc: 0.8176\n",
      "val Loss: 0.6987 Acc: 0.7343\n",
      "\n",
      "Epoch 28/100\n",
      "----------\n",
      "train Loss: 0.4089 Acc: 0.8312\n",
      "val Loss: 0.4903 Acc: 0.7925\n",
      "\n",
      "Epoch 29/100\n",
      "----------\n",
      "train Loss: 0.4000 Acc: 0.8305\n",
      "val Loss: 0.4647 Acc: 0.7959\n",
      "\n",
      "Epoch 30/100\n",
      "----------\n",
      "train Loss: 0.3759 Acc: 0.8427\n",
      "val Loss: 0.4689 Acc: 0.7970\n",
      "\n",
      "Epoch 31/100\n",
      "----------\n",
      "train Loss: 0.5167 Acc: 0.7850\n",
      "val Loss: 30.9641 Acc: 0.2794\n",
      "\n",
      "Epoch 32/100\n",
      "----------\n",
      "train Loss: 0.5119 Acc: 0.7867\n",
      "val Loss: 0.7966 Acc: 0.7241\n",
      "\n",
      "Epoch 33/100\n",
      "----------\n",
      "train Loss: 0.4930 Acc: 0.7979\n",
      "val Loss: 1.0964 Acc: 0.6784\n",
      "\n",
      "Epoch 34/100\n",
      "----------\n",
      "train Loss: 0.4453 Acc: 0.8091\n",
      "val Loss: 1.3816 Acc: 0.6351\n",
      "\n",
      "Epoch 35/100\n",
      "----------\n",
      "train Loss: 0.4288 Acc: 0.8213\n",
      "val Loss: 0.8006 Acc: 0.7115\n",
      "\n",
      "Epoch 36/100\n",
      "----------\n",
      "train Loss: 0.4329 Acc: 0.8159\n",
      "val Loss: 0.5072 Acc: 0.7731\n",
      "\n",
      "Epoch 37/100\n",
      "----------\n",
      "train Loss: 0.3996 Acc: 0.8322\n",
      "val Loss: 0.4809 Acc: 0.7970\n",
      "\n",
      "Epoch 38/100\n",
      "----------\n",
      "train Loss: 0.3849 Acc: 0.8363\n",
      "val Loss: 0.5044 Acc: 0.7891\n",
      "\n",
      "Epoch 39/100\n",
      "----------\n",
      "train Loss: 0.3604 Acc: 0.8497\n",
      "val Loss: 0.4593 Acc: 0.7902\n",
      "\n",
      "Epoch 40/100\n",
      "----------\n",
      "train Loss: 0.3531 Acc: 0.8534\n",
      "val Loss: 0.4600 Acc: 0.8016\n",
      "\n",
      "Epoch 41/100\n",
      "----------\n",
      "train Loss: 0.4865 Acc: 0.7999\n",
      "val Loss: 3.6831 Acc: 0.3820\n",
      "\n",
      "Epoch 42/100\n",
      "----------\n",
      "train Loss: 0.4822 Acc: 0.8050\n",
      "val Loss: 1.5681 Acc: 0.6693\n",
      "\n",
      "Epoch 43/100\n",
      "----------\n",
      "train Loss: 0.4611 Acc: 0.8115\n",
      "val Loss: 1.3617 Acc: 0.7001\n",
      "\n",
      "Epoch 44/100\n",
      "----------\n",
      "train Loss: 0.4513 Acc: 0.8108\n",
      "val Loss: 0.7747 Acc: 0.7400\n",
      "\n",
      "Epoch 45/100\n",
      "----------\n",
      "train Loss: 0.4372 Acc: 0.8203\n",
      "val Loss: 0.6176 Acc: 0.7446\n",
      "\n",
      "Epoch 46/100\n",
      "----------\n",
      "train Loss: 0.4014 Acc: 0.8317\n",
      "val Loss: 0.5549 Acc: 0.7548\n",
      "\n",
      "Epoch 47/100\n",
      "----------\n",
      "train Loss: 0.3796 Acc: 0.8465\n",
      "val Loss: 0.5094 Acc: 0.7891\n",
      "\n",
      "Epoch 48/100\n",
      "----------\n",
      "train Loss: 0.3592 Acc: 0.8507\n",
      "val Loss: 0.4710 Acc: 0.7856\n",
      "\n",
      "Epoch 49/100\n",
      "----------\n",
      "train Loss: 0.3431 Acc: 0.8585\n",
      "val Loss: 0.4580 Acc: 0.7948\n",
      "\n",
      "Epoch 50/100\n",
      "----------\n",
      "train Loss: 0.3220 Acc: 0.8643\n",
      "val Loss: 0.4502 Acc: 0.8039\n",
      "\n",
      "Epoch 51/100\n",
      "----------\n",
      "train Loss: 0.4825 Acc: 0.8011\n",
      "val Loss: 6.9834 Acc: 0.4173\n",
      "\n",
      "Epoch 52/100\n",
      "----------\n",
      "train Loss: 0.4857 Acc: 0.7967\n",
      "val Loss: 1.3940 Acc: 0.7138\n",
      "\n",
      "Epoch 53/100\n",
      "----------\n",
      "train Loss: 0.4740 Acc: 0.8030\n",
      "val Loss: 0.7914 Acc: 0.7184\n",
      "\n",
      "Epoch 54/100\n",
      "----------\n",
      "train Loss: 0.4382 Acc: 0.8183\n",
      "val Loss: 0.5632 Acc: 0.7913\n",
      "\n",
      "Epoch 55/100\n",
      "----------\n",
      "train Loss: 0.4180 Acc: 0.8215\n",
      "val Loss: 0.6602 Acc: 0.7640\n",
      "\n",
      "Epoch 56/100\n",
      "----------\n",
      "train Loss: 0.3787 Acc: 0.8402\n",
      "val Loss: 0.5994 Acc: 0.7685\n",
      "\n",
      "Epoch 57/100\n",
      "----------\n",
      "train Loss: 0.3702 Acc: 0.8512\n",
      "val Loss: 0.5206 Acc: 0.7936\n",
      "\n",
      "Epoch 58/100\n",
      "----------\n",
      "train Loss: 0.3577 Acc: 0.8482\n",
      "val Loss: 0.4861 Acc: 0.7697\n",
      "\n",
      "Epoch 59/100\n",
      "----------\n",
      "train Loss: 0.3220 Acc: 0.8624\n",
      "val Loss: 0.4871 Acc: 0.8073\n",
      "\n",
      "Epoch 60/100\n",
      "----------\n",
      "train Loss: 0.3134 Acc: 0.8672\n",
      "val Loss: 0.4844 Acc: 0.7982\n",
      "\n",
      "Epoch 61/100\n",
      "----------\n",
      "train Loss: 0.4438 Acc: 0.8217\n",
      "val Loss: 1.7212 Acc: 0.6021\n",
      "\n",
      "Epoch 62/100\n",
      "----------\n",
      "train Loss: 0.4356 Acc: 0.8154\n",
      "val Loss: 1.0509 Acc: 0.5884\n",
      "\n",
      "Epoch 63/100\n",
      "----------\n",
      "train Loss: 0.4082 Acc: 0.8286\n",
      "val Loss: 3.2983 Acc: 0.4424\n",
      "\n",
      "Epoch 64/100\n",
      "----------\n",
      "train Loss: 0.4145 Acc: 0.8217\n",
      "val Loss: 1.2698 Acc: 0.6499\n",
      "\n",
      "Epoch 65/100\n",
      "----------\n",
      "train Loss: 0.4497 Acc: 0.8115\n",
      "val Loss: 0.8064 Acc: 0.6784\n",
      "\n",
      "Epoch 66/100\n",
      "----------\n",
      "train Loss: 0.3911 Acc: 0.8356\n",
      "val Loss: 0.6906 Acc: 0.6306\n",
      "\n",
      "Epoch 67/100\n",
      "----------\n",
      "train Loss: 0.3596 Acc: 0.8482\n",
      "val Loss: 0.5232 Acc: 0.7697\n",
      "\n",
      "Epoch 68/100\n",
      "----------\n",
      "train Loss: 0.3173 Acc: 0.8682\n",
      "val Loss: 0.5057 Acc: 0.7936\n",
      "\n",
      "Epoch 69/100\n",
      "----------\n",
      "train Loss: 0.2935 Acc: 0.8784\n",
      "val Loss: 0.5624 Acc: 0.7651\n",
      "\n",
      "Epoch 70/100\n",
      "----------\n",
      "train Loss: 0.2768 Acc: 0.8864\n",
      "val Loss: 0.6041 Acc: 0.7434\n",
      "\n",
      "Epoch 71/100\n",
      "----------\n",
      "train Loss: 0.4150 Acc: 0.8261\n",
      "val Loss: 1.0401 Acc: 0.6055\n",
      "\n",
      "Epoch 72/100\n",
      "----------\n",
      "train Loss: 0.4090 Acc: 0.8286\n",
      "val Loss: 2.7138 Acc: 0.5644\n",
      "\n",
      "Epoch 73/100\n",
      "----------\n",
      "train Loss: 0.3972 Acc: 0.8329\n",
      "val Loss: 0.9982 Acc: 0.7423\n",
      "\n",
      "Epoch 74/100\n",
      "----------\n",
      "train Loss: 0.3822 Acc: 0.8407\n",
      "val Loss: 2.0365 Acc: 0.5713\n",
      "\n",
      "Epoch 75/100\n",
      "----------\n",
      "train Loss: 0.3707 Acc: 0.8446\n",
      "val Loss: 1.4227 Acc: 0.6226\n",
      "\n",
      "Epoch 76/100\n",
      "----------\n",
      "train Loss: 0.3198 Acc: 0.8655\n",
      "val Loss: 0.5588 Acc: 0.7765\n",
      "\n",
      "Epoch 77/100\n",
      "----------\n",
      "train Loss: 0.3004 Acc: 0.8757\n",
      "val Loss: 0.5986 Acc: 0.7993\n",
      "\n",
      "Epoch 78/100\n",
      "----------\n",
      "train Loss: 0.2706 Acc: 0.8886\n",
      "val Loss: 0.5284 Acc: 0.7948\n",
      "\n",
      "Epoch 79/100\n",
      "----------\n",
      "train Loss: 0.2381 Acc: 0.9022\n",
      "val Loss: 0.5282 Acc: 0.7913\n",
      "\n",
      "Epoch 80/100\n",
      "----------\n",
      "train Loss: 0.2220 Acc: 0.9122\n",
      "val Loss: 0.5783 Acc: 0.7697\n",
      "\n",
      "Epoch 81/100\n",
      "----------\n",
      "train Loss: 0.4069 Acc: 0.8268\n",
      "val Loss: 1.4077 Acc: 0.6374\n",
      "\n",
      "Epoch 82/100\n",
      "----------\n",
      "train Loss: 0.3766 Acc: 0.8383\n",
      "val Loss: 0.8897 Acc: 0.7434\n",
      "\n",
      "Epoch 83/100\n",
      "----------\n",
      "train Loss: 0.3790 Acc: 0.8444\n",
      "val Loss: 1.5358 Acc: 0.6180\n",
      "\n",
      "Epoch 84/100\n",
      "----------\n",
      "train Loss: 0.3381 Acc: 0.8572\n",
      "val Loss: 0.6690 Acc: 0.7640\n",
      "\n",
      "Epoch 85/100\n",
      "----------\n",
      "train Loss: 0.3143 Acc: 0.8709\n",
      "val Loss: 1.6612 Acc: 0.6670\n",
      "\n",
      "Epoch 86/100\n",
      "----------\n",
      "train Loss: 0.2892 Acc: 0.8769\n",
      "val Loss: 0.7874 Acc: 0.7366\n",
      "\n",
      "Epoch 87/100\n",
      "----------\n",
      "train Loss: 0.2572 Acc: 0.8923\n",
      "val Loss: 0.8459 Acc: 0.7503\n",
      "\n",
      "Epoch 88/100\n",
      "----------\n",
      "train Loss: 0.2293 Acc: 0.9086\n",
      "val Loss: 0.5965 Acc: 0.7925\n",
      "\n",
      "Epoch 89/100\n",
      "----------\n",
      "train Loss: 0.1967 Acc: 0.9166\n",
      "val Loss: 0.6092 Acc: 0.8016\n",
      "\n",
      "Epoch 90/100\n",
      "----------\n",
      "train Loss: 0.1634 Acc: 0.9363\n",
      "val Loss: 0.6666 Acc: 0.8016\n",
      "\n",
      "Epoch 91/100\n",
      "----------\n",
      "train Loss: 0.3589 Acc: 0.8551\n",
      "val Loss: 1.4250 Acc: 0.6340\n",
      "\n",
      "Epoch 92/100\n",
      "----------\n",
      "train Loss: 0.3672 Acc: 0.8458\n",
      "val Loss: 5.3945 Acc: 0.3387\n",
      "\n",
      "Epoch 93/100\n",
      "----------\n",
      "train Loss: 0.3879 Acc: 0.8368\n",
      "val Loss: 1.1992 Acc: 0.6260\n",
      "\n",
      "Epoch 94/100\n",
      "----------\n",
      "train Loss: 0.3500 Acc: 0.8577\n",
      "val Loss: 1.0360 Acc: 0.7218\n",
      "\n",
      "Epoch 95/100\n",
      "----------\n",
      "train Loss: 0.2920 Acc: 0.8755\n",
      "val Loss: 0.5887 Acc: 0.7788\n",
      "\n",
      "Epoch 96/100\n",
      "----------\n",
      "train Loss: 0.2487 Acc: 0.9020\n",
      "val Loss: 0.9515 Acc: 0.7286\n",
      "\n",
      "Epoch 97/100\n",
      "----------\n",
      "train Loss: 0.2005 Acc: 0.9185\n",
      "val Loss: 0.6592 Acc: 0.7845\n",
      "\n",
      "Epoch 98/100\n",
      "----------\n",
      "train Loss: 0.1736 Acc: 0.9382\n",
      "val Loss: 0.7215 Acc: 0.7799\n",
      "\n",
      "Epoch 99/100\n",
      "----------\n",
      "train Loss: 0.1541 Acc: 0.9446\n",
      "val Loss: 0.7058 Acc: 0.7719\n",
      "\n",
      "Epoch 100/100\n",
      "----------\n",
      "train Loss: 0.1114 Acc: 0.9570\n",
      "val Loss: 0.8900 Acc: 0.7799\n",
      "\n",
      "Training complete in 134m 21s\n",
      "Best val Acc: 0.807298\n"
     ]
    }
   ],
   "source": [
    "# Observe that all parameters are being optimized\n",
    "optimizer = optim.Adam(params_to_update, lr=lr_start)\n",
    "\n",
    "# Learning rate scheduler.\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=iter_restart, T_mult=mul_restart, \n",
    "                                                           eta_min=lr_end, last_epoch=-1)\n",
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate\n",
    "model_pyt, prof, val_history, train_history = train_model(device, model_pyt, data_loaders, \n",
    "                                                            optimizer, scheduler,\n",
    "                                                            criterion, \n",
    "                                                            num_epochs=num_epochs,\n",
    "                                                            num_classes=num_classes,\n",
    "                                                            is_inception=(model_name==\"inceptionv3\"),\n",
    "                                                            profiler=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'pneumonia'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "zM-dorQBJAZb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_model(model_pyt, '../../models/', \n",
    "           f'{model_name}_{dataset_name}_{num_epochs}_{batch_size}_{lr_start}_{lr_end}_model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics = eval_model(device=device, model=model_pyt, test_loader=data_loaders['test'], is_inception=(model_name==\"inceptionv3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.8050171037628279\n",
      "f1: 0.7872908896595345\n",
      "cm: [[220   5  11]\n",
      " [  5 371  41]\n",
      " [  3 106 115]]\n",
      "outputs: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 2 0\n",
      " 1 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2\n",
      " 0 0 0 1 0 0 2 0 0 0 0 0 0 0 0 0 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 2 0 0 0 0 1 1 2 2 1 1 2 2 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 2 2 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1\n",
      " 1 2 1 1 1 1 1 2 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 2 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 2 2 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 2 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 2 2 1 1 1 1 1 1 1 1 2 1 1 1 1 2 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1\n",
      " 1 1 2 1 1 1 1 1 1 1 1 2 1 1 2 2 1 1 2 2 1 1 1 1 0 1 1 1 1 1 1 2 1 1 1 1 1\n",
      " 1 1 2 2 1 1 2 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 1 1 1 2 2 2 2 2 2\n",
      " 1 1 2 2 2 1 1 1 1 2 2 1 2 2 2 2 2 2 2 1 1 2 2 2 1 1 1 2 2 2 1 1 1 1 2 1 2\n",
      " 2 2 1 2 2 1 1 1 1 2 1 1 2 2 1 0 2 2 1 2 2 2 2 2 1 2 2 1 1 2 2 2 1 2 2 2 1\n",
      " 2 1 2 2 2 1 2 2 2 1 1 1 2 1 1 1 1 1 2 2 1 1 2 2 2 2 0 1 2 2 1 2 2 2 2 2 2\n",
      " 2 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 2 1 1 1 2 1 1 1 2 2 1 1 1 1 1 1 2 2 2 2 1\n",
      " 2 1 2 2 2 1 1 1 1 1 1 2 2 1 2 1 1 2 1 1 2 1 1 2 2 1 2 2 1 1 2 2 2 1 1 1 1\n",
      " 1 2 1 2 1 2 1 1 2 2 2 2 1 2 0 2 1 1 1 2 2 1 1 2 2 2]\n",
      "targets: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "for i, v in eval_metrics.items():\n",
    "    print(f\"{i}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../../models/val_history_{model_name}_{dataset_name}_{num_epochs}_{batch_size}_{lr_start}_{lr_end}_.pickle', 'wb') as handle:\n",
    "    pickle.dump(val_history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(f'../../models/train_history_{model_name}_{dataset_name}_{num_epochs}_{batch_size}_{lr_start}_{lr_end}_.pickle', 'wb') as handle:\n",
    "    pickle.dump(train_history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(f'../../models/eval_metrics_{model_name}_{dataset_name}_{num_epochs}_{batch_size}_{lr_start}_{lr_end}_.pickle', 'wb') as handle:\n",
    "    pickle.dump(eval_metrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#with open('filename.pickle', 'rb') as handle:\n",
    "#    b = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WvFygat_aiDN"
   },
   "outputs": [],
   "source": [
    "#print(prof.key_averages(group_by_stack_n=5).table(sort_by='self_cpu_time_total', row_limit=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "EQ6hb3iO2mXv"
   },
   "outputs": [],
   "source": [
    "# Plot the training curves of validation accuracy vs. number\n",
    "#  of training epochs for the transfer learning method and\n",
    "#  the model trained from scratch\n",
    "# vhist = []\n",
    "# vhist = [h.cpu().numpy() for h in val_acc_history]\n",
    "# thist = []\n",
    "# thist = [h.cpu().numpy() for h in train_acc_history]\n",
    "\n",
    "# plt.title(\"Accuracy vs. Number of Training Epochs\")\n",
    "# plt.xlabel(\"Training Epochs\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# #plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\n",
    "# plt.plot(range(1,num_epochs+1),vhist,label=\"Validation\")\n",
    "# plt.plot(range(1,num_epochs+1),thist,label=\"Training\")\n",
    "# plt.ylim((0,1.))\n",
    "# plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "aXpHASjTUE_Q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
