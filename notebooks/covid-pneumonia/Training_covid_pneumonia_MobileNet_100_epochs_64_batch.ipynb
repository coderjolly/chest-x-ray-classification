{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: \n",
    "## 1. Get mean and std of dataset - done\n",
    "## 2. Write a script to plot loss + accuracy graph\n",
    "## 3. Get FLOPs - done\n",
    "## 4. Get num layers - done\n",
    "## ----------------------------------------------------\n",
    "## 1. Implement differentiable F1 loss function\n",
    "## 2. Add class weights\n",
    "## 3. Implement transfer learning part - done\n",
    "## 4. Implement T-SNE\n",
    "## 5. Implement gradcam\n",
    "## 6. Ablation study\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from train import train_model\n",
    "from model import initialize_model\n",
    "from utils import set_requires_grad, save_model\n",
    "from data import load_data\n",
    "from plotting import plot_data_loader\n",
    "from eval import eval_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0w24yf-Tj47H"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from numpy.random import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.autograd.profiler as tprofiler\n",
    "import torch.utils.data as td\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "O7W8BTtF3BN1"
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "\n",
    "# pytorch RNGs\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# numpy RNG\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "21_bts2Wj47M",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = \"../../data\"\n",
    "images_dir = \"../../data/covid_pneumonia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nr7fQfkuj47u"
   },
   "outputs": [],
   "source": [
    "# Get best num_workers\n",
    "# for i in range(97):\n",
    "#     start = time.time()\n",
    "#     data_loader = load_data(images_dir,\n",
    "#                                                                    batch_size = 96, \n",
    "#                                                                    input_size = 299, \n",
    "#                                                                    norm_arr = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "#                                                                    num_workers = i)\n",
    "#     iter(data_loader['train']).next()[0].shape\n",
    "#     print(f\"{i}: {time.time()-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96wB0P9Gj47u"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k-vpcOXE1pmg",
    "outputId": "25752964-a425-490c-d154-0cc8baab3b61"
   },
   "outputs": [],
   "source": [
    "# Models options: resnet50, resnet34, inceptionv3, vgg16, mobile_net_v3_large, efficient_net_b1, efficient_net_b0.\n",
    "model_name = \"mobile_net_v3_large\"\n",
    "\n",
    "# Number of classes.\n",
    "num_classes = 3\n",
    "\n",
    "# Batch Size.\n",
    "batch_size = 64\n",
    "\n",
    "# Epochs to train for.\n",
    "num_epochs = 100\n",
    "\n",
    "# Number of workers for data loader.\n",
    "num_workers = 12\n",
    "\n",
    "# Imagenet norm array passed as default value.\n",
    "norm_arr=([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "# Chest x-ray8 training dataset metrics \n",
    "norm_arr=([0.4951, 0.4951, 0.4951], [0.2896, 0.2896, 0.2896])\n",
    "# Covid Pneumonia dataset metrics\n",
    "norm_arr = ([0.5159, 0.5159, 0.5159], [0.2554, 0.2554, 0.2554])\n",
    "\n",
    "# Feature extract flag: False - Tune the whole model,\n",
    "#                       True - Update only the reshaped layer parameters.\n",
    "feature_extract = False\n",
    "\n",
    "# Use pretrained flag: None - Use random weights\n",
    "#                      String - Use pretrained weights given by String\n",
    "use_pretrained = None\n",
    "\n",
    "# Initialize the model for this run.\n",
    "model_pyt, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=use_pretrained)\n",
    "\n",
    "# lr start and end points for training.\n",
    "lr_start = 0.01\n",
    "lr_end = 0.001\n",
    "\n",
    "# How many epochs to restart.\n",
    "iter_restart = 10\n",
    "\n",
    "# Multiplication factor after restart.\n",
    "mul_restart = 1\n",
    "\n",
    "# Print the model we just instantiated\n",
    "#print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "e9FOwaso3LAc"
   },
   "outputs": [],
   "source": [
    "data_loaders = load_data(images_dir,\n",
    "                         batch_size = batch_size, \n",
    "                         input_size = input_size, \n",
    "                         norm_arr = norm_arr,\n",
    "                         num_workers = num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "vacZgHSCj47u",
    "outputId": "35a65cef-1d6d-4657-ff01-be15854ca24b"
   },
   "source": [
    "plot_data_loader(data_loader['train'], (2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRIOYWbV1cnS"
   },
   "source": [
    "plot_data_loader(data_loader['test'], (2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znBg5tkd1dXF"
   },
   "source": [
    "plot_data_loader(data_loader['val'], (2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0rj7Qeg41wLm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Params to learn:\n",
      "\t features.0.0.weight\n",
      "\t features.0.1.weight\n",
      "\t features.0.1.bias\n",
      "\t features.1.block.0.0.weight\n",
      "\t features.1.block.0.1.weight\n",
      "\t features.1.block.0.1.bias\n",
      "\t features.1.block.1.0.weight\n",
      "\t features.1.block.1.1.weight\n",
      "\t features.1.block.1.1.bias\n",
      "\t features.2.block.0.0.weight\n",
      "\t features.2.block.0.1.weight\n",
      "\t features.2.block.0.1.bias\n",
      "\t features.2.block.1.0.weight\n",
      "\t features.2.block.1.1.weight\n",
      "\t features.2.block.1.1.bias\n",
      "\t features.2.block.2.0.weight\n",
      "\t features.2.block.2.1.weight\n",
      "\t features.2.block.2.1.bias\n",
      "\t features.3.block.0.0.weight\n",
      "\t features.3.block.0.1.weight\n",
      "\t features.3.block.0.1.bias\n",
      "\t features.3.block.1.0.weight\n",
      "\t features.3.block.1.1.weight\n",
      "\t features.3.block.1.1.bias\n",
      "\t features.3.block.2.0.weight\n",
      "\t features.3.block.2.1.weight\n",
      "\t features.3.block.2.1.bias\n",
      "\t features.4.block.0.0.weight\n",
      "\t features.4.block.0.1.weight\n",
      "\t features.4.block.0.1.bias\n",
      "\t features.4.block.1.0.weight\n",
      "\t features.4.block.1.1.weight\n",
      "\t features.4.block.1.1.bias\n",
      "\t features.4.block.2.fc1.weight\n",
      "\t features.4.block.2.fc1.bias\n",
      "\t features.4.block.2.fc2.weight\n",
      "\t features.4.block.2.fc2.bias\n",
      "\t features.4.block.3.0.weight\n",
      "\t features.4.block.3.1.weight\n",
      "\t features.4.block.3.1.bias\n",
      "\t features.5.block.0.0.weight\n",
      "\t features.5.block.0.1.weight\n",
      "\t features.5.block.0.1.bias\n",
      "\t features.5.block.1.0.weight\n",
      "\t features.5.block.1.1.weight\n",
      "\t features.5.block.1.1.bias\n",
      "\t features.5.block.2.fc1.weight\n",
      "\t features.5.block.2.fc1.bias\n",
      "\t features.5.block.2.fc2.weight\n",
      "\t features.5.block.2.fc2.bias\n",
      "\t features.5.block.3.0.weight\n",
      "\t features.5.block.3.1.weight\n",
      "\t features.5.block.3.1.bias\n",
      "\t features.6.block.0.0.weight\n",
      "\t features.6.block.0.1.weight\n",
      "\t features.6.block.0.1.bias\n",
      "\t features.6.block.1.0.weight\n",
      "\t features.6.block.1.1.weight\n",
      "\t features.6.block.1.1.bias\n",
      "\t features.6.block.2.fc1.weight\n",
      "\t features.6.block.2.fc1.bias\n",
      "\t features.6.block.2.fc2.weight\n",
      "\t features.6.block.2.fc2.bias\n",
      "\t features.6.block.3.0.weight\n",
      "\t features.6.block.3.1.weight\n",
      "\t features.6.block.3.1.bias\n",
      "\t features.7.block.0.0.weight\n",
      "\t features.7.block.0.1.weight\n",
      "\t features.7.block.0.1.bias\n",
      "\t features.7.block.1.0.weight\n",
      "\t features.7.block.1.1.weight\n",
      "\t features.7.block.1.1.bias\n",
      "\t features.7.block.2.0.weight\n",
      "\t features.7.block.2.1.weight\n",
      "\t features.7.block.2.1.bias\n",
      "\t features.8.block.0.0.weight\n",
      "\t features.8.block.0.1.weight\n",
      "\t features.8.block.0.1.bias\n",
      "\t features.8.block.1.0.weight\n",
      "\t features.8.block.1.1.weight\n",
      "\t features.8.block.1.1.bias\n",
      "\t features.8.block.2.0.weight\n",
      "\t features.8.block.2.1.weight\n",
      "\t features.8.block.2.1.bias\n",
      "\t features.9.block.0.0.weight\n",
      "\t features.9.block.0.1.weight\n",
      "\t features.9.block.0.1.bias\n",
      "\t features.9.block.1.0.weight\n",
      "\t features.9.block.1.1.weight\n",
      "\t features.9.block.1.1.bias\n",
      "\t features.9.block.2.0.weight\n",
      "\t features.9.block.2.1.weight\n",
      "\t features.9.block.2.1.bias\n",
      "\t features.10.block.0.0.weight\n",
      "\t features.10.block.0.1.weight\n",
      "\t features.10.block.0.1.bias\n",
      "\t features.10.block.1.0.weight\n",
      "\t features.10.block.1.1.weight\n",
      "\t features.10.block.1.1.bias\n",
      "\t features.10.block.2.0.weight\n",
      "\t features.10.block.2.1.weight\n",
      "\t features.10.block.2.1.bias\n",
      "\t features.11.block.0.0.weight\n",
      "\t features.11.block.0.1.weight\n",
      "\t features.11.block.0.1.bias\n",
      "\t features.11.block.1.0.weight\n",
      "\t features.11.block.1.1.weight\n",
      "\t features.11.block.1.1.bias\n",
      "\t features.11.block.2.fc1.weight\n",
      "\t features.11.block.2.fc1.bias\n",
      "\t features.11.block.2.fc2.weight\n",
      "\t features.11.block.2.fc2.bias\n",
      "\t features.11.block.3.0.weight\n",
      "\t features.11.block.3.1.weight\n",
      "\t features.11.block.3.1.bias\n",
      "\t features.12.block.0.0.weight\n",
      "\t features.12.block.0.1.weight\n",
      "\t features.12.block.0.1.bias\n",
      "\t features.12.block.1.0.weight\n",
      "\t features.12.block.1.1.weight\n",
      "\t features.12.block.1.1.bias\n",
      "\t features.12.block.2.fc1.weight\n",
      "\t features.12.block.2.fc1.bias\n",
      "\t features.12.block.2.fc2.weight\n",
      "\t features.12.block.2.fc2.bias\n",
      "\t features.12.block.3.0.weight\n",
      "\t features.12.block.3.1.weight\n",
      "\t features.12.block.3.1.bias\n",
      "\t features.13.block.0.0.weight\n",
      "\t features.13.block.0.1.weight\n",
      "\t features.13.block.0.1.bias\n",
      "\t features.13.block.1.0.weight\n",
      "\t features.13.block.1.1.weight\n",
      "\t features.13.block.1.1.bias\n",
      "\t features.13.block.2.fc1.weight\n",
      "\t features.13.block.2.fc1.bias\n",
      "\t features.13.block.2.fc2.weight\n",
      "\t features.13.block.2.fc2.bias\n",
      "\t features.13.block.3.0.weight\n",
      "\t features.13.block.3.1.weight\n",
      "\t features.13.block.3.1.bias\n",
      "\t features.14.block.0.0.weight\n",
      "\t features.14.block.0.1.weight\n",
      "\t features.14.block.0.1.bias\n",
      "\t features.14.block.1.0.weight\n",
      "\t features.14.block.1.1.weight\n",
      "\t features.14.block.1.1.bias\n",
      "\t features.14.block.2.fc1.weight\n",
      "\t features.14.block.2.fc1.bias\n",
      "\t features.14.block.2.fc2.weight\n",
      "\t features.14.block.2.fc2.bias\n",
      "\t features.14.block.3.0.weight\n",
      "\t features.14.block.3.1.weight\n",
      "\t features.14.block.3.1.bias\n",
      "\t features.15.block.0.0.weight\n",
      "\t features.15.block.0.1.weight\n",
      "\t features.15.block.0.1.bias\n",
      "\t features.15.block.1.0.weight\n",
      "\t features.15.block.1.1.weight\n",
      "\t features.15.block.1.1.bias\n",
      "\t features.15.block.2.fc1.weight\n",
      "\t features.15.block.2.fc1.bias\n",
      "\t features.15.block.2.fc2.weight\n",
      "\t features.15.block.2.fc2.bias\n",
      "\t features.15.block.3.0.weight\n",
      "\t features.15.block.3.1.weight\n",
      "\t features.15.block.3.1.bias\n",
      "\t features.16.0.weight\n",
      "\t features.16.1.weight\n",
      "\t features.16.1.bias\n",
      "\t classifier.0.weight\n",
      "\t classifier.0.bias\n",
      "\t classifier.3.weight\n",
      "\t classifier.3.bias\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Send model to GPU\n",
    "model_pyt = model_pyt.to(device)\n",
    "\n",
    "# Find parameters to be updated in this run.\n",
    "# parameters with requires_grad = True.\n",
    "params_to_update = model_pyt.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_pyt.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_pyt.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "A4B1-pfYm0Ea"
   },
   "outputs": [],
   "source": [
    "# 17 min 1 epoch - 128 batch size - inception\n",
    "# Efficientnet b0 - batch 96 - epoch 50 - num_workers 2 - flip, auto cont, sharp - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8wBR8vcG2jcd",
    "outputId": "7d01aa07-d235-4cb6-dfaf-53ce0c5a577d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "----------\n",
      "Epoch [1/100], Step [100/166], Loss: 0.8486, Accuracy: 36.00%\n",
      "train Loss: 0.9195 Acc: 0.6482\n",
      "val Loss: 1.0806 Acc: 0.6727\n",
      "\n",
      "Epoch 2/100\n",
      "----------\n",
      "Epoch [2/100], Step [100/166], Loss: 0.6057, Accuracy: 48.00%\n",
      "train Loss: 0.6138 Acc: 0.7333\n",
      "val Loss: 0.6028 Acc: 0.7281\n",
      "\n",
      "Epoch 3/100\n",
      "----------\n",
      "Epoch [3/100], Step [100/166], Loss: 0.3085, Accuracy: 56.00%\n",
      "train Loss: 0.4617 Acc: 0.8040\n",
      "val Loss: 0.4009 Acc: 0.8223\n",
      "\n",
      "Epoch 4/100\n",
      "----------\n",
      "Epoch [4/100], Step [100/166], Loss: 0.3935, Accuracy: 52.00%\n",
      "train Loss: 0.3774 Acc: 0.8473\n",
      "val Loss: 1.5413 Acc: 0.5645\n",
      "\n",
      "Epoch 5/100\n",
      "----------\n",
      "Epoch [5/100], Step [100/166], Loss: 0.2799, Accuracy: 56.00%\n",
      "train Loss: 0.3362 Acc: 0.8635\n",
      "val Loss: 1.5397 Acc: 0.4677\n",
      "\n",
      "Epoch 6/100\n",
      "----------\n",
      "Epoch [6/100], Step [100/166], Loss: 0.3189, Accuracy: 55.00%\n",
      "train Loss: 0.2668 Acc: 0.8967\n",
      "val Loss: 0.3919 Acc: 0.8495\n",
      "\n",
      "Epoch 7/100\n",
      "----------\n",
      "Epoch [7/100], Step [100/166], Loss: 0.1733, Accuracy: 56.00%\n",
      "train Loss: 0.2319 Acc: 0.9130\n",
      "val Loss: 0.3619 Acc: 0.8636\n",
      "\n",
      "Epoch 8/100\n",
      "----------\n",
      "Epoch [8/100], Step [100/166], Loss: 0.1830, Accuracy: 60.00%\n",
      "train Loss: 0.1894 Acc: 0.9293\n",
      "val Loss: 0.2147 Acc: 0.9168\n",
      "\n",
      "Epoch 9/100\n",
      "----------\n",
      "Epoch [9/100], Step [100/166], Loss: 0.2256, Accuracy: 59.00%\n",
      "train Loss: 0.1632 Acc: 0.9413\n",
      "val Loss: 0.1743 Acc: 0.9340\n",
      "\n",
      "Epoch 10/100\n",
      "----------\n",
      "Epoch [10/100], Step [100/166], Loss: 0.1111, Accuracy: 62.00%\n",
      "train Loss: 0.1356 Acc: 0.9525\n",
      "val Loss: 0.1770 Acc: 0.9349\n",
      "\n",
      "Epoch 11/100\n",
      "----------\n",
      "Epoch [11/100], Step [100/166], Loss: 0.4695, Accuracy: 53.00%\n",
      "train Loss: 0.4023 Acc: 0.8479\n",
      "val Loss: 5.8050 Acc: 0.3432\n",
      "\n",
      "Epoch 12/100\n",
      "----------\n",
      "Epoch [12/100], Step [100/166], Loss: 0.4392, Accuracy: 54.00%\n",
      "train Loss: 0.3041 Acc: 0.8866\n",
      "val Loss: 3.4900 Acc: 0.3458\n",
      "\n",
      "Epoch 13/100\n",
      "----------\n",
      "Epoch [13/100], Step [100/166], Loss: 0.1999, Accuracy: 59.00%\n",
      "train Loss: 0.2312 Acc: 0.9148\n",
      "val Loss: 0.3448 Acc: 0.8817\n",
      "\n",
      "Epoch 14/100\n",
      "----------\n",
      "Epoch [14/100], Step [100/166], Loss: 0.2234, Accuracy: 57.00%\n",
      "train Loss: 0.2063 Acc: 0.9272\n",
      "val Loss: 0.5308 Acc: 0.8231\n",
      "\n",
      "Epoch 15/100\n",
      "----------\n",
      "Epoch [15/100], Step [100/166], Loss: 0.2563, Accuracy: 58.00%\n",
      "train Loss: 0.1766 Acc: 0.9367\n",
      "val Loss: 0.5589 Acc: 0.7721\n",
      "\n",
      "Epoch 16/100\n",
      "----------\n",
      "Epoch [16/100], Step [100/166], Loss: 0.1433, Accuracy: 59.00%\n",
      "train Loss: 0.1421 Acc: 0.9494\n",
      "val Loss: 0.2146 Acc: 0.9199\n",
      "\n",
      "Epoch 17/100\n",
      "----------\n",
      "Epoch [17/100], Step [100/166], Loss: 0.0414, Accuracy: 63.00%\n",
      "train Loss: 0.1203 Acc: 0.9565\n",
      "val Loss: 0.1333 Acc: 0.9494\n",
      "\n",
      "Epoch 18/100\n",
      "----------\n",
      "Epoch [18/100], Step [100/166], Loss: 0.0875, Accuracy: 60.00%\n",
      "train Loss: 0.0907 Acc: 0.9707\n",
      "val Loss: 0.1753 Acc: 0.9384\n",
      "\n",
      "Epoch 19/100\n",
      "----------\n",
      "Epoch [19/100], Step [100/166], Loss: 0.1003, Accuracy: 62.00%\n",
      "train Loss: 0.0768 Acc: 0.9733\n",
      "val Loss: 0.1149 Acc: 0.9582\n",
      "\n",
      "Epoch 20/100\n",
      "----------\n",
      "Epoch [20/100], Step [100/166], Loss: 0.0439, Accuracy: 63.00%\n",
      "train Loss: 0.0647 Acc: 0.9781\n",
      "val Loss: 0.2289 Acc: 0.9081\n",
      "\n",
      "Epoch 21/100\n",
      "----------\n",
      "Epoch [21/100], Step [100/166], Loss: 0.2705, Accuracy: 59.00%\n",
      "train Loss: 0.2573 Acc: 0.9068\n",
      "val Loss: 3.3819 Acc: 0.4985\n",
      "\n",
      "Epoch 22/100\n",
      "----------\n",
      "Epoch [22/100], Step [100/166], Loss: 0.2365, Accuracy: 59.00%\n",
      "train Loss: 0.1999 Acc: 0.9289\n",
      "val Loss: 0.5659 Acc: 0.8491\n",
      "\n",
      "Epoch 23/100\n",
      "----------\n",
      "Epoch [23/100], Step [100/166], Loss: 0.1179, Accuracy: 60.00%\n",
      "train Loss: 0.1616 Acc: 0.9447\n",
      "val Loss: 0.2433 Acc: 0.9081\n",
      "\n",
      "Epoch 24/100\n",
      "----------\n",
      "Epoch [24/100], Step [100/166], Loss: 0.1015, Accuracy: 61.00%\n",
      "train Loss: 0.1343 Acc: 0.9539\n",
      "val Loss: 0.2958 Acc: 0.9265\n",
      "\n",
      "Epoch 25/100\n",
      "----------\n",
      "Epoch [25/100], Step [100/166], Loss: 0.0997, Accuracy: 60.00%\n",
      "train Loss: 0.1156 Acc: 0.9595\n",
      "val Loss: 0.2746 Acc: 0.9010\n",
      "\n",
      "Epoch 26/100\n",
      "----------\n",
      "Epoch [26/100], Step [100/166], Loss: 0.1620, Accuracy: 61.00%\n",
      "train Loss: 0.0846 Acc: 0.9704\n",
      "val Loss: 0.1644 Acc: 0.9503\n",
      "\n",
      "Epoch 27/100\n",
      "----------\n",
      "Epoch [27/100], Step [100/166], Loss: 0.2379, Accuracy: 61.00%\n",
      "train Loss: 0.0824 Acc: 0.9697\n",
      "val Loss: 0.0942 Acc: 0.9670\n",
      "\n",
      "Epoch 28/100\n",
      "----------\n",
      "Epoch [28/100], Step [100/166], Loss: 0.1082, Accuracy: 61.00%\n",
      "train Loss: 0.0596 Acc: 0.9808\n",
      "val Loss: 0.4739 Acc: 0.8284\n",
      "\n",
      "Epoch 29/100\n",
      "----------\n",
      "Epoch [29/100], Step [100/166], Loss: 0.0470, Accuracy: 62.00%\n",
      "train Loss: 0.0461 Acc: 0.9839\n",
      "val Loss: 0.0782 Acc: 0.9767\n",
      "\n",
      "Epoch 30/100\n",
      "----------\n",
      "Epoch [30/100], Step [100/166], Loss: 0.0187, Accuracy: 63.00%\n",
      "train Loss: 0.0390 Acc: 0.9860\n",
      "val Loss: 0.0641 Acc: 0.9798\n",
      "\n",
      "Epoch 31/100\n",
      "----------\n",
      "Epoch [31/100], Step [100/166], Loss: 0.1313, Accuracy: 63.00%\n",
      "train Loss: 0.1642 Acc: 0.9429\n",
      "val Loss: 1.0555 Acc: 0.7769\n",
      "\n",
      "Epoch 32/100\n",
      "----------\n",
      "Epoch [32/100], Step [100/166], Loss: 0.1046, Accuracy: 62.00%\n",
      "train Loss: 0.1554 Acc: 0.9455\n",
      "val Loss: 1.2124 Acc: 0.7651\n",
      "\n",
      "Epoch 33/100\n",
      "----------\n",
      "Epoch [33/100], Step [100/166], Loss: 0.1267, Accuracy: 60.00%\n",
      "train Loss: 0.1188 Acc: 0.9600\n",
      "val Loss: 0.5947 Acc: 0.8619\n",
      "\n",
      "Epoch 34/100\n",
      "----------\n",
      "Epoch [34/100], Step [100/166], Loss: 0.0841, Accuracy: 63.00%\n",
      "train Loss: 0.1087 Acc: 0.9621\n",
      "val Loss: 0.2920 Acc: 0.8979\n",
      "\n",
      "Epoch 35/100\n",
      "----------\n",
      "Epoch [35/100], Step [100/166], Loss: 0.0092, Accuracy: 64.00%\n",
      "train Loss: 0.0924 Acc: 0.9679\n",
      "val Loss: 1.1442 Acc: 0.7088\n",
      "\n",
      "Epoch 36/100\n",
      "----------\n",
      "Epoch [36/100], Step [100/166], Loss: 0.0984, Accuracy: 60.00%\n",
      "train Loss: 0.0768 Acc: 0.9757\n",
      "val Loss: 0.1257 Acc: 0.9586\n",
      "\n",
      "Epoch 37/100\n",
      "----------\n",
      "Epoch [37/100], Step [100/166], Loss: 0.0331, Accuracy: 63.00%\n",
      "train Loss: 0.0541 Acc: 0.9819\n",
      "val Loss: 0.2496 Acc: 0.9129\n",
      "\n",
      "Epoch 38/100\n",
      "----------\n",
      "Epoch [38/100], Step [100/166], Loss: 0.0103, Accuracy: 64.00%\n",
      "train Loss: 0.0425 Acc: 0.9850\n",
      "val Loss: 0.1020 Acc: 0.9718\n",
      "\n",
      "Epoch 39/100\n",
      "----------\n",
      "Epoch [39/100], Step [100/166], Loss: 0.0201, Accuracy: 63.00%\n",
      "train Loss: 0.0281 Acc: 0.9900\n",
      "val Loss: 0.0948 Acc: 0.9701\n",
      "\n",
      "Epoch 40/100\n",
      "----------\n",
      "Epoch [40/100], Step [100/166], Loss: 0.0432, Accuracy: 63.00%\n",
      "train Loss: 0.0216 Acc: 0.9923\n",
      "val Loss: 0.0675 Acc: 0.9811\n",
      "\n",
      "Epoch 41/100\n",
      "----------\n",
      "Epoch [41/100], Step [100/166], Loss: 0.1277, Accuracy: 60.00%\n",
      "train Loss: 0.1480 Acc: 0.9490\n",
      "val Loss: 6.6342 Acc: 0.4487\n",
      "\n",
      "Epoch 42/100\n",
      "----------\n",
      "Epoch [42/100], Step [100/166], Loss: 0.3148, Accuracy: 59.00%\n",
      "train Loss: 0.1428 Acc: 0.9493\n",
      "val Loss: 0.5444 Acc: 0.8280\n",
      "\n",
      "Epoch 43/100\n",
      "----------\n",
      "Epoch [43/100], Step [100/166], Loss: 0.0658, Accuracy: 63.00%\n",
      "train Loss: 0.1026 Acc: 0.9659\n",
      "val Loss: 0.3363 Acc: 0.8962\n",
      "\n",
      "Epoch 44/100\n",
      "----------\n",
      "Epoch [44/100], Step [100/166], Loss: 0.1163, Accuracy: 61.00%\n",
      "train Loss: 0.0810 Acc: 0.9723\n",
      "val Loss: 0.4141 Acc: 0.8988\n",
      "\n",
      "Epoch 45/100\n",
      "----------\n",
      "Epoch [45/100], Step [100/166], Loss: 0.1036, Accuracy: 62.00%\n",
      "train Loss: 0.0646 Acc: 0.9788\n",
      "val Loss: 0.1762 Acc: 0.9542\n",
      "\n",
      "Epoch 46/100\n",
      "----------\n",
      "Epoch [46/100], Step [100/166], Loss: 0.0221, Accuracy: 64.00%\n",
      "train Loss: 0.0525 Acc: 0.9826\n",
      "val Loss: 6.2251 Acc: 0.4795\n",
      "\n",
      "Epoch 47/100\n",
      "----------\n",
      "Epoch [47/100], Step [100/166], Loss: 0.0577, Accuracy: 63.00%\n",
      "train Loss: 0.0513 Acc: 0.9839\n",
      "val Loss: 0.1407 Acc: 0.9613\n",
      "\n",
      "Epoch 48/100\n",
      "----------\n",
      "Epoch [48/100], Step [100/166], Loss: 0.0025, Accuracy: 64.00%\n",
      "train Loss: 0.0327 Acc: 0.9886\n",
      "val Loss: 0.0908 Acc: 0.9701\n",
      "\n",
      "Epoch 49/100\n",
      "----------\n",
      "Epoch [49/100], Step [100/166], Loss: 0.0285, Accuracy: 63.00%\n",
      "train Loss: 0.0222 Acc: 0.9930\n",
      "val Loss: 0.0731 Acc: 0.9771\n",
      "\n",
      "Epoch 50/100\n",
      "----------\n",
      "Epoch [50/100], Step [100/166], Loss: 0.0067, Accuracy: 64.00%\n",
      "train Loss: 0.0157 Acc: 0.9952\n",
      "val Loss: 0.0763 Acc: 0.9767\n",
      "\n",
      "Epoch 51/100\n",
      "----------\n",
      "Epoch [51/100], Step [100/166], Loss: 0.1334, Accuracy: 60.00%\n",
      "train Loss: 0.1006 Acc: 0.9662\n",
      "val Loss: 0.9726 Acc: 0.8055\n",
      "\n",
      "Epoch 52/100\n",
      "----------\n",
      "Epoch [52/100], Step [100/166], Loss: 0.0182, Accuracy: 64.00%\n",
      "train Loss: 0.1024 Acc: 0.9650\n",
      "val Loss: 0.5479 Acc: 0.8764\n",
      "\n",
      "Epoch 53/100\n",
      "----------\n",
      "Epoch [53/100], Step [100/166], Loss: 0.2713, Accuracy: 61.00%\n",
      "train Loss: 0.0993 Acc: 0.9675\n",
      "val Loss: 3.1508 Acc: 0.5693\n",
      "\n",
      "Epoch 54/100\n",
      "----------\n",
      "Epoch [54/100], Step [100/166], Loss: 0.0271, Accuracy: 64.00%\n",
      "train Loss: 0.0715 Acc: 0.9770\n",
      "val Loss: 0.3783 Acc: 0.9164\n",
      "\n",
      "Epoch 55/100\n",
      "----------\n",
      "Epoch [55/100], Step [100/166], Loss: 0.1164, Accuracy: 61.00%\n",
      "train Loss: 0.0650 Acc: 0.9775\n",
      "val Loss: 0.2987 Acc: 0.9160\n",
      "\n",
      "Epoch 56/100\n",
      "----------\n",
      "Epoch [56/100], Step [100/166], Loss: 0.1275, Accuracy: 61.00%\n",
      "train Loss: 0.0454 Acc: 0.9843\n",
      "val Loss: 0.1191 Acc: 0.9648\n",
      "\n",
      "Epoch 57/100\n",
      "----------\n",
      "Epoch [57/100], Step [100/166], Loss: 0.0079, Accuracy: 64.00%\n",
      "train Loss: 0.0323 Acc: 0.9894\n",
      "val Loss: 0.4069 Acc: 0.8698\n",
      "\n",
      "Epoch 58/100\n",
      "----------\n",
      "Epoch [58/100], Step [100/166], Loss: 0.0225, Accuracy: 63.00%\n",
      "train Loss: 0.0229 Acc: 0.9926\n",
      "val Loss: 0.1110 Acc: 0.9723\n",
      "\n",
      "Epoch 59/100\n",
      "----------\n",
      "Epoch [59/100], Step [100/166], Loss: 0.0056, Accuracy: 64.00%\n",
      "train Loss: 0.0176 Acc: 0.9943\n",
      "val Loss: 0.0757 Acc: 0.9798\n",
      "\n",
      "Epoch 60/100\n",
      "----------\n",
      "Epoch [60/100], Step [100/166], Loss: 0.0005, Accuracy: 64.00%\n",
      "train Loss: 0.0127 Acc: 0.9948\n",
      "val Loss: 0.0731 Acc: 0.9811\n",
      "\n",
      "Epoch 61/100\n",
      "----------\n",
      "Epoch [61/100], Step [100/166], Loss: 0.1582, Accuracy: 60.00%\n",
      "train Loss: 0.1410 Acc: 0.9531\n",
      "val Loss: 1.5309 Acc: 0.6771\n",
      "\n",
      "Epoch 62/100\n",
      "----------\n",
      "Epoch [62/100], Step [100/166], Loss: 0.0721, Accuracy: 63.00%\n",
      "train Loss: 0.1223 Acc: 0.9611\n",
      "val Loss: 0.4592 Acc: 0.8927\n",
      "\n",
      "Epoch 63/100\n",
      "----------\n",
      "Epoch [63/100], Step [100/166], Loss: 0.0369, Accuracy: 64.00%\n",
      "train Loss: 0.0931 Acc: 0.9696\n",
      "val Loss: 0.4577 Acc: 0.8874\n",
      "\n",
      "Epoch 64/100\n",
      "----------\n",
      "Epoch [64/100], Step [100/166], Loss: 0.0661, Accuracy: 63.00%\n",
      "train Loss: 0.0626 Acc: 0.9817\n",
      "val Loss: 0.1147 Acc: 0.9683\n",
      "\n",
      "Epoch 65/100\n",
      "----------\n",
      "Epoch [65/100], Step [100/166], Loss: 0.1135, Accuracy: 63.00%\n",
      "train Loss: 0.0568 Acc: 0.9817\n",
      "val Loss: 0.2625 Acc: 0.9331\n",
      "\n",
      "Epoch 66/100\n",
      "----------\n",
      "Epoch [66/100], Step [100/166], Loss: 0.0718, Accuracy: 62.00%\n",
      "train Loss: 0.0444 Acc: 0.9844\n",
      "val Loss: 0.0886 Acc: 0.9718\n",
      "\n",
      "Epoch 67/100\n",
      "----------\n",
      "Epoch [67/100], Step [100/166], Loss: 0.0387, Accuracy: 63.00%\n",
      "train Loss: 0.0328 Acc: 0.9894\n",
      "val Loss: 0.0701 Acc: 0.9793\n",
      "\n",
      "Epoch 68/100\n",
      "----------\n",
      "Epoch [68/100], Step [100/166], Loss: 0.0169, Accuracy: 64.00%\n",
      "train Loss: 0.0210 Acc: 0.9936\n",
      "val Loss: 0.0926 Acc: 0.9740\n",
      "\n",
      "Epoch 69/100\n",
      "----------\n",
      "Epoch [69/100], Step [100/166], Loss: 0.0152, Accuracy: 63.00%\n",
      "train Loss: 0.0146 Acc: 0.9949\n",
      "val Loss: 0.0590 Acc: 0.9798\n",
      "\n",
      "Epoch 70/100\n",
      "----------\n",
      "Epoch [70/100], Step [100/166], Loss: 0.0070, Accuracy: 64.00%\n",
      "train Loss: 0.0152 Acc: 0.9951\n",
      "val Loss: 0.0616 Acc: 0.9820\n",
      "\n",
      "Epoch 71/100\n",
      "----------\n",
      "Epoch [71/100], Step [100/166], Loss: 0.0812, Accuracy: 63.00%\n",
      "train Loss: 0.2373 Acc: 0.9394\n",
      "val Loss: 19.6865 Acc: 0.0933\n",
      "\n",
      "Epoch 72/100\n",
      "----------\n",
      "Epoch [72/100], Step [100/166], Loss: 0.0711, Accuracy: 63.00%\n",
      "train Loss: 0.1525 Acc: 0.9513\n",
      "val Loss: 0.1733 Acc: 0.9344\n",
      "\n",
      "Epoch 73/100\n",
      "----------\n",
      "Epoch [73/100], Step [100/166], Loss: 0.0246, Accuracy: 64.00%\n",
      "train Loss: 0.0796 Acc: 0.9733\n",
      "val Loss: 0.2011 Acc: 0.9340\n",
      "\n",
      "Epoch 74/100\n",
      "----------\n",
      "Epoch [74/100], Step [100/166], Loss: 0.0415, Accuracy: 63.00%\n",
      "train Loss: 0.0725 Acc: 0.9758\n",
      "val Loss: 0.1246 Acc: 0.9661\n",
      "\n",
      "Epoch 75/100\n",
      "----------\n",
      "Epoch [75/100], Step [100/166], Loss: 0.0331, Accuracy: 63.00%\n",
      "train Loss: 0.0512 Acc: 0.9829\n",
      "val Loss: 0.1574 Acc: 0.9516\n",
      "\n",
      "Epoch 76/100\n",
      "----------\n",
      "Epoch [76/100], Step [100/166], Loss: 0.0050, Accuracy: 64.00%\n",
      "train Loss: 0.0431 Acc: 0.9854\n",
      "val Loss: 0.2837 Acc: 0.9256\n",
      "\n",
      "Epoch 77/100\n",
      "----------\n",
      "Epoch [77/100], Step [100/166], Loss: 0.0172, Accuracy: 63.00%\n",
      "train Loss: 0.0385 Acc: 0.9872\n",
      "val Loss: 0.0841 Acc: 0.9758\n",
      "\n",
      "Epoch 78/100\n",
      "----------\n",
      "Epoch [78/100], Step [100/166], Loss: 0.0156, Accuracy: 64.00%\n",
      "train Loss: 0.0239 Acc: 0.9926\n",
      "val Loss: 0.0713 Acc: 0.9749\n",
      "\n",
      "Epoch 79/100\n",
      "----------\n",
      "Epoch [79/100], Step [100/166], Loss: 0.0336, Accuracy: 63.00%\n",
      "train Loss: 0.0219 Acc: 0.9931\n",
      "val Loss: 0.0658 Acc: 0.9824\n",
      "\n",
      "Epoch 80/100\n",
      "----------\n",
      "Epoch [80/100], Step [100/166], Loss: 0.0172, Accuracy: 63.00%\n",
      "train Loss: 0.0168 Acc: 0.9942\n",
      "val Loss: 0.0611 Acc: 0.9855\n",
      "\n",
      "Epoch 81/100\n",
      "----------\n",
      "Epoch [81/100], Step [100/166], Loss: 0.0265, Accuracy: 64.00%\n",
      "train Loss: 0.0920 Acc: 0.9714\n",
      "val Loss: 0.2529 Acc: 0.9085\n",
      "\n",
      "Epoch 82/100\n",
      "----------\n",
      "Epoch [82/100], Step [100/166], Loss: 0.0188, Accuracy: 64.00%\n",
      "train Loss: 0.0736 Acc: 0.9767\n",
      "val Loss: 0.1690 Acc: 0.9547\n",
      "\n",
      "Epoch 83/100\n",
      "----------\n",
      "Epoch [83/100], Step [100/166], Loss: 0.0646, Accuracy: 62.00%\n",
      "train Loss: 0.0649 Acc: 0.9781\n",
      "val Loss: 0.9211 Acc: 0.8548\n",
      "\n",
      "Epoch 84/100\n",
      "----------\n",
      "Epoch [84/100], Step [100/166], Loss: 0.1112, Accuracy: 61.00%\n",
      "train Loss: 0.0671 Acc: 0.9782\n",
      "val Loss: 1.5266 Acc: 0.7308\n",
      "\n",
      "Epoch 85/100\n",
      "----------\n",
      "Epoch [85/100], Step [100/166], Loss: 0.0450, Accuracy: 63.00%\n",
      "train Loss: 0.0442 Acc: 0.9860\n",
      "val Loss: 0.0874 Acc: 0.9749\n",
      "\n",
      "Epoch 86/100\n",
      "----------\n",
      "Epoch [86/100], Step [100/166], Loss: 0.0106, Accuracy: 64.00%\n",
      "train Loss: 0.0326 Acc: 0.9893\n",
      "val Loss: 0.0956 Acc: 0.9683\n",
      "\n",
      "Epoch 87/100\n",
      "----------\n",
      "Epoch [87/100], Step [100/166], Loss: 0.0204, Accuracy: 63.00%\n",
      "train Loss: 0.0288 Acc: 0.9913\n",
      "val Loss: 0.1103 Acc: 0.9736\n",
      "\n",
      "Epoch 88/100\n",
      "----------\n",
      "Epoch [88/100], Step [100/166], Loss: 0.0044, Accuracy: 64.00%\n",
      "train Loss: 0.0198 Acc: 0.9935\n",
      "val Loss: 0.0768 Acc: 0.9793\n",
      "\n",
      "Epoch 89/100\n",
      "----------\n",
      "Epoch [89/100], Step [100/166], Loss: 0.0034, Accuracy: 64.00%\n",
      "train Loss: 0.0147 Acc: 0.9956\n",
      "val Loss: 0.0761 Acc: 0.9824\n",
      "\n",
      "Epoch 90/100\n",
      "----------\n",
      "Epoch [90/100], Step [100/166], Loss: 0.0017, Accuracy: 64.00%\n",
      "train Loss: 0.0088 Acc: 0.9975\n",
      "val Loss: 0.0759 Acc: 0.9824\n",
      "\n",
      "Epoch 91/100\n",
      "----------\n",
      "Epoch [91/100], Step [100/166], Loss: 0.5884, Accuracy: 59.00%\n",
      "train Loss: 0.0960 Acc: 0.9685\n",
      "val Loss: 0.6860 Acc: 0.8737\n",
      "\n",
      "Epoch 92/100\n",
      "----------\n",
      "Epoch [92/100], Step [100/166], Loss: 0.0495, Accuracy: 63.00%\n",
      "train Loss: 0.0934 Acc: 0.9679\n",
      "val Loss: 0.2668 Acc: 0.9424\n",
      "\n",
      "Epoch 93/100\n",
      "----------\n",
      "Epoch [93/100], Step [100/166], Loss: 0.0400, Accuracy: 63.00%\n",
      "train Loss: 0.0595 Acc: 0.9807\n",
      "val Loss: 3.6744 Acc: 0.5517\n",
      "\n",
      "Epoch 94/100\n",
      "----------\n",
      "Epoch [94/100], Step [100/166], Loss: 0.0860, Accuracy: 63.00%\n",
      "train Loss: 0.0510 Acc: 0.9845\n",
      "val Loss: 0.2704 Acc: 0.9186\n",
      "\n",
      "Epoch 95/100\n",
      "----------\n",
      "Epoch [95/100], Step [100/166], Loss: 0.0214, Accuracy: 64.00%\n",
      "train Loss: 0.0359 Acc: 0.9872\n",
      "val Loss: 0.1872 Acc: 0.9556\n",
      "\n",
      "Epoch 96/100\n",
      "----------\n",
      "Epoch [96/100], Step [100/166], Loss: 0.1090, Accuracy: 63.00%\n",
      "train Loss: 0.0278 Acc: 0.9918\n",
      "val Loss: 0.1530 Acc: 0.9604\n",
      "\n",
      "Epoch 97/100\n",
      "----------\n",
      "Epoch [97/100], Step [100/166], Loss: 0.0004, Accuracy: 64.00%\n",
      "train Loss: 0.0209 Acc: 0.9929\n",
      "val Loss: 0.0674 Acc: 0.9815\n",
      "\n",
      "Epoch 98/100\n",
      "----------\n",
      "Epoch [98/100], Step [100/166], Loss: 0.0004, Accuracy: 64.00%\n",
      "train Loss: 0.0106 Acc: 0.9958\n",
      "val Loss: 0.0700 Acc: 0.9820\n",
      "\n",
      "Epoch 99/100\n",
      "----------\n",
      "Epoch [99/100], Step [100/166], Loss: 0.0004, Accuracy: 64.00%\n",
      "train Loss: 0.0116 Acc: 0.9960\n",
      "val Loss: 0.0742 Acc: 0.9824\n",
      "\n",
      "Epoch 100/100\n",
      "----------\n",
      "Epoch [100/100], Step [100/166], Loss: 0.0529, Accuracy: 63.00%\n",
      "train Loss: 0.0066 Acc: 0.9980\n",
      "val Loss: 0.0789 Acc: 0.9855\n",
      "\n",
      "Training complete in 74m 20s\n",
      "Best val Acc: 0.985482\n"
     ]
    }
   ],
   "source": [
    "# Observe that all parameters are being optimized\n",
    "optimizer = optim.Adam(params_to_update, lr=lr_start)\n",
    "\n",
    "# Learning rate scheduler.\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=iter_restart, T_mult=mul_restart, \n",
    "                                                           eta_min=lr_end, last_epoch=-1)\n",
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate\n",
    "model_pyt, prof, val_history, train_history = train_model(device, model_pyt, data_loaders, \n",
    "                                                            optimizer, scheduler,\n",
    "                                                            criterion, \n",
    "                                                            num_epochs=num_epochs,\n",
    "                                                            num_classes=num_classes,\n",
    "                                                            is_inception=(model_name==\"inceptionv3\"),\n",
    "                                                            profiler=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'covid_pneumonia'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "zM-dorQBJAZb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_model(model_pyt, '../../models/', \n",
    "           f'{model_name}_{dataset_name}_{num_epochs}_{batch_size}_{lr_start}_{lr_end}_model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics = eval_model(device=device, model=model_pyt, test_loader=data_loaders['test'], is_inception=(model_name==\"inceptionv3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9858962693357598\n",
      "f1: 0.9827359179918623\n",
      "cm: [[ 527   16    0]\n",
      " [   7 1442    4]\n",
      " [   0    4  198]]\n",
      "outputs: [0 0 0 ... 2 2 2]\n",
      "targets: [0 0 0 ... 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "for i, v in eval_metrics.items():\n",
    "    print(f\"{i}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../../models/val_history_{model_name}_{dataset_name}_{num_epochs}_{batch_size}_{lr_start}_{lr_end}_.pickle', 'wb') as handle:\n",
    "    pickle.dump(val_history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(f'../../models/train_history_{model_name}_{dataset_name}_{num_epochs}_{batch_size}_{lr_start}_{lr_end}_.pickle', 'wb') as handle:\n",
    "    pickle.dump(train_history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(f'../../models/eval_metrics_{model_name}_{dataset_name}_{num_epochs}_{batch_size}_{lr_start}_{lr_end}_.pickle', 'wb') as handle:\n",
    "    pickle.dump(eval_metrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#with open('filename.pickle', 'rb') as handle:\n",
    "#    b = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WvFygat_aiDN"
   },
   "outputs": [],
   "source": [
    "#print(prof.key_averages(group_by_stack_n=5).table(sort_by='self_cpu_time_total', row_limit=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "EQ6hb3iO2mXv"
   },
   "outputs": [],
   "source": [
    "# Plot the training curves of validation accuracy vs. number\n",
    "#  of training epochs for the transfer learning method and\n",
    "#  the model trained from scratch\n",
    "# vhist = []\n",
    "# vhist = [h.cpu().numpy() for h in val_acc_history]\n",
    "# thist = []\n",
    "# thist = [h.cpu().numpy() for h in train_acc_history]\n",
    "\n",
    "# plt.title(\"Accuracy vs. Number of Training Epochs\")\n",
    "# plt.xlabel(\"Training Epochs\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# #plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\n",
    "# plt.plot(range(1,num_epochs+1),vhist,label=\"Validation\")\n",
    "# plt.plot(range(1,num_epochs+1),thist,label=\"Training\")\n",
    "# plt.ylim((0,1.))\n",
    "# plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "aXpHASjTUE_Q"
   },
   "outputs": [],
   "source": [
    "# Plot the training curves of validation accuracy vs. number\n",
    "#  of training epochs for the transfer learning method and\n",
    "#  the model trained from scratch\n",
    "# vhist = []\n",
    "# vhist = [h for h in val_loss_history]\n",
    "# thist = []\n",
    "# thist = [h for h in train_loss_history]\n",
    "\n",
    "# plt.title(\"Loss vs. Number of Training Epochs\")\n",
    "# plt.xlabel(\"Training Epochs\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# #plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\n",
    "# plt.plot(range(1,num_epochs+1),vhist,label=\"Validation\")\n",
    "# plt.plot(range(1,num_epochs+1),thist,label=\"Training\")\n",
    "# plt.ylim((0,1.))\n",
    "# plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
